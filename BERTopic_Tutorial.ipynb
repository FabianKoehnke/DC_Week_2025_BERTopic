{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic Tutorial\n",
    "\n",
    "This tutorial is a compilation of the BERTopic documetation, the corresponding paper, the SBERT documentation and own code to give an introduction to topic modeling with BERTopic. This should enable you to carry out practical experiments with your own data. ðŸš€\n",
    "\n",
    "## Introduction\n",
    "\n",
    "BERTopic leverages ðŸ¤— transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n",
    "\n",
    "We can use topic modeling, e.g., for:\n",
    "\n",
    "- Clustering : Grouping related papers based on their topics\n",
    "- Automatic Summarization : Extracting main topics from a vast number of papers\n",
    "- Research Gap Identification : Detecting underexplored areas by analyzing topic distributions\n",
    "- Interdisciplinary Studies : Finding connections between different research domains\n",
    "- Recommendation Systems â€“ Suggesting relevant papers based on topic similarities\n",
    "\n",
    "## Main Representation of the Algotihm \n",
    "\n",
    "Step 1. Embedding  \n",
    "Step 2. Dimensionality Reduction  \n",
    "Step 3. Clustering  \n",
    "Step 4. Tokenizer  \n",
    "Step 5. Weighting scheme  \n",
    "\n",
    "NOTE: All steps are independent and modulare. E.g., you so can use a variety of clustering models (k-Means, HDBSCAN...)\n",
    "\n",
    "### 1. Embedding \n",
    "\n",
    "Embedding refers to the process of converting high-dimensional data (like words) into a lower-dimensional, dense numerical representation. These representations (vectors), capture meaningful relationships between entities, making it easier for AI models to understand and process data efficiently.\n",
    "\n",
    "With BERTopic and Sentence Transformers we try to capture the meaning of whole sentences or documents. According to the documentation, sentences or entire documents can be used for embedding. We can use multiple pre-trained sentence-transformers. Here is a list:\n",
    "\n",
    "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "\n",
    "<div style=\"border: 1px solid white; padding: 10px;\">\n",
    "DEGRESSION - Transformers:\n",
    "\n",
    "A Transformer is a deep learning architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. (2017). It revolutionized natural language processing (NLP) by replacing older models like RNNs and LSTMs with a more efficient mechanism: self-attention.\n",
    "\n",
    "Self-Attention Mechanism\n",
    "- Each word (or token) in a sentence attends to all other words, learning contextual relationships dynamically.  \n",
    "\n",
    "Positional Encoding\n",
    "- Since Transformers donâ€™t process words sequentially like RNNs, they need a way to understand the order of words.\n",
    "- Positional encoding adds information about word order to the embeddings.  \n",
    "\n",
    "Multi-Head Attention\n",
    "- Instead of just one self-attention mechanism, multiple attention heads allow the model to focus on different parts of a sentence simultaneously.\n",
    "</div>\n",
    "\n",
    "The embedding are primarily used to cluster semantically similar documents and not directly used in generating the topics...\n",
    "\n",
    "### 2. Dimensionality Reduction \n",
    "\n",
    "After having created our numerical representations of the documents we have to reduce the dimensionality of these representations. Cluster models typically have difficulty handling high dimensional data due to the curse of dimensionality. There are great approaches that can reduce dimensionality, such as PCA, but as a default UMAP is selected in BERTopic. It is a technique that can keep some of a dataset's local and global structure when reducing its dimensionality. This structure is important to keep as it contains the information necessary to create clusters of semantically similar documents.\n",
    "\n",
    "### 3. Cluster Documents\n",
    "\n",
    "After having reduced our embeddings, we can start clustering our data. For that, we leverage a density-based clustering technique, HDBSCAN. It can find clusters of different shapes and has the nice feature of identifying outliers where possible. As a result, we do not force documents into a cluster where they might not belong. This will improve the resulting topic representation as there is less noise to draw from.\n",
    "\n",
    "NOTE: Paper of HDBSCAN: Leland McInnes, John Healy, and Steve Astels. 2017. hdbscan: Hierarchical density based clustering. The Journal of Open Source Software, 2(11):205.\n",
    "\n",
    "<div style=\"border: 1px solid white; padding: 10px;\">\n",
    "DEGRESSION - HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise):\n",
    "\n",
    "It works in four main steps:\n",
    "\n",
    "1. Construct Mutual Reachability Graph â€“ This distance metric considers both the core distance (the distance to the k-th nearest neighbor) and the mutual reachability distance between points.\n",
    "2. Generate Minimum Spanning Tree (MST) â€“ Builds a MST from the distance graph, capturing cluster connectivity.\n",
    "3. Hierarchical Clustering â€“ Forms a hierarchy by progressively removing longest edges in the MST.\n",
    "4. Condense and Extract Stable Clusters â€“ instead of manually selecting a clustering level, HDBSCAN automatically determines the most stable clusters by analyzing the persistence of clusters in the hierarchy. Clusters that persist across different levels of granularity are considered meaningful.\n",
    "5. Identifying Noise - HDBSCAN automatically classifies outliers as noise when they do not belong to any stable cluster.\n",
    "\n",
    "</div>\n",
    "\n",
    "### 4. Tokenizer \n",
    "\n",
    "We want a topic representation technique that makes little to no assumption on the expected structure of the clusters. To do this, we first combine all documents in a cluster into a single document. That, very long, document then represents the cluster. Then, we can count how often each word appears in each cluster. This generates something called a bag-of-words representation in which the frequency of each word in each cluster can be found. This bag-of-words representation is therefore on a cluster level and not on a document level. This distinction is important as we are interested in words on a topic level (i.e., cluster level). By using a bag-of-words representation, no assumption is made concerning the structure of the clusters. \n",
    "\n",
    "NOTE: detailed informations here: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "### 5. Weighting scheme  \n",
    "\n",
    "From the generated bag-of-words representation, we want to know what makes one cluster different from another. Which words are typical for cluster 1 and not so much for all other clusters? \n",
    "\n",
    "The topic representations are modeled based on the documents in each cluster where each cluster will\n",
    "be assigned one topic. For each topic, we want to know what makes one topic, based on its cluster-\n",
    "word distribution, different from another? For this purpose, we can modify TF-IDF (Term Frequency-Inverse Document Frequency), a measure for representing the importance of a word to a document, such that it allows for a representation of a termâ€™s importance to a topic instead.\n",
    "\n",
    "The classic TF-IDF procedure combines two statistics, term frequency, and inverse document\n",
    "frequency: \n",
    "\n",
    "$$\n",
    "W_{t,d}=tf_{t,d} \\cdot log(\\frac{N}{df_t})\n",
    "$$\n",
    "\n",
    "Where the term frequency models the frequency of term $t$ in document $d$. The inverse document frequency measures how much information a term provides to a document and is calculated by taking the logarithm of the number of documents in a corpus $N$ divided by the total number of documents that contain $t$.\n",
    "\n",
    "We generalize this procedure to clusters of documents: \n",
    "\n",
    "$$\n",
    "W_{t,c}=tf_{t,c} \\cdot log(1+\\frac{A}{tf_t})\n",
    "$$\n",
    "\n",
    "Where the term frequency models the frequency of term $t$ in a class $c$ or in this instance. Here, the class $c$ is the collection of documents concatenated into a single document for each cluster. Then, the inverse document frequency is replaced by the inverse class frequency to measure how much information a term provides to a class. It is calculated by taking the logarithm of the average number of words per class $A$ divided by the frequency of term $t$ across all classes. To output only positive values, we add one to the division within the logarithm. Thus, this class-based TF-IDF procedure models the importance of words in clusters instead of individual documents. This allows us to generate topic-word distributions for each cluster of documents.\n",
    "\n",
    "In other words, if we extract the most important words per cluster, we get descriptions of topics!\n",
    "\n",
    "### Further Informations \n",
    "\n",
    "- Paper : https://arxiv.org/abs/2203.05794\n",
    "- Project Homepage : https://maartengr.github.io/BERTopic/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to run the whole notebook with all dependencies. \n",
    "2. Play around with some parameters:\n",
    "    - Load more data by changing the slicer to a larger number (Loading Data - line 2)\n",
    "    - Pass another LLM (Training - line 2)\n",
    "    - change some paramters (Training)  \n",
    "        - n_neighbors = controls the balance between local and global structure; lower value local structure focus; higher values global structure focus. It determines how many nearest neighbors are considered when constructing the initial high-dimensional graph.\n",
    "        - min_cluster_size = minimum number of points required to form a cluster. Low values (e.g., 5â€“15): Detects smaller, fine-grained clusters but may increase noise. Higher values (e.g., 50â€“100): Produces larger, more stable clusters but may miss small but meaningful groups.\n",
    "3. Load your own Data or the CShorten/ML-ArXiv-Papers and transform a new topic afterwards so find maybee a research gap \n",
    "4. or compare the results of the different new_topic_text inputs in \"Predictions\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install bertopic\n",
    "!pip install datasets\n",
    "!pip isntall PyPDF2\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: We pobably need to restart the notebook after the installations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bertopic as bt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from hdbscan import HDBSCAN\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PyPDF2 import PdfReader\n",
    "import os \n",
    "from umap import UMAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you load 500 different paper/docs\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "# data = load_dataset(\"newsgroup\", '18828_alt.atheism')[\"train\"][\"text\"]\n",
    "data = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"][\"abstract\"][0:500]\n",
    "'''\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    #text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "    text = reader.pages[0].extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "data = []\n",
    "path_data = \"/Users/Fabian/Documents/DataScience/Phd/Disertation/Research/Paper/\" # TODO change to your path\n",
    "\n",
    "for pdf in os.listdir(path_data):\n",
    "    if pdf[-3:] == \"pdf\":\n",
    "        pdf_text = extract_text_from_pdf(path_data + pdf)\n",
    "        data.append(pdf_text)\n",
    "    else:\n",
    "        print(\"file is not a pdf\")\n",
    "'''\n",
    "print(f\"you load {len(data)} different paper/docs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can change the embedding model to any other models from the sentence-transformers here:\n",
    "\n",
    "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 11:59:16,547 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:22<00:00,  1.43s/it]\n",
      "2025-02-25 11:59:41,525 - BERTopic - Embedding - Completed âœ“\n",
      "2025-02-25 11:59:41,527 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-02-25 11:59:42,482 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-02-25 11:59:42,484 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-02-25 11:59:42,510 - BERTopic - Cluster - Completed âœ“\n",
      "2025-02-25 11:59:42,522 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-02-25 11:59:49,728 - BERTopic - Representation - Completed âœ“\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Extract embeddings\n",
    "embedding_model = \"all-MiniLM-L6-v2\" # We can change to a larger model (all-mpnet-base-v2) or any other model from the list above.\n",
    "'''\n",
    "NOTE: \n",
    "  all-MiniLM-L6-v2\n",
    "    - Size : 22.7M params\n",
    "    - Speed : 14,200 sentences per sec.\n",
    "  all-mpnet-base-v2\n",
    "    - Size : 109M params\n",
    "    - Speed : 2,800 sentences per sec.\n",
    "  \n",
    "'''\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15, \n",
    "    n_components=2, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine',\n",
    "    random_state=random_seed\n",
    "    )\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=5, \n",
    "    metric='euclidean', \n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=True,\n",
    "    )\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = bt.vectorizers.ClassTfidfTransformer()\n",
    "# Step 6 - (Optional) Fine-tune topic representations with \n",
    "# a `bertopic.representation` model\n",
    "representation_model = bt.representation.KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = bt.BERTopic(\n",
    "  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "  representation_model=representation_model, # Step 6 - (Optional) Fine-tune topic representations\n",
    "  verbose=True,\n",
    "  language=\"english\"\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>118</td>\n",
       "      <td>-1_classifiers_classification_classifier_networks</td>\n",
       "      <td>[classifiers, classification, classifier, netw...</td>\n",
       "      <td>[  We have proposed a model based upon flockin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0_generalization_models_markov_algorithms</td>\n",
       "      <td>[generalization, models, markov, algorithms, e...</td>\n",
       "      <td>[  The problem of statistical learning is to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1_adaptive_reinforcement_markov_bayesian</td>\n",
       "      <td>[adaptive, reinforcement, markov, bayesian, ag...</td>\n",
       "      <td>[  General purpose intelligent learning agents...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name   \n",
       "0     -1    118  -1_classifiers_classification_classifier_networks  \\\n",
       "1      0    108          0_generalization_models_markov_algorithms   \n",
       "2      1     30           1_adaptive_reinforcement_markov_bayesian   \n",
       "\n",
       "                                      Representation   \n",
       "0  [classifiers, classification, classifier, netw...  \\\n",
       "1  [generalization, models, markov, algorithms, e...   \n",
       "2  [adaptive, reinforcement, markov, bayesian, ag...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [  We have proposed a model based upon flockin...  \n",
       "1  [  The problem of statistical learning is to c...  \n",
       "2  [  General purpose intelligent learning agents...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = topic_model.get_topic_info() \n",
    "freq.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: -1 refers to all outliers and should typically be ignored."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With visualize_barchart() we can visualize our topics for a first overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.3162913918495178,
          0.3204805850982666,
          0.32073456048965454,
          0.3427988290786743,
          0.3727502226829529
         ],
         "xaxis": "x",
         "y": [
          "estimation  ",
          "algorithms  ",
          "markov  ",
          "models  ",
          "generalization  "
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#0072B2"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.34868353605270386,
          0.35635867714881897,
          0.4117738604545593,
          0.41945087909698486,
          0.47866085171699524
         ],
         "xaxis": "x2",
         "y": [
          "agents  ",
          "bayesian  ",
          "markov  ",
          "reinforcement  ",
          "adaptive  "
         ],
         "yaxis": "y2"
        },
        {
         "marker": {
          "color": "#CC79A7"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.36518925428390503,
          0.3677459955215454,
          0.40961992740631104,
          0.5483195185661316,
          0.5501387119293213
         ],
         "xaxis": "x3",
         "y": [
          "exploitation  ",
          "strategy  ",
          "optimal  ",
          "bandit  ",
          "bandits  "
         ],
         "yaxis": "y3"
        },
        {
         "marker": {
          "color": "#E69F00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.4518260955810547,
          0.4585795998573303,
          0.46519267559051514,
          0.48808103799819946,
          0.5920175313949585
         ],
         "xaxis": "x4",
         "y": [
          "classification  ",
          "regularization  ",
          "supervised  ",
          "kernels  ",
          "kernelized  "
         ],
         "yaxis": "y4"
        },
        {
         "marker": {
          "color": "#56B4E9"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.35509413480758667,
          0.3592901825904846,
          0.37973254919052124,
          0.38608163595199585,
          0.44471031427383423
         ],
         "xaxis": "x5",
         "y": [
          "relational  ",
          "rules  ",
          "patterns  ",
          "association  ",
          "discovering  "
         ],
         "yaxis": "y5"
        },
        {
         "marker": {
          "color": "#009E73"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.49077874422073364,
          0.49845215678215027,
          0.5250996351242065,
          0.5348179936408997,
          0.553007960319519
         ],
         "xaxis": "x6",
         "y": [
          "svm  ",
          "classify  ",
          "classifier  ",
          "classification  ",
          "classifiers  "
         ],
         "yaxis": "y6"
        },
        {
         "marker": {
          "color": "#F0E442"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.5155954360961914,
          0.5180850028991699,
          0.5279490947723389,
          0.5854389071464539,
          0.6264727115631104
         ],
         "xaxis": "x7",
         "y": [
          "ensembles  ",
          "ensemble  ",
          "classification  ",
          "classifier  ",
          "classifiers  "
         ],
         "yaxis": "y7"
        },
        {
         "marker": {
          "color": "#D55E00"
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.33401015400886536,
          0.3608083128929138,
          0.3768908381462097,
          0.38023102283477783,
          0.38295304775238037
         ],
         "xaxis": "x8",
         "y": [
          "completion  ",
          "reconstructing  ",
          "sparse  ",
          "matrices  ",
          "matrix  "
         ],
         "yaxis": "y8"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 0",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 1",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 2",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 3",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 4",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 5",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 6",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Topic 7",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "Topic Word Scores",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          0.175
         ],
         "showgrid": true
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.275,
          0.45
         ],
         "showgrid": true
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showgrid": true
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0.825,
          1
         ],
         "showgrid": true
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.6000000000000001,
          1
         ],
         "showgrid": true
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0,
          0.4
         ],
         "showgrid": true
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have a look on the clustered topics with visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           0,
           "generalization | models | markov | algorithms | estimation",
           108
          ],
          [
           1,
           "adaptive | reinforcement | markov | bayesian | agents",
           30
          ],
          [
           2,
           "bandits | bandit | optimal | strategy | exploitation",
           20
          ],
          [
           3,
           "kernelized | kernels | supervised | regularization | classification",
           19
          ],
          [
           4,
           "discovering | association | patterns | rules | relational",
           19
          ],
          [
           5,
           "classifiers | classification | classifier | classify | svm",
           18
          ],
          [
           6,
           "classifiers | classifier | classification | ensemble | ensembles",
           17
          ],
          [
           7,
           "matrix | matrices | sparse | reconstructing | completion",
           15
          ],
          [
           8,
           "supervised | classifiers | classifier | classification | labeling",
           15
          ],
          [
           9,
           "models | sparse | latent | priors | inference",
           14
          ],
          [
           10,
           "quantum | qavb | qrw | superposition | algorithms",
           14
          ],
          [
           11,
           "optimal | optimization | bandwidth | radio | mdp",
           13
          ],
          [
           12,
           "semantic | semantics | analogies | linguistic | analogy",
           13
          ],
          [
           13,
           "labeling | supervised | labels | classification | label",
           12
          ],
          [
           14,
           "svms | svm | classification | regularized | learning",
           10
          ],
          [
           15,
           "lasso | regularization | regularized | sparse | regularizer",
           9
          ],
          [
           16,
           "clustering | cluster | clusters | clustered | algorithms",
           9
          ],
          [
           17,
           "clustering | cluster | algorithms | similarity | organizing",
           8
          ],
          [
           18,
           "forecasting | prediction | forecaster | forecasts | deterministic",
           7
          ],
          [
           19,
           "manifolds | manifold | supervised | dimensionality | classification",
           7
          ],
          [
           20,
           "pca | dimensionality | sparse | clustering | feature",
           5
          ]
         ],
         "hovertemplate": "<b>Topic %{customdata[0]}</b><br>%{customdata[1]}<br>Size: %{customdata[2]}",
         "legendgroup": "",
         "marker": {
          "color": "#B0BEC5",
          "line": {
           "color": "DarkSlateGrey",
           "width": 2
          },
          "size": [
           108,
           30,
           20,
           19,
           19,
           18,
           17,
           15,
           15,
           14,
           14,
           13,
           13,
           12,
           10,
           9,
           9,
           8,
           7,
           7,
           5
          ],
          "sizemode": "area",
          "sizeref": 0.0675,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          5.435866832733154,
          14.159461975097656,
          14.415873527526855,
          -7.158319473266602,
          -1.0780688524246216,
          5.556177139282227,
          5.635486125946045,
          5.599880695343018,
          5.346552848815918,
          5.92204475402832,
          5.331527233123779,
          14.645796775817871,
          -1.193406105041504,
          4.982146263122559,
          -6.877919673919678,
          -7.064516067504883,
          -21.466148376464844,
          -21.18715476989746,
          13.92197322845459,
          -6.729492664337158,
          -6.385440349578857
         ],
         "xaxis": "x",
         "y": [
          13.624838829040527,
          7.609025478363037,
          7.353355407714844,
          -6.187893390655518,
          14.02988052368164,
          12.090099334716797,
          12.393625259399414,
          13.420857429504395,
          12.732773780822754,
          13.935247421264648,
          13.965615272521973,
          7.123987674713135,
          14.145170211791992,
          13.272516250610352,
          -6.361572742462158,
          -6.671586513519287,
          23.256671905517578,
          22.977676391601562,
          7.845877170562744,
          -6.0037713050842285,
          -5.85317850112915
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": -24.68607063293457,
          "y": 9.536424100399017,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": -3.9217021703720096,
          "xshift": 10,
          "y": 26.745172691345214
         }
        ],
        "height": 650,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "legend": {
         "itemsizing": "constant",
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": -3.9217021703720096,
          "x1": -3.9217021703720096,
          "y0": -7.67232449054718,
          "y1": 26.745172691345214
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": -24.68607063293457,
          "x1": 16.84266629219055,
          "y0": 9.536424100399017,
          "y1": 9.536424100399017
         }
        ],
        "sliders": [
         {
          "active": 0,
          "pad": {
           "t": 50
          },
          "steps": [
           {
            "args": [
             {
              "marker.color": [
               [
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 0",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 1",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 2",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 3",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 4",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 5",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 6",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 7",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 8",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 9",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 10",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 11",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 12",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 13",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 14",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 15",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 16",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 17",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 18",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 19",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red"
               ]
              ]
             }
            ],
            "label": "Topic 20",
            "method": "update"
           }
          ]
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Intertopic Distance Map</b>",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 650,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "range": [
          -24.68607063293457,
          16.84266629219055
         ],
         "title": {
          "text": ""
         },
         "visible": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          -7.67232449054718,
          26.745172691345214
         ],
         "title": {
          "text": ""
         },
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot a more detailed overview where we can visualize the documents and topics to see if they were assigned correctly or whether they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "hovertext": [
          "  We analyze and evaluate an online gradient descent algorithm with adaptive\nper-coordinate adjustment of learning rates. Our algorithm can be thought of as\nan online version of batch gradient descent with a diagonal preconditioner.\nThis approach leads to regret bounds that are stronger than those of standard\nonline gradient descent for general online convex optimization problems.\nExperimentally, we show that our algorithm is competitive with state-of-the-art\nalgorithms for large scale machine learning problems.\n",
          "  Given a sample covariance matrix, we examine the problem of maximizing the\nvariance explained by a linear combination of the input variables while\nconstraining the number of nonzero coefficients in this combination. This is\nknown as sparse principal component analysis and has a wide array of\napplications in machine learning and engineering. We formulate a new\nsemidefinite relaxation to this problem and derive a greedy algorithm that\ncomputes a full set of good solutions for all target numbers of non zero\ncoefficients, with total complexity O(n^3), where n is the number of variables.\nWe then use the same relaxation to derive sufficient conditions for global\noptimality of a solution, which can be tested in O(n^3) per pattern. We discuss\napplications in subset selection and sparse recovery and show on artificial\nexamples and biological data that our algorithm does provide globally optimal\nsolutions in many cases.\n",
          "  A natural optimization model that formulates many online resource allocation\nand revenue management problems is the online linear program (LP) in which the\nconstraint matrix is revealed column by column along with the corresponding\nobjective coefficient. In such a model, a decision variable has to be set each\ntime a column is revealed without observing the future inputs and the goal is\nto maximize the overall objective function. In this paper, we provide a\nnear-optimal algorithm for this general class of online problems under the\nassumption of random order of arrival and some mild conditions on the size of\nthe LP right-hand-side input. Specifically, our learning-based algorithm works\nby dynamically updating a threshold price vector at geometric time intervals,\nwhere the dual prices learned from the revealed columns in the previous period\nare used to determine the sequential decisions in the current period. Due to\nthe feature of dynamic learning, the competitiveness of our algorithm improves\nover the past study of the same problem. We also present a worst-case example\nshowing that the performance of our algorithm is near-optimal.\n",
          "  In recent years, predicting the user's next request in web navigation has\nreceived much attention. An information source to be used for dealing with such\nproblem is the left information by the previous web users stored at the web\naccess log on the web servers. Purposed systems for this problem work based on\nthis idea that if a large number of web users request specific pages of a\nwebsite on a given session, it can be concluded that these pages are satisfying\nsimilar information needs, and therefore they are conceptually related. In this\nstudy, a new clustering approach is introduced that employs logical path\nstoring of a website pages as another parameter which is regarded as a\nsimilarity parameter and conceptual relation between web pages. The results of\nsimulation have shown that the proposed approach is more than others precise in\ndetermining the clusters.\n",
          "  A collaborative filtering system recommends to users products that similar\nusers like. Collaborative filtering systems influence purchase decisions, and\nhence have become targets of manipulation by unscrupulous vendors. We provide\ntheoretical and empirical results demonstrating that while common nearest\nneighbor algorithms, which are widely used in commercial systems, can be highly\nsusceptible to manipulation, two classes of collaborative filtering algorithms\nwhich we refer to as linear and asymptotically linear are relatively robust.\nThese results provide guidance for the design of future collaborative filtering\nsystems.\n",
          "  We describe an approach to domain adaptation that is appropriate exactly in\nthe case when one has enough ``target'' data to do slightly better than just\nusing only ``source'' data. Our approach is incredibly simple, easy to\nimplement as a preprocessing step (10 lines of Perl!) and outperforms\nstate-of-the-art approaches on a range of datasets. Moreover, it is trivially\nextended to a multi-domain adaptation problem, where one has data from a\nvariety of different domains.\n",
          "  This paper has been withdrawn by the author. This draft is withdrawn for its\npoor quality in english, unfortunately produced by the author when he was just\nstarting his science route. Look at the ICML version instead:\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\n",
          "  Counting is among the most fundamental operations in computing. For example,\ncounting the pth frequency moment has been a very active area of research, in\ntheoretical computer science, databases, and data mining. When p=1, the task\n(i.e., counting the sum) can be accomplished using a simple counter.\n  Compressed Counting (CC) is proposed for efficiently computing the pth\nfrequency moment of a data stream signal A_t, where 0<p<=2. CC is applicable if\nthe streaming data follow the Turnstile model, with the restriction that at the\ntime t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile\nmodel as a special case. For natural data streams encountered in practice, this\nrestriction is minor.\n  The underly technique for CC is what we call skewed stable random\nprojections, which captures the intuition that, when p=1 a simple counter\nsuffices, and when p = 1+/\\Delta with small \\Delta, the sample complexity of a\ncounter system should be low (continuously as a function of \\Delta). We show at\nsmall \\Delta the sample complexity (number of projections) k = O(1/\\epsilon)\ninstead of O(1/\\epsilon^2).\n  Compressed Counting can serve a basic building block for other tasks in\nstatistics and computing, for example, estimation entropies of data streams,\nparameter estimations using the method of moments and maximum likelihood.\n  Finally, another contribution is an algorithm for approximating the\nlogarithmic norm, \\sum_{i=1}^D\\log A_t[i], and logarithmic distance. The\nlogarithmic distance is useful in machine learning practice with heavy-tailed\ndata.\n",
          "  For a variety of regularized optimization problems in machine learning,\nalgorithms computing the entire solution path have been developed recently.\nMost of these methods are quadratic programs that are parameterized by a single\nparameter, as for example the Support Vector Machine (SVM). Solution path\nalgorithms do not only compute the solution for one particular value of the\nregularization parameter but the entire path of solutions, making the selection\nof an optimal parameter much easier.\n  It has been assumed that these piecewise linear solution paths have only\nlinear complexity, i.e. linearly many bends. We prove that for the support\nvector machine this complexity can be exponential in the number of training\npoints in the worst case. More strongly, we construct a single instance of n\ninput points in d dimensions for an SVM such that at least \\Theta(2^{n/2}) =\n\\Theta(2^d) many distinct subsets of support vectors occur as the\nregularization parameter changes.\n",
          "  This paper generalizes the traditional statistical concept of prediction\nintervals for arbitrary probability density functions in high-dimensional\nfeature spaces by introducing significance level distributions, which provides\ninterval-independent probabilities for continuous random variables. The\nadvantage of the transformation of a probability density function into a\nsignificance level distribution is that it enables one-class classification or\noutlier detection in a direct manner.\n",
          "  We introduce a simple and computationally trivial method for binary\nclassification based on the evaluation of potential functions. We demonstrate\nthat despite the conceptual and computational simplicity of the method its\nperformance can match or exceed that of standard Support Vector Machine\nmethods.\n",
          "  The performance in higher secondary school education in India is a turning\npoint in the academic lives of all students. As this academic performance is\ninfluenced by many factors, it is essential to develop predictive data mining\nmodel for students' performance so as to identify the slow learners and study\nthe influence of the dominant factors on their academic performance. In the\npresent investigation, a survey cum experimental methodology was adopted to\ngenerate a database and it was constructed from a primary and a secondary\nsource. While the primary data was collected from the regular students, the\nsecondary data was gathered from the school and office of the Chief Educational\nOfficer (CEO). A total of 1000 datasets of the year 2006 from five different\nschools in three different districts of Tamilnadu were collected. The raw data\nwas preprocessed in terms of filling up missing values, transforming values in\none form into another and relevant attribute/ variable selection. As a result,\nwe had 772 student records, which were used for CHAID prediction model\nconstruction. A set of prediction rules were extracted from CHIAD prediction\nmodel and the efficiency of the generated CHIAD prediction model was found. The\naccuracy of the present model was compared with other model and it has been\nfound to be satisfactory.\n",
          "  This paper introduces a new approach to solve sensor management problems.\nClassically sensor management problems can be well formalized as\nPartially-Observed Markov Decision Processes (POMPD). The original approach\ndevelopped here consists in deriving the optimal parameterized policy based on\na stochastic gradient estimation. We assume in this work that it is possible to\nlearn the optimal policy off-line (in simulation) using models of the\nenvironement and of the sensor(s). The learned policy can then be used to\nmanage the sensor(s). In order to approximate the gradient in a stochastic\ncontext, we introduce a new method to approximate the gradient, based on\nInfinitesimal Perturbation Approximation (IPA). The effectiveness of this\ngeneral framework is illustrated by the managing of an Electronically Scanned\nArray Radar. First simulations results are finally proposed.\n",
          "  We give a universal kernel that renders all the regular languages linearly\nseparable. We are not able to compute this kernel efficiently and conjecture\nthat it is intractable, but we do have an efficient $\\eps$-approximation.\n",
          "  In this paper, we prove a crucial theorem called Mirroring Theorem which\naffirms that given a collection of samples with enough information in it such\nthat it can be classified into classes and subclasses then (i) There exists a\nmapping which classifies and subclassifies these samples (ii) There exists a\nhierarchical classifier which can be constructed by using Mirroring Neural\nNetworks (MNNs) in combination with a clustering algorithm that can approximate\nthis mapping. Thus, the proof of the Mirroring theorem provides a theoretical\nbasis for the existence and a practical feasibility of constructing\nhierarchical classifiers, given the maps. Our proposed Mirroring Theorem can\nalso be considered as an extension to Kolmogrovs theorem in providing a\nrealistic solution for unsupervised classification. The techniques we develop,\nare general in nature and have led to the construction of learning machines\nwhich are (i) tree like in structure, (ii) modular (iii) with each module\nrunning on a common algorithm (tandem algorithm) and (iv) selfsupervised. We\nhave actually built the architecture, developed the tandem algorithm of such a\nhierarchical classifier and demonstrated it on an example problem.\n",
          "  Neighborhood graphs are gaining popularity as a concise data representation\nin machine learning. However, naive graph construction by pairwise distance\ncalculation takes $O(n^2)$ runtime for $n$ data points and this is\nprohibitively slow for millions of data points. For strings of equal length,\nthe multiple sorting method (Uno, 2008) can construct an $\\epsilon$-neighbor\ngraph in $O(n+m)$ time, where $m$ is the number of $\\epsilon$-neighbor pairs in\nthe data. To introduce this remarkably efficient algorithm to continuous\ndomains such as images, signals and texts, we employ a random projection method\nto convert vectors to strings. Theoretical results are presented to elucidate\nthe trade-off between approximation quality and computation time. Empirical\nresults show the efficiency of our method in comparison to fast nearest\nneighbor alternatives.\n",
          "  We present a general approach for collaborative filtering (CF) using spectral\nregularization to learn linear operators from \"users\" to the \"objects\" they\nrate. Recent low-rank type matrix completion approaches to CF are shown to be\nspecial cases. However, unlike existing regularization based CF methods, our\napproach can be used to also incorporate information such as attributes of the\nusers or the objects -- a limitation of existing regularization based CF\nmethods. We then provide novel representer theorems that we use to develop new\nestimation methods. We provide learning algorithms based on low-rank\ndecompositions, and test them on a standard CF dataset. The experiments\nindicate the advantages of generalizing the existing regularization based CF\nmethods to incorporate related information about users and objects. Finally, we\nshow that certain multi-task learning methods can be also seen as special cases\nof our proposed approach.\n",
          "  The selection of features that are relevant for a prediction or\nclassification problem is an important problem in many domains involving\nhigh-dimensional data. Selecting features helps fighting the curse of\ndimensionality, improving the performances of prediction or classification\nmethods, and interpreting the application. In a nonlinear context, the mutual\ninformation is widely used as relevance criterion for features and sets of\nfeatures. Nevertheless, it suffers from at least three major limitations:\nmutual information estimators depend on smoothing parameters, there is no\ntheoretically justified stopping criterion in the feature selection greedy\nprocedure, and the estimation itself suffers from the curse of dimensionality.\nThis chapter shows how to deal with these problems. The two first ones are\naddressed by using resampling techniques that provide a statistical basis to\nselect the estimator parameters and to stop the search procedure. The third one\nis addressed by modifying the mutual information criterion into a measure of\nhow features are complementary (and not only informative) for the problem at\nhand.\n",
          "  Data from spectrophotometers form vectors of a large number of exploitable\nvariables. Building quantitative models using these variables most often\nrequires using a smaller set of variables than the initial one. Indeed, a too\nlarge number of input variables to a model results in a too large number of\nparameters, leading to overfitting and poor generalization abilities. In this\npaper, we suggest the use of the mutual information measure to select variables\nfrom the initial set. The mutual information measures the information content\nin input variables with respect to the model output, without making any\nassumption on the model that will be used; it is thus suitable for nonlinear\nmodelling. In addition, it leads to the selection of variables among the\ninitial set, and not to linear or nonlinear combinations of them. Without\ndecreasing the model performances compared to other variable projection\nmethods, it allows therefore a greater interpretability of the results.\n",
          "  Images can be segmented by first using a classifier to predict an affinity\ngraph that reflects the degree to which image pixels must be grouped together\nand then partitioning the graph to yield a segmentation. Machine learning has\nbeen applied to the affinity classifier to produce affinity graphs that are\ngood in the sense of minimizing edge misclassification rates. However, this\nerror measure is only indirectly related to the quality of segmentations\nproduced by ultimately partitioning the affinity graph. We present the first\nmachine learning algorithm for training a classifier to produce affinity graphs\nthat are good in the sense of producing segmentations that directly minimize\nthe Rand index, a well known segmentation performance measure. The Rand index\nmeasures segmentation performance by quantifying the classification of the\nconnectivity of image pixel pairs after segmentation. By using the simple graph\npartitioning algorithm of finding the connected components of the thresholded\naffinity graph, we are able to train an affinity classifier to directly\nminimize the Rand index of segmentations resulting from the graph partitioning.\nOur learning algorithm corresponds to the learning of maximin affinities\nbetween image pixel pairs, which are predictive of the pixel-pair connectivity.\n",
          "  File type identification and file type clustering may be difficult tasks that\nhave an increasingly importance in the field of computer and network security.\nClassical methods of file type detection including considering file extensions\nand magic bytes can be easily spoofed. Content-based file type detection is a\nnewer way that is taken into account recently. In this paper, a new\ncontent-based method for the purpose of file type detection and file type\nclustering is proposed that is based on the PCA and neural networks. The\nproposed method has a good accuracy and is fast enough.\n",
          "  Maximum likelihood estimators are often of limited practical use due to the\nintensive computation they require. We propose a family of alternative\nestimators that maximize a stochastic variation of the composite likelihood\nfunction. Each of the estimators resolve the computation-accuracy tradeoff\ndifferently, and taken together they span a continuous spectrum of\ncomputation-accuracy tradeoff resolutions. We prove the consistency of the\nestimators, provide formulas for their asymptotic variance, statistical\nrobustness, and computational complexity. We discuss experimental results in\nthe context of Boltzmann machines and conditional random fields. The\ntheoretical and experimental studies demonstrate the effectiveness of the\nestimators when the computational resources are insufficient. They also\ndemonstrate that in some cases reduced computational complexity is associated\nwith robustness thereby increasing statistical accuracy.\n",
          "  We report a new optimal resolution for the statistical stratification problem\nunder proportional sampling allocation among strata. Consider a finite\npopulation of N units, a random sample of n units selected from this population\nand a number L of strata. Thus, we have to define which units belong to each\nstratum so as to minimize the variance of a total estimator for one desired\nvariable of interest in each stratum,and consequently reduce the overall\nvariance for such quantity. In order to solve this problem, an exact algorithm\nbased on the concept of minimal path in a graph is proposed and assessed.\nComputational results using real data from IBGE (Brazilian Central Statistical\nOffice) are provided.\n",
          "  Catalogs of periodic variable stars contain large numbers of periodic\nlight-curves (photometric time series data from the astrophysics domain).\nSeparating anomalous objects from well-known classes is an important step\ntowards the discovery of new classes of astronomical objects. Most anomaly\ndetection methods for time series data assume either a single continuous time\nseries or a set of time series whose periods are aligned. Light-curve data\nprecludes the use of these methods as the periods of any given pair of\nlight-curves may be out of sync. One may use an existing anomaly detection\nmethod if, prior to similarity calculation, one performs the costly act of\naligning two light-curves, an operation that scales poorly to massive data\nsets. This paper presents PCAD, an unsupervised anomaly detection method for\nlarge sets of unsynchronized periodic time-series data, that outputs a ranked\nlist of both global and local anomalies. It calculates its anomaly score for\neach light-curve in relation to a set of centroids produced by a modified\nk-means clustering algorithm. Our method is able to scale to large data sets\nthrough the use of sampling. We validate our method on both light-curve data\nand other time series data sets. We demonstrate its effectiveness at finding\nknown anomalies, and discuss the effect of sample size and number of centroids\non our results. We compare our method to naive solutions and existing time\nseries anomaly detection methods for unphased data, and show that PCAD's\nreported anomalies are comparable to or better than all other methods. Finally,\nastrophysicists on our team have verified that PCAD finds true anomalies that\nmight be indicative of novel astrophysical phenomena.\n",
          "  Applications in machine learning and data mining require computing pairwise\nLp distances in a data matrix A. For massive high-dimensional data, computing\nall pairwise distances of A can be infeasible. In fact, even storing A or all\npairwise distances of A in the memory may be also infeasible. This paper\nproposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where\np is even) distances into a sum of 2 marginal norms and p-1 ``inner products''\nat different orders. Then we apply normal or sub-Gaussian random projections to\napproximate the resultant ``inner products,'' assuming that the marginal norms\ncan be computed exactly by a linear scan. We propose two strategies for\napplying random projections. The basic projection strategy requires only one\nprojection matrix but it is more difficult to analyze, while the alternative\nprojection strategy requires p-1 projection matrices but its theoretical\nanalysis is much easier. In terms of the accuracy, at least for p=4, the basic\nstrategy is always more accurate than the alternative strategy if the data are\nnon-negative, which is common in reality.\n",
          "  Given a time series of multicomponent measurements x(t), the usual objective\nof nonlinear blind source separation (BSS) is to find a \"source\" time series\ns(t), comprised of statistically independent combinations of the measured\ncomponents. In this paper, the source time series is required to have a density\nfunction in (s,ds/dt)-space that is equal to the product of density functions\nof individual components. This formulation of the BSS problem has a solution\nthat is unique, up to permutations and component-wise transformations.\nSeparability is shown to impose constraints on certain locally invariant\n(scalar) functions of x, which are derived from local higher-order correlations\nof the data's velocity dx/dt. The data are separable if and only if they\nsatisfy these constraints, and, if the constraints are satisfied, the sources\ncan be explicitly constructed from the data. The method is illustrated by using\nit to separate two speech-like sounds recorded with a single microphone.\n",
          "  We propose a novel non-parametric adaptive anomaly detection algorithm for\nhigh dimensional data based on score functions derived from nearest neighbor\ngraphs on $n$-point nominal data. Anomalies are declared whenever the score of\na test sample falls below $\\alpha$, which is supposed to be the desired false\nalarm level. The resulting anomaly detector is shown to be asymptotically\noptimal in that it is uniformly most powerful for the specified false alarm\nlevel, $\\alpha$, for the case when the anomaly density is a mixture of the\nnominal and a known density. Our algorithm is computationally efficient, being\nlinear in dimension and quadratic in data size. It does not require choosing\ncomplicated tuning parameters or function approximation classes and it can\nadapt to local structure such as local change in dimensionality. We demonstrate\nthe algorithm on both artificial and real data sets in high dimensional feature\nspaces.\n",
          "  We present a family of pairwise tournaments reducing $k$-class classification\nto binary classification. These reductions are provably robust against a\nconstant fraction of binary errors. The results improve on the PECOC\nconstruction \\cite{SECOC} with an exponential improvement in computation, from\n$O(k)$ to $O(\\log_2 k)$, and the removal of a square root in the regret\ndependence, matching the best possible computation and regret up to a constant.\n",
          "  The maze traversal problem (finding the shortest distance to the goal from\nany position in a maze) has been an interesting challenge in computational\nintelligence. Recent work has shown that the cellular simultaneous recurrent\nneural network (CSRN) can solve this problem for simple mazes. This thesis\nfocuses on exploiting relevant information about the maze to improve learning\nand decrease the training time for the CSRN to solve mazes. Appropriate\nvariables are identified to create useful clusters using relevant information.\nThe CSRN was next modified to allow for an additional external input. With this\nadditional input, several methods were tested and results show that clustering\nthe mazes improves the overall learning of the traversal problem for the CSRN.\n",
          "  In many physical, statistical, biological and other investigations it is\ndesirable to approximate a system of points by objects of lower dimension\nand/or complexity. For this purpose, Karl Pearson invented principal component\nanalysis in 1901 and found 'lines and planes of closest fit to system of\npoints'. The famous k-means algorithm solves the approximation problem too, but\nby finite sets instead of lines and planes. This chapter gives a brief\npractical introduction into the methods of construction of general principal\nobjects, i.e. objects embedded in the 'middle' of the multidimensional data\nset. As a basis, the unifying framework of mean squared distance approximation\nof finite datasets is selected. Principal graphs and manifolds are constructed\nas generalisations of principal components and k-means principal points. For\nthis purpose, the family of expectation/maximisation algorithms with nearest\ngeneralisations is presented. Construction of principal graphs with controlled\ncomplexity is based on the graph grammar approach.\n",
          "  Two ubiquitous aspects of large-scale data analysis are that the data often\nhave heavy-tailed properties and that diffusion-based or spectral-based methods\nare often used to identify and extract structure of interest. Perhaps\nsurprisingly, popular distribution-independent methods such as those based on\nthe VC dimension fail to provide nontrivial results for even simple learning\nproblems such as binary classification in these two settings. In this paper, we\ndevelop distribution-dependent learning methods that can be used to provide\ndimension-independent sample complexity bounds for the binary classification\nproblem in these two popular settings. In particular, we provide bounds on the\nsample complexity of maximum margin classifiers when the magnitude of the\nentries in the feature vector decays according to a power law and also when\nlearning is performed with the so-called Diffusion Maps kernel. Both of these\nresults rely on bounding the annealed entropy of gap-tolerant classifiers in a\nHilbert space. We provide such a bound, and we demonstrate that our proof\ntechnique generalizes to the case when the margin is measured with respect to\nmore general Banach space norms. The latter result is of potential interest in\ncases where modeling the relationship between data elements as a dot product in\na Hilbert space is too restrictive.\n",
          "  The paper deals with on-line regression settings with signals belonging to a\nBanach lattice. Our algorithms work in a semi-online setting where all the\ninputs are known in advance and outcomes are unknown and given step by step. We\napply the Aggregating Algorithm to construct a prediction method whose\ncumulative loss over all the input vectors is comparable with the cumulative\nloss of any linear functional on the Banach lattice. As a by-product we get an\nalgorithm that takes signals from an arbitrary domain. Its cumulative loss is\ncomparable with the cumulative loss of any predictor function from Besov and\nTriebel-Lizorkin spaces. We describe several applications of our setting.\n",
          "  Pac-Bayes bounds are among the most accurate generalization bounds for\nclassifiers learned from independently and identically distributed (IID) data,\nand it is particularly so for margin classifiers: there have been recent\ncontributions showing how practical these bounds can be either to perform model\nselection (Ambroladze et al., 2007) or even to directly guide the learning of\nlinear classifiers (Germain et al., 2009). However, there are many practical\nsituations where the training data show some dependencies and where the\ntraditional IID assumption does not hold. Stating generalization bounds for\nsuch frameworks is therefore of the utmost interest, both from theoretical and\npractical standpoints. In this work, we propose the first - to the best of our\nknowledge - Pac-Bayes generalization bounds for classifiers trained on data\nexhibiting interdependencies. The approach undertaken to establish our results\nis based on the decomposition of a so-called dependency graph that encodes the\ndependencies within the data, in sets of independent data, thanks to graph\nfractional covers. Our bounds are very general, since being able to find an\nupper bound on the fractional chromatic number of the dependency graph is\nsufficient to get new Pac-Bayes bounds for specific settings. We show how our\nresults can be used to derive bounds for ranking statistics (such as Auc) and\nclassifiers trained on data distributed according to a stationary {\\ss}-mixing\nprocess. In the way, we show how our approach seemlessly allows us to deal with\nU-processes. As a side note, we also provide a Pac-Bayes generalization bound\nfor classifiers learned on data from stationary $\\varphi$-mixing distributions.\n",
          "  A $p$-adic modification of the split-LBG classification method is presented\nin which first clusterings and then cluster centers are computed which locally\nminimise an energy function. The outcome for a fixed dataset is independent of\nthe prime number $p$ with finitely many exceptions. The methods are applied to\nthe construction of $p$-adic classifiers in the context of learning.\n",
          "  We consider a class of fully stochastic and fully distributed algorithms,\nthat we prove to learn equilibria in games.\n  Indeed, we consider a family of stochastic distributed dynamics that we prove\nto converge weakly (in the sense of weak convergence for probabilistic\nprocesses) towards their mean-field limit, i.e an ordinary differential\nequation (ODE) in the general case. We focus then on a class of stochastic\ndynamics where this ODE turns out to be related to multipopulation replicator\ndynamics.\n  Using facts known about convergence of this ODE, we discuss the convergence\nof the initial stochastic dynamics: For general games, there might be\nnon-convergence, but when convergence of the ODE holds, considered stochastic\nalgorithms converge towards Nash equilibria. For games admitting Lyapunov\nfunctions, that we call Lyapunov games, the stochastic dynamics converge. We\nprove that any ordinal potential game, and hence any potential game is a\nLyapunov game, with a multiaffine Lyapunov function. For Lyapunov games with a\nmultiaffine Lyapunov function, we prove that this Lyapunov function is a\nsuper-martingale over the stochastic dynamics. This leads a way to provide\nbounds on their time of convergence by martingale arguments. This applies in\nparticular for many classes of games that have been considered in literature,\nincluding several load balancing game scenarios and congestion games.\n",
          "  Probabilistic graphical models (PGMs) have become a popular tool for\ncomputational analysis of biological data in a variety of domains. But, what\nexactly are they and how do they work? How can we use PGMs to discover patterns\nthat are biologically relevant? And to what extent can PGMs help us formulate\nnew hypotheses that are testable at the bench? This note sketches out some\nanswers and illustrates the main ideas behind the statistical approach to\nbiological pattern discovery.\n",
          "  Grammar inference deals with determining (preferable simple) models/grammars\nconsistent with a set of observations. There is a large body of research on\ngrammar inference within the theory of formal languages. However, there is\nsurprisingly little known on grammar inference for graph grammars. In this\npaper we take a further step in this direction and work within the framework of\nnode label controlled (NLC) graph grammars. Specifically, we characterize,\ngiven a set of disjoint and isomorphic subgraphs of a graph $G$, whether or not\nthere is a NLC graph grammar rule which can generate these subgraphs to obtain\n$G$. This generalizes previous results by assuming that the set of isomorphic\nsubgraphs is disjoint instead of non-touching. This leads naturally to consider\nthe more involved ``non-confluent'' graph grammar rules.\n",
          "  We study the problem of online regression. We prove a theoretical bound on\nthe square loss of Ridge Regression. We do not make any assumptions about input\nvectors or outcomes. We also show that Bayesian Ridge Regression can be thought\nof as an online algorithm competing with all the Gaussian linear experts.\n",
          "  We have proposed a model based upon flocking on a complex network, and then\ndeveloped two clustering algorithms on the basis of it. In the algorithms,\nfirstly a \\textit{k}-nearest neighbor (knn) graph as a weighted and directed\ngraph is produced among all data points in a dataset each of which is regarded\nas an agent who can move in space, and then a time-varying complex network is\ncreated by adding long-range links for each data point. Furthermore, each data\npoint is not only acted by its \\textit{k} nearest neighbors but also \\textit{r}\nlong-range neighbors through fields established in space by them together, so\nit will take a step along the direction of the vector sum of all fields. It is\nmore important that these long-range links provides some hidden information for\neach data point when it moves and at the same time accelerate its speed\nconverging to a center. As they move in space according to the proposed model,\ndata points that belong to the same class are located at a same position\ngradually, whereas those that belong to different classes are away from one\nanother. Consequently, the experimental results have demonstrated that data\npoints in datasets are clustered reasonably and efficiently, and the rates of\nconvergence of clustering algorithms are fast enough. Moreover, the comparison\nwith other algorithms also provides an indication of the effectiveness of the\nproposed approach.\n",
          "  Motivation: Several different threads of research have been proposed for\nmodeling and mining temporal data. On the one hand, approaches such as dynamic\nBayesian networks (DBNs) provide a formal probabilistic basis to model\nrelationships between time-indexed random variables but these models are\nintractable to learn in the general case. On the other, algorithms such as\nfrequent episode mining are scalable to large datasets but do not exhibit the\nrigorous probabilistic interpretations that are the mainstay of the graphical\nmodels literature.\n  Results: We present a unification of these two seemingly diverse threads of\nresearch, by demonstrating how dynamic (discrete) Bayesian networks can be\ninferred from the results of frequent episode mining. This helps bridge the\nmodeling emphasis of the former with the counting emphasis of the latter.\nFirst, we show how, under reasonable assumptions on data characteristics and on\ninfluences of random variables, the optimal DBN structure can be computed using\na greedy, local, algorithm. Next, we connect the optimality of the DBN\nstructure with the notion of fixed-delay episodes and their counts of distinct\noccurrences. Finally, to demonstrate the practical feasibility of our approach,\nwe focus on a specific (but broadly applicable) class of networks, called\nexcitatory networks, and show how the search for the optimal DBN structure can\nbe conducted using just information from frequent episodes. Application on\ndatasets gathered from mathematical models of spiking neurons as well as real\nneuroscience datasets are presented.\n  Availability: Algorithmic implementations, simulator codebases, and datasets\nare available from our website at http://neural-code.cs.vt.edu/dbn\n",
          "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
          "  This paper has been withdrawn due to an error found by Dana Angluin and Lev\nReyzin.\n",
          "  We consider a group of Bayesian agents who try to estimate a state of the\nworld $\\theta$ through interaction on a social network. Each agent $v$\ninitially receives a private measurement of $\\theta$: a number $S_v$ picked\nfrom a Gaussian distribution with mean $\\theta$ and standard deviation one.\nThen, in each discrete time iteration, each reveals its estimate of $\\theta$ to\nits neighbors, and, observing its neighbors' actions, updates its belief using\nBayes' Law.\n  This process aggregates information efficiently, in the sense that all the\nagents converge to the belief that they would have, had they access to all the\nprivate measurements. We show that this process is computationally efficient,\nso that each agent's calculation can be easily carried out. We also show that\non any graph the process converges after at most $2N \\cdot D$ steps, where $N$\nis the number of agents and $D$ is the diameter of the network. Finally, we\nshow that on trees and on distance transitive-graphs the process converges\nafter $D$ steps, and that it preserves privacy, so that agents learn very\nlittle about the private signal of most other agents, despite the efficient\naggregation of information. Our results extend those in an unpublished\nmanuscript of the first and last authors.\n",
          "  This preprint has been withdrawn by the author for revision\n",
          "  Mobile ad hoc networking (MANET) has become an exciting and important\ntechnology in recent years because of the rapid proliferation of wireless\ndevices. MANETs are highly vulnerable to attacks due to the open medium,\ndynamically changing network topology and lack of centralized monitoring point.\nIt is important to search new architecture and mechanisms to protect the\nwireless networks and mobile computing application. IDS analyze the network\nactivities by means of audit data and use patterns of well-known attacks or\nnormal profile to detect potential attacks. There are two methods to analyze:\nmisuse detection and anomaly detection. Misuse detection is not effective\nagainst unknown attacks and therefore, anomaly detection method is used. In\nthis approach, the audit data is collected from each mobile node after\nsimulating the attack and compared with the normal behavior of the system. If\nthere is any deviation from normal behavior then the event is considered as an\nattack. Some of the features of collected audit data may be redundant or\ncontribute little to the detection process. So it is essential to select the\nimportant features to increase the detection rate. This paper focuses on\nimplementing two feature selection methods namely, markov blanket discovery and\ngenetic algorithm. In genetic algorithm, bayesian network is constructed over\nthe collected features and fitness function is calculated. Based on the fitness\nvalue the features are selected. Markov blanket discovery also uses bayesian\nnetwork and the features are selected depending on the minimum description\nlength. During the evaluation phase, the performances of both approaches are\ncompared based on detection rate and false alarm rate.\n",
          "  Dependence strucuture estimation is one of the important problems in machine\nlearning domain and has many applications in different scientific areas. In\nthis paper, a theoretical framework for such estimation based on copula and\ncopula entropy -- the probabilistic theory of representation and measurement of\nstatistical dependence, is proposed. Graphical models are considered as a\nspecial case of the copula framework. A method of the framework for estimating\nmaximum spanning copula is proposed. Due to copula, the method is irrelevant to\nthe properties of individual variables, insensitive to outlier and able to deal\nwith non-Gaussianity. Experiments on both simulated data and real dataset\ndemonstrated the effectiveness of the proposed method.\n",
          "  Jerry Fodor argues that Darwin was wrong about \"natural selection\" because\n(1) it is only a tautology rather than a scientific law that can support\ncounterfactuals (\"If X had happened, Y would have happened\") and because (2)\nonly minds can select. Hence Darwin's analogy with \"artificial selection\" by\nanimal breeders was misleading and evolutionary explanation is nothing but\npost-hoc historical narrative. I argue that Darwin was right on all counts.\n",
          "  Zoonosis refers to the transmission of infectious diseases from animal to\nhuman. The increasing number of zoonosis incidence makes the great losses to\nlives, including humans and animals, and also the impact in social economic. It\nmotivates development of a system that can predict the future number of\nzoonosis occurrences in human. This paper analyses and presents the use of\nSeasonal Autoregressive Integrated Moving Average (SARIMA) method for\ndeveloping a forecasting model that able to support and provide prediction\nnumber of zoonosis human incidence. The dataset for model development was\ncollected on a time series data of human tuberculosis occurrences in United\nStates which comprises of fourteen years of monthly data obtained from a study\npublished by Centers for Disease Control and Prevention (CDC). Several trial\nmodels of SARIMA were compared to obtain the most appropriate model. Then,\ndiagnostic tests were used to determine model validity. The result showed that\nthe SARIMA(9,0,14)(12,1,24)12 is the fittest model. While in the measure of\naccuracy, the selected model achieved 0.062 of Theils U value. It implied that\nthe model was highly accurate and a close fit. It was also indicated the\ncapability of final model to closely represent and made prediction based on the\ntuberculosis historical dataset.\n",
          "  Tree reconstruction methods are often judged by their accuracy, measured by\nhow close they get to the true tree. Yet most reconstruction methods like ML do\nnot explicitly maximize this accuracy. To address this problem, we propose a\nBayesian solution. Given tree samples, we propose finding the tree estimate\nwhich is closest on average to the samples. This ``median'' tree is known as\nthe Bayes estimator (BE). The BE literally maximizes posterior expected\naccuracy, measured in terms of closeness (distance) to the true tree. We\ndiscuss a unified framework of BE trees, focusing especially on tree distances\nwhich are expressible as squared euclidean distances. Notable examples include\nRobinson--Foulds distance, quartet distance, and squared path difference. Using\nsimulated data, we show Bayes estimators can be efficiently computed in\npractice by hill climbing. We also show that Bayes estimators achieve higher\naccuracy, compared to maximum likelihood and neighbor joining.\n",
          "  We consider the problem of reconstructing a discrete-time signal (sequence)\nwith continuous-valued components corrupted by a known memoryless channel. When\nperformance is measured using a per-symbol loss function satisfying mild\nregularity conditions, we develop a sequence of denoisers that, although\nindependent of the distribution of the underlying `clean' sequence, is\nuniversally optimal in the limit of large sequence length. This sequence of\ndenoisers is universal in the sense of performing as well as any sliding window\ndenoising scheme which may be optimized for the underlying clean signal. Our\nresults are initially developed in a ``semi-stochastic'' setting, where the\nnoiseless signal is an unknown individual sequence, and the only source of\nrandomness is due to the channel noise. It is subsequently shown that in the\nfully stochastic setting, where the noiseless sequence is a stationary\nstochastic process, our schemes universally attain optimum performance. The\nproposed schemes draw from nonparametric density estimation techniques and are\npractically implementable. We demonstrate efficacy of the proposed schemes in\ndenoising gray-scale images in the conventional additive white Gaussian noise\nsetting, with additional promising results for less conventional noise\ndistributions.\n",
          "  Several variants of a stochastic local search process for constructing the\nsynaptic weights of an Ising perceptron are studied. In this process, binary\npatterns are sequentially presented to the Ising perceptron and are then\nlearned as the synaptic weight configuration is modified through a chain of\nsingle- or double-weight flips within the compatible weight configuration space\nof the earlier learned patterns. This process is able to reach a storage\ncapacity of $\\alpha \\approx 0.63$ for pattern length N = 101 and $\\alpha\n\\approx 0.41$ for N = 1001. If in addition a relearning process is exploited,\nthe learning performance is further improved to a storage capacity of $\\alpha\n\\approx 0.80$ for N = 101 and $\\alpha \\approx 0.42$ for N=1001. We found that,\nfor a given learning task, the solutions constructed by the random walk\nlearning process are separated by a typical Hamming distance, which decreases\nwith the constraint density $\\alpha$ of the learning task; at a fixed value of\n$\\alpha$, the width of the Hamming distance distributions decreases with $N$.\n",
          "  Growing neuropsychological and neurophysiological evidence suggests that the\nvisual cortex uses parts-based representations to encode, store and retrieve\nrelevant objects. In such a scheme, objects are represented as a set of\nspatially distributed local features, or parts, arranged in stereotypical\nfashion. To encode the local appearance and to represent the relations between\nthe constituent parts, there has to be an appropriate memory structure formed\nby previous experience with visual objects. Here, we propose a model how a\nhierarchical memory structure supporting efficient storage and rapid recall of\nparts-based representations can be established by an experience-driven process\nof self-organization. The process is based on the collaboration of slow\nbidirectional synaptic plasticity and homeostatic unit activity regulation,\nboth running at the top of fast activity dynamics with winner-take-all\ncharacter modulated by an oscillatory rhythm. These neural mechanisms lay down\nthe basis for cooperation and competition between the distributed units and\ntheir synaptic connections. Choosing human face recognition as a test task, we\nshow that, under the condition of open-ended, unsupervised incremental\nlearning, the system is able to form memory traces for individual faces in a\nparts-based fashion. On a lower memory layer the synaptic structure is\ndeveloped to represent local facial features and their interrelations, while\nthe identities of different persons are captured explicitly on a higher layer.\nAn additional property of the resulting representations is the sparseness of\nboth the activity during the recall and the synaptic patterns comprising the\nmemory traces.\n",
          "  This correspondence studies the basic problem of classifications - how to\nevaluate different classifiers. Although the conventional performance indexes,\nsuch as accuracy, are commonly used in classifier selection or evaluation,\ninformation-based criteria, such as mutual information, are becoming popular in\nfeature/model selections. In this work, we propose to assess classifiers in\nterms of normalized mutual information (NI), which is novel and well defined in\na compact range for classifier evaluation. We derive close-form relations of\nnormalized mutual information with respect to accuracy, precision, and recall\nin binary classifications. By exploring the relations among them, we reveal\nthat NI is actually a set of nonlinear functions, with a concordant\npower-exponent form, to each performance index. The relations can also be\nexpressed with respect to precision and recall, or to false alarm and hitting\nrate (recall).\n",
          "  We participated in three of the protein-protein interaction subtasks of the\nSecond BioCreative Challenge: classification of abstracts relevant for\nprotein-protein interaction (IAS), discovery of protein pairs (IPS) and text\npassages characterizing protein interaction (ISS) in full text documents. We\napproached the abstract classification task with a novel, lightweight linear\nmodel inspired by spam-detection techniques, as well as an uncertainty-based\nintegration scheme. We also used a Support Vector Machine and the Singular\nValue Decomposition on the same features for comparison purposes. Our approach\nto the full text subtasks (protein pair and passage identification) includes a\nfeature expansion method based on word-proximity networks. Our approach to the\nabstract classification task (IAS) was among the top submissions for this task\nin terms of the measures of performance used in the challenge evaluation\n(accuracy, F-score and AUC). We also report on a web-tool we produced using our\napproach: the Protein Interaction Abstract Relevance Evaluator (PIARE). Our\napproach to the full text tasks resulted in one of the highest recall rates as\nwell as mean reciprocal rank of correct passages. Our approach to abstract\nclassification shows that a simple linear model, using relatively few features,\nis capable of generalizing and uncovering the conceptual nature of\nprotein-protein interaction from the bibliome. Since the novel approach is\nbased on a very lightweight linear model, it can be easily ported and applied\nto similar problems. In full text problems, the expansion of word features with\nword-proximity networks is shown to be useful, though the need for some\nimprovements is discussed.\n",
          "  Privacy-preserving machine learning algorithms are crucial for the\nincreasingly common setting in which personal data, such as medical or\nfinancial records, are analyzed. We provide general techniques to produce\nprivacy-preserving approximations of classifiers learned via (regularized)\nempirical risk minimization (ERM). These algorithms are private under the\n$\\epsilon$-differential privacy definition due to Dwork et al. (2006). First we\napply the output perturbation ideas of Dwork et al. (2006), to ERM\nclassification. Then we propose a new method, objective perturbation, for\nprivacy-preserving machine learning algorithm design. This method entails\nperturbing the objective function before optimizing over classifiers. If the\nloss and regularizer satisfy certain convexity and differentiability criteria,\nwe prove theoretical results showing that our algorithms preserve privacy, and\nprovide generalization bounds for linear and nonlinear kernels. We further\npresent a privacy-preserving technique for tuning the parameters in general\nmachine learning algorithms, thereby providing end-to-end privacy guarantees\nfor the training process. We apply these results to produce privacy-preserving\nanalogues of regularized logistic regression and support vector machines. We\nobtain encouraging results from evaluating their performance on real\ndemographic and benchmark data sets. Our results show that both theoretically\nand empirically, objective perturbation is superior to the previous\nstate-of-the-art, output perturbation, in managing the inherent tradeoff\nbetween privacy and learning performance.\n",
          "  Regularization by the sum of singular values, also referred to as the trace\nnorm, is a popular technique for estimating low rank rectangular matrices. In\nthis paper, we extend some of the consistency results of the Lasso to provide\nnecessary and sufficient conditions for rank consistency of trace norm\nminimization with the square loss. We also provide an adaptive version that is\nrank consistent even when the necessary condition for the non adaptive version\nis not fulfilled.\n",
          "  In this paper we shall review the common problems associated with Piecewise\nLinear Separation incremental algorithms. This kind of neural models yield poor\nperformances when dealing with some classification problems, due to the\nevolving schemes used to construct the resulting networks. So as to avoid this\nundesirable behavior we shall propose a modification criterion. It is based\nupon the definition of a function which will provide information about the\nquality of the network growth process during the learning phase. This function\nis evaluated periodically as the network structure evolves, and will permit, as\nwe shall show through exhaustive benchmarks, to considerably improve the\nperformance(measured in terms of network complexity and generalization\ncapabilities) offered by the networks generated by these incremental models.\n",
          "  Structured output prediction is an important machine learning problem both in\ntheory and practice, and the max-margin Markov network (\\mcn) is an effective\napproach. All state-of-the-art algorithms for optimizing \\mcn\\ objectives take\nat least $O(1/\\epsilon)$ number of iterations to find an $\\epsilon$ accurate\nsolution. Recent results in structured optimization suggest that faster rates\nare possible by exploiting the structure of the objective function. Towards\nthis end \\citet{Nesterov05} proposed an excessive gap reduction technique based\non Euclidean projections which converges in $O(1/\\sqrt{\\epsilon})$ iterations\non strongly convex functions. Unfortunately when applied to \\mcn s, this\napproach does not admit graphical model factorization which, as in many\nexisting algorithms, is crucial for keeping the cost per iteration tractable.\nIn this paper, we present a new excessive gap reduction technique based on\nBregman projections which admits graphical model factorization naturally, and\nconverges in $O(1/\\sqrt{\\epsilon})$ iterations. Compared with existing\nalgorithms, the convergence rate of our method has better dependence on\n$\\epsilon$ and other parameters of the problem, and can be easily kernelized.\n",
          "  We present a method for learning max-weight matching predictors in bipartite\ngraphs. The method consists of performing maximum a posteriori estimation in\nexponential families with sufficient statistics that encode permutations and\ndata features. Although inference is in general hard, we show that for one very\nrelevant application - web page ranking - exact inference is efficient. For\ngeneral model instances, an appropriate sampler is readily available. Contrary\nto existing max-margin matching models, our approach is statistically\nconsistent and, in addition, experiments with increasing sample sizes indicate\nsuperior improvement over such models. We apply the method to graph matching in\ncomputer vision as well as to a standard benchmark dataset for learning web\npage ranking, in which we obtain state-of-the-art results, in particular\nimproving on max-margin variants. The drawback of this method with respect to\nmax-margin alternatives is its runtime for large graphs, which is comparatively\nhigh.\n",
          "  The proposal is to use clusters, graphs and networks as models in order to\nanalyse the Web structure. Clusters, graphs and networks provide knowledge\nrepresentation and organization. Clusters were generated by co-site analysis.\nThe sample is a set of academic Web sites from the countries belonging to the\nEuropean Union. These clusters are here revisited from the point of view of\ngraph theory and social network analysis. This is a quantitative and structural\nanalysis. In fact, the Internet is a computer network that connects people and\norganizations. Thus we may consider it to be a social network. The set of Web\nacademic sites represents an empirical social network, and is viewed as a\nvirtual community. The network structural properties are here analysed applying\ntogether cluster analysis, graph theory and social network analysis.\n",
          "  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n",
          "  Several recent studies in privacy-preserving learning have considered the\ntrade-off between utility or risk and the level of differential privacy\nguaranteed by mechanisms for statistical query processing. In this paper we\nstudy this trade-off in private Support Vector Machine (SVM) learning. We\npresent two efficient mechanisms, one for the case of finite-dimensional\nfeature mappings and one for potentially infinite-dimensional feature mappings\nwith translation-invariant kernels. For the case of translation-invariant\nkernels, the proposed mechanism minimizes regularized empirical risk in a\nrandom Reproducing Kernel Hilbert Space whose kernel uniformly approximates the\ndesired kernel with high probability. This technique, borrowed from large-scale\nlearning, allows the mechanism to respond with a finite encoding of the\nclassifier, even when the function class is of infinite VC dimension.\nDifferential privacy is established using a proof technique from algorithmic\nstability. Utility--the mechanism's response function is pointwise\nepsilon-close to non-private SVM with probability 1-delta--is proven by\nappealing to the smoothness of regularized empirical risk minimization with\nrespect to small perturbations to the feature mapping. We conclude with a lower\nbound on the optimal differential privacy of the SVM. This negative result\nstates that for any delta, no mechanism can be simultaneously\n(epsilon,delta)-useful and beta-differentially private for small epsilon and\nsmall beta.\n",
          "  We prove that the optimal assignment kernel, proposed recently as an attempt\nto embed labeled graphs and more generally tuples of basic data to a Hilbert\nspace, is in fact not always positive definite.\n",
          "  In many fields where human understanding plays a crucial role, such as\nbioprocesses, the capacity of extracting knowledge from data is of critical\nimportance. Within this framework, fuzzy learning methods, if properly used,\ncan greatly help human experts. Amongst these methods, the aim of orthogonal\ntransformations, which have been proven to be mathematically robust, is to\nbuild rules from a set of training data and to select the most important ones\nby linear regression or rank revealing techniques. The OLS algorithm is a good\nrepresentative of those methods. However, it was originally designed so that it\nonly cared about numerical performance. Thus, we propose some modifications of\nthe original method to take interpretability into account. After recalling the\noriginal algorithm, this paper presents the changes made to the original\nmethod, then discusses some results obtained from benchmark problems. Finally,\nthe algorithm is applied to a real-world fault detection depollution problem.\n",
          "  Two meta-evolutionary optimization strategies described in this paper\naccelerate the convergence of evolutionary programming algorithms while still\nretaining much of their ability to deal with multi-modal problems. The\nstrategies, called directional mutation and recorded step in this paper, can\noperate independently but together they greatly enhance the ability of\nevolutionary programming algorithms to deal with fitness landscapes\ncharacterized by long narrow valleys. The directional mutation aspect of this\ncombined method uses correlated meta-mutation but does not introduce a full\ncovariance matrix. These new methods are thus much more economical in terms of\nstorage for problems with high dimensionality. Additionally, directional\nmutation is rotationally invariant which is a substantial advantage over\nself-adaptive methods which use a single variance per coordinate for problems\nwhere the natural orientation of the problem is not oriented along the axes.\n",
          "  The ability to detect weak distributed activation patterns in networks is\ncritical to several applications, such as identifying the onset of anomalous\nactivity or incipient congestion in the Internet, or faint traces of a\nbiochemical spread by a sensor network. This is a challenging problem since\nweak distributed patterns can be invisible in per node statistics as well as a\nglobal network-wide aggregate. Most prior work considers situations in which\nthe activation/non-activation of each node is statistically independent, but\nthis is unrealistic in many problems. In this paper, we consider structured\npatterns arising from statistical dependencies in the activation process. Our\ncontributions are three-fold. First, we propose a sparsifying transform that\nsuccinctly represents structured activation patterns that conform to a\nhierarchical dependency graph. Second, we establish that the proposed transform\nfacilitates detection of very weak activation patterns that cannot be detected\nwith existing methods. Third, we show that the structure of the hierarchical\ndependency graph governing the activation process, and hence the network\ntransform, can be learnt from very few (logarithmic in network size)\nindependent snapshots of network activity.\n",
          "  In this paper, we propose a spreading activation approach for collaborative\nfiltering (SA-CF). By using the opinion spreading process, the similarity\nbetween any users can be obtained. The algorithm has remarkably higher accuracy\nthan the standard collaborative filtering (CF) using Pearson correlation.\nFurthermore, we introduce a free parameter $\\beta$ to regulate the\ncontributions of objects to user-user correlations. The numerical results\nindicate that decreasing the influence of popular objects can further improve\nthe algorithmic accuracy and personality. We argue that a better algorithm\nshould simultaneously require less computation and generate higher accuracy.\nAccordingly, we further propose an algorithm involving only the top-$N$ similar\nneighbors for each target user, which has both less computational complexity\nand higher algorithmic accuracy.\n",
          "  In this contribution, we propose a generic online (also sometimes called\nadaptive or recursive) version of the Expectation-Maximisation (EM) algorithm\napplicable to latent variable models of independent observations. Compared to\nthe algorithm of Titterington (1984), this approach is more directly connected\nto the usual EM algorithm and does not rely on integration with respect to the\ncomplete data distribution. The resulting algorithm is usually simpler and is\nshown to achieve convergence to the stationary points of the Kullback-Leibler\ndivergence between the marginal distribution of the observation and the model\ndistribution at the optimal rate, i.e., that of the maximum likelihood\nestimator. In addition, the proposed approach is also suitable for conditional\n(or regression) models, as illustrated in the case of the mixture of linear\nregressions model.\n",
          "  Detecting outliers which are grossly different from or inconsistent with the\nremaining dataset is a major challenge in real-world KDD applications. Existing\noutlier detection methods are ineffective on scattered real-world datasets due\nto implicit data patterns and parameter setting issues. We define a novel\n\"Local Distance-based Outlier Factor\" (LDOF) to measure the {outlier-ness} of\nobjects in scattered datasets which addresses these issues. LDOF uses the\nrelative location of an object to its neighbours to determine the degree to\nwhich the object deviates from its neighbourhood. Properties of LDOF are\ntheoretically analysed including LDOF's lower bound and its false-detection\nprobability, as well as parameter settings. In order to facilitate parameter\nsettings in real-world applications, we employ a top-n technique in our outlier\ndetection approach, where only the objects with the highest LDOF values are\nregarded as outliers. Compared to conventional approaches (such as top-n KNN\nand top-n LOF), our method top-n LDOF is more effective at detecting outliers\nin scattered data. It is also easier to set parameters, since its performance\nis relatively stable over a large range of parameter values, as illustrated by\nexperimental results on both real-world and synthetic datasets.\n",
          "  The avalanche quantity of the information developed by mankind has led to\nconcept of automation of knowledge extraction - Data Mining ([1]). This\ndirection is connected with a wide spectrum of problems - from recognition of\nthe fuzzy set to creation of search machines. Important component of Data\nMining is processing of the text information. Such problems lean on concept of\nclassification and clustering ([2]). Classification consists in definition of\nan accessory of some element (text) to one of in advance created classes.\nClustering means splitting a set of elements (texts) on clusters which quantity\nare defined by localization of elements of the given set in vicinities of these\nsome natural centers of these clusters. Realization of a problem of\nclassification initially should lean on the given postulates, basic of which -\nthe aprioristic information on primary set of texts and a measure of affinity\nof elements and classes.\n",
          "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
          "  We study the tracking problem, namely, estimating the hidden state of an\nobject over time, from unreliable and noisy measurements. The standard\nframework for the tracking problem is the generative framework, which is the\nbasis of solutions such as the Bayesian algorithm and its approximation, the\nparticle filters. However, the problem with these solutions is that they are\nvery sensitive to model mismatches. In this paper, motivated by online\nlearning, we introduce a new framework -- an {\\em explanatory} framework -- for\ntracking. We provide an efficient tracking algorithm for this framework. We\nprovide experimental results comparing our algorithm to the Bayesian algorithm\non simulated data. Our experiments show that when there are slight model\nmismatches, our algorithm vastly outperforms the Bayesian algorithm.\n",
          "  We provide a sound and consistent foundation for the use of \\emph{nonrandom}\nexploration data in \"contextual bandit\" or \"partially labeled\" settings where\nonly the value of a chosen action is learned.\n  The primary challenge in a variety of settings is that the exploration\npolicy, in which \"offline\" data is logged, is not explicitly known. Prior\nsolutions here require either control of the actions during the learning\nprocess, recorded random exploration, or actions chosen obliviously in a\nrepeated manner. The techniques reported here lift these restrictions, allowing\nthe learning of a policy for choosing actions given features from historical\ndata where no randomization occurred or was logged.\n  We empirically verify our solution on two reasonably sized sets of real-world\ndata obtained from Yahoo!.\n",
          "  In this paper we apply computer learning methods to diagnosing ovarian cancer\nusing the level of the standard biomarker CA125 in conjunction with information\nprovided by mass-spectrometry. We are working with a new data set collected\nover a period of 7 years. Using the level of CA125 and mass-spectrometry peaks,\nour algorithm gives probability predictions for the disease. To estimate\nclassification accuracy we convert probability predictions into strict\npredictions. Our algorithm makes fewer errors than almost any linear\ncombination of the CA125 level and one peak's intensity (taken on the log\nscale). To check the power of our algorithm we use it to test the hypothesis\nthat CA125 and the peaks do not contain useful information for the prediction\nof the disease at a particular time before the diagnosis. Our algorithm\nproduces $p$-values that are better than those produced by the algorithm that\nhas been previously applied to this data set. Our conclusion is that the\nproposed algorithm is more reliable for prediction on new data.\n",
          "  After building a classifier with modern tools of machine learning we\ntypically have a black box at hand that is able to predict well for unseen\ndata. Thus, we get an answer to the question what is the most likely label of a\ngiven unseen data point. However, most methods will provide no answer why the\nmodel predicted the particular label for a single instance and what features\nwere most influential for that particular instance. The only method that is\ncurrently able to provide such explanations are decision trees. This paper\nproposes a procedure which (based on a set of assumptions) allows to explain\nthe decisions of any classification method.\n",
          "  The LETOR website contains three information retrieval datasets used as a\nbenchmark for testing machine learning ideas for ranking. Algorithms\nparticipating in the challenge are required to assign score values to search\nresults for a collection of queries, and are measured using standard IR ranking\nmeasures (NDCG, precision, MAP) that depend only the relative score-induced\norder of the results. Similarly to many of the ideas proposed in the\nparticipating algorithms, we train a linear classifier. In contrast with other\nparticipating algorithms, we define an additional free variable (intercept, or\nbenchmark) for each query. This allows expressing the fact that results for\ndifferent queries are incomparable for the purpose of determining relevance.\nThe cost of this idea is the addition of relatively few nuisance parameters.\nOur approach is simple, and we used a standard logistic regression library to\ntest it. The results beat the reported participating algorithms. Hence, it\nseems promising to combine our approach with other more complex ideas.\n",
          "  There has been a lot of recent work on Bayesian methods for reinforcement\nlearning exhibiting near-optimal online performance. The main obstacle facing\nsuch methods is that in most problems of interest, the optimal solution\ninvolves planning in an infinitely large tree. However, it is possible to\nobtain stochastic lower and upper bounds on the value of each tree node. This\nenables us to use stochastic branch and bound algorithms to search the tree\nefficiently. This paper proposes two such algorithms and examines their\ncomplexity in this setting.\n",
          "  The cross-entropy method is a simple but efficient method for global\noptimization. In this paper we provide two online variants of the basic CEM,\ntogether with a proof of convergence.\n",
          "  Unlike static documents, version controlled documents are continuously edited\nby one or more authors. Such collaborative revision process makes traditional\nmodeling and visualization techniques inappropriate. In this paper we propose a\nnew representation based on local space-time smoothing that captures important\nrevision patterns. We demonstrate the applicability of our framework using\nexperiments on synthetic and real-world data.\n",
          "  This paper describes an efficient reduction of the learning problem of\nranking to binary classification. The reduction guarantees an average pairwise\nmisranking regret of at most that of the binary classifier regret, improving a\nrecent result of Balcan et al which only guarantees a factor of 2. Moreover,\nour reduction applies to a broader class of ranking loss functions, admits a\nsimpler proof, and the expected running time complexity of our algorithm in\nterms of number of calls to a classifier or preference function is improved\nfrom $\\Omega(n^2)$ to $O(n \\log n)$. In addition, when the top $k$ ranked\nelements only are required ($k \\ll n$), as in many applications in information\nextraction or search engines, the time complexity of our algorithm can be\nfurther reduced to $O(k \\log k + n)$. Our reduction and algorithm are thus\npractical for realistic applications where the number of points to rank exceeds\nseveral thousands. Much of our results also extend beyond the bipartite case\npreviously studied.\n  Our rediction is a randomized one. To complement our result, we also derive\nlower bounds on any deterministic reduction from binary (preference)\nclassification to ranking, implying that our use of a randomized reduction is\nessentially necessary for the guarantees we provide.\n",
          "  We explore the striking mathematical connections that exist between market\nscoring rules, cost function based prediction markets, and no-regret learning.\nWe show that any cost function based prediction market can be interpreted as an\nalgorithm for the commonly studied problem of learning from expert advice by\nequating trades made in the market with losses observed by the learning\nalgorithm. If the loss of the market organizer is bounded, this bound can be\nused to derive an O(sqrt(T)) regret bound for the corresponding learning\nalgorithm. We then show that the class of markets with convex cost functions\nexactly corresponds to the class of Follow the Regularized Leader learning\nalgorithms, with the choice of a cost function in the market corresponding to\nthe choice of a regularizer in the learning problem. Finally, we show an\nequivalence between market scoring rules and prediction markets with convex\ncost functions. This implies that market scoring rules can also be interpreted\nnaturally as Follow the Regularized Leader algorithms, and may be of\nindependent interest. These connections provide new insight into how it is that\ncommonly studied markets, such as the Logarithmic Market Scoring Rule, can\naggregate opinions into accurate estimates of the likelihood of future events.\n",
          "  This paper proposes an unsupervised learning technique by using Multi-layer\nMirroring Neural Network and Forgy's clustering algorithm. Multi-layer\nMirroring Neural Network is a neural network that can be trained with\ngeneralized data inputs (different categories of image patterns) to perform\nnon-linear dimensionality reduction and the resultant low-dimensional code is\nused for unsupervised pattern classification using Forgy's algorithm. By\nadapting the non-linear activation function (modified sigmoidal function) and\ninitializing the weights and bias terms to small random values, mirroring of\nthe input pattern is initiated. In training, the weights and bias terms are\nchanged in such a way that the input presented is reproduced at the output by\nback propagating the error. The mirroring neural network is capable of reducing\nthe input vector to a great degree (approximately 1/30th the original size) and\nalso able to reconstruct the input pattern at the output layer from this\nreduced code units. The feature set (output of central hidden layer) extracted\nfrom this network is fed to Forgy's algorithm, which classify input data\npatterns into distinguishable classes. In the implementation of Forgy's\nalgorithm, initial seed points are selected in such a way that they are distant\nenough to be perfectly grouped into different categories. Thus a new method of\nunsupervised learning is formulated and demonstrated in this paper. This method\ngave impressive results when applied to classification of different image\npatterns.\n",
          "  We propose a novel model for nonlinear dimension reduction motivated by the\nprobabilistic formulation of principal component analysis. Nonlinearity is\nachieved by specifying different transformation matrices at different locations\nof the latent space and smoothing the transformation using a Markov random\nfield type prior. The computation is made feasible by the recent advances in\nsampling from von Mises-Fisher distributions.\n",
          "  The remarkable results of Foster and Vohra was a starting point for a series\nof papers which show that any sequence of outcomes can be learned (with no\nprior knowledge) using some universal randomized forecasting algorithm and\nforecast-dependent checking rules. We show that for the class of all\ncomputationally efficient outcome-forecast-based checking rules, this property\nis violated. Moreover, we present a probabilistic algorithm generating with\nprobability close to one a sequence with a subsequence which simultaneously\nmiscalibrates all partially weakly computable randomized forecasting\nalgorithms. %subsequences non-learnable by each randomized algorithm.\n  According to the Dawid's prequential framework we consider partial recursive\nrandomized algorithms.\n",
          "  This article describes a new type of artificial neuron, called the authors\n\"cyberneuron\". Unlike classical models of artificial neurons, this type of\nneuron used table substitution instead of the operation of multiplication of\ninput values for the weights. This allowed to significantly increase the\ninformation capacity of a single neuron, but also greatly simplify the process\nof learning. Considered an example of the use of \"cyberneuron\" with the task of\ndetecting computer viruses.\n",
          "  Recursive Neural Networks are non-linear adaptive models that are able to\nlearn deep structured information. However, these models have not yet been\nbroadly accepted. This fact is mainly due to its inherent complexity. In\nparticular, not only for being extremely complex information processing models,\nbut also because of a computational expensive learning phase. The most popular\ntraining method for these models is back-propagation through the structure.\nThis algorithm has been revealed not to be the most appropriate for structured\nprocessing due to problems of convergence, while more sophisticated training\nmethods enhance the speed of convergence at the expense of increasing\nsignificantly the computational cost. In this paper, we firstly perform an\nanalysis of the underlying principles behind these models aimed at\nunderstanding their computational power. Secondly, we propose an approximate\nsecond order stochastic learning algorithm. The proposed algorithm dynamically\nadapts the learning rate throughout the training phase of the network without\nincurring excessively expensive computational effort. The algorithm operates in\nboth on-line and batch modes. Furthermore, the resulting learning scheme is\nrobust against the vanishing gradients problem. The advantages of the proposed\nalgorithm are demonstrated with a real-world application example.\n",
          "  Regularized risk minimization with the binary hinge loss and its variants\nlies at the heart of many machine learning problems. Bundle methods for\nregularized risk minimization (BMRM) and the closely related SVMStruct are\nconsidered the best general purpose solvers to tackle this problem. It was\nrecently shown that BMRM requires $O(1/\\epsilon)$ iterations to converge to an\n$\\epsilon$ accurate solution. In the first part of the paper we use the\nHadamard matrix to construct a regularized risk minimization problem and show\nthat these rates cannot be improved. We then show how one can exploit the\nstructure of the objective function to devise an algorithm for the binary hinge\nloss which converges to an $\\epsilon$ accurate solution in\n$O(1/\\sqrt{\\epsilon})$ iterations.\n",
          "  We consider the problem of estimating the parameters of a Gaussian or binary\ndistribution in such a way that the resulting undirected graphical model is\nsparse. Our approach is to solve a maximum likelihood problem with an added\nl_1-norm penalty term. The problem as formulated is convex but the memory\nrequirements and complexity of existing interior point methods are prohibitive\nfor problems with more than tens of nodes. We present two new algorithms for\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\nalgorithm uses block coordinate descent, and can be interpreted as recursive\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\norder method, yields a complexity estimate with a better dependence on problem\nsize than existing interior point methods. Using a log determinant relaxation\nof the log partition function (Wainwright & Jordan (2006)), we show that these\nsame algorithms can be used to solve an approximate sparse maximum likelihood\nproblem for the binary case. We test our algorithms on synthetic data, as well\nas on gene expression and senate voting records data.\n",
          "  The use of computational intelligence techniques for classification has been\nused in numerous applications. This paper compares the use of a Multi Layer\nPerceptron Neural Network and a new Relational Network on classifying the HIV\nstatus of women at ante-natal clinics. The paper discusses the architecture of\nthe relational network and its merits compared to a neural network and most\nother computational intelligence classifiers. Results gathered from the study\nindicate comparable classification accuracies as well as revealed relationships\nbetween data features in the classification data. Much higher classification\naccuracies are recommended for future research in the area of HIV\nclassification as well as missing data estimation.\n",
          "  Combining the mutual information criterion with a forward feature selection\nstrategy offers a good trade-off between optimality of the selected feature\nsubset and computation time. However, it requires to set the parameter(s) of\nthe mutual information estimator and to determine when to halt the forward\nprocedure. These two choices are difficult to make because, as the\ndimensionality of the subset increases, the estimation of the mutual\ninformation becomes less and less reliable. This paper proposes to use\nresampling methods, a K-fold cross-validation and the permutation test, to\naddress both issues. The resampling methods bring information about the\nvariance of the estimator, information which can then be used to automatically\nset the parameter and to calculate a threshold to stop the forward procedure.\nThe procedure is illustrated on a synthetic dataset as well as on real-world\nexamples.\n",
          "  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers\nfor an offline signature system. From the signature images, global and local\nfeatures are extracted and the signatures are verified with the help of\nGaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.\nSVM is used to fuse matching scores of these matchers. Finally, recognition of\nquery signatures is done by comparing it with all signatures of the database.\nThe proposed system is tested on a signature database contains 5400 offline\nsignatures of 600 individuals and the results are found to be promising.\n",
          "  We analyze the generalization performance of a student in a model composed of\nnonlinear perceptrons: a true teacher, ensemble teachers, and the student. We\ncalculate the generalization error of the student analytically or numerically\nusing statistical mechanics in the framework of on-line learning. We treat two\nwell-known learning rules: Hebbian learning and perceptron learning. As a\nresult, it is proven that the nonlinear model shows qualitatively different\nbehaviors from the linear model. Moreover, it is clarified that Hebbian\nlearning and perceptron learning show qualitatively different behaviors from\neach other. In Hebbian learning, we can analytically obtain the solutions. In\nthis case, the generalization error monotonically decreases. The steady value\nof the generalization error is independent of the learning rate. The larger the\nnumber of teachers is and the more variety the ensemble teachers have, the\nsmaller the generalization error is. In perceptron learning, we have to\nnumerically obtain the solutions. In this case, the dynamical behaviors of the\ngeneralization error are non-monotonic. The smaller the learning rate is, the\nlarger the number of teachers is; and the more variety the ensemble teachers\nhave, the smaller the minimum value of the generalization error is.\n",
          "  We present a unified framework to study graph kernels, special cases of which\ninclude the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05},\nmarginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},\nand geometric kernel on graphs \\citep{Gaertner02}. Through extensions of linear\nalgebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\nSylvester equation, we construct an algorithm that improves the time complexity\nof kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,\nconjugate gradient solvers or fixed-point iterations bring our algorithm into\nthe sub-cubic domain. Experiments on graphs from bioinformatics and other\napplication domains show that it is often more than a thousand times faster\nthan previous approaches. We then explore connections between diffusion kernels\n\\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels,\nand use these connections to propose new graph kernels. Finally, we show that\nrational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized\nto graphs reduce to the random walk graph kernel.\n",
          "  Networks are ubiquitous in science and have become a focal point for\ndiscussion in everyday life. Formal statistical models for the analysis of\nnetwork data have emerged as a major topic of interest in diverse areas of\nstudy, and most of these involve a form of graphical representation.\nProbability models on graphs date back to 1959. Along with empirical studies in\nsocial psychology and sociology from the 1960s, these early works generated an\nactive network community and a substantial literature in the 1970s. This effort\nmoved into the statistical literature in the late 1970s and 1980s, and the past\ndecade has seen a burgeoning network literature in statistical physics and\ncomputer science. The growth of the World Wide Web and the emergence of online\nnetworking communities such as Facebook, MySpace, and LinkedIn, and a host of\nmore specialized professional network communities has intensified interest in\nthe study of networks and network data. Our goal in this review is to provide\nthe reader with an entry point to this burgeoning literature. We begin with an\noverview of the historical development of statistical network modeling and then\nwe introduce a number of examples that have been studied in the network\nliterature. Our subsequent discussion focuses on a number of prominent static\nand dynamic network models and their interconnections. We emphasize formal\nmodel descriptions, and pay special attention to the interpretation of\nparameters and their estimation. We end with a description of some open\nproblems and challenges for machine learning and statistics.\n",
          "  The Bayesian framework is a well-studied and successful framework for\ninductive reasoning, which includes hypothesis testing and confirmation,\nparameter estimation, sequence prediction, classification, and regression. But\nstandard statistical guidelines for choosing the model class and prior are not\nalways available or fail, in particular in complex situations. Solomonoff\ncompleted the Bayesian framework by providing a rigorous, unique, formal, and\nuniversal choice for the model class and the prior. We discuss in breadth how\nand in which sense universal (non-i.i.d.) sequence prediction solves various\n(philosophical) problems of traditional Bayesian sequence prediction. We show\nthat Solomonoff's model possesses many desirable properties: Strong total and\nweak instantaneous bounds, and in contrast to most classical continuous prior\ndensities has no zero p(oste)rior problem, i.e. can confirm universal\nhypotheses, is reparametrization and regrouping invariant, and avoids the\nold-evidence and updating problem. It even performs well (actually better) in\nnon-computable environments.\n",
          "  We introduce a new online convex optimization algorithm that adaptively\nchooses its regularization function based on the loss functions observed so\nfar. This is in contrast to previous algorithms that use a fixed regularization\nfunction such as L2-squared, and modify it only via a single time-dependent\nparameter. Our algorithm's regret bounds are worst-case optimal, and for\ncertain realistic classes of loss functions they are much better than existing\nbounds. These bounds are problem-dependent, which means they can exploit the\nstructure of the actual problem instance. Critically, however, our algorithm\ndoes not need to know this structure in advance. Rather, we prove competitive\nguarantees that show the algorithm provides a bound within a constant factor of\nthe best possible bound (of a certain functional form) in hindsight.\n",
          "  This paper I assume that in humans the creation of knowledge depends on a\ndiscrete time, or stage, sequential decision-making process subjected to a\nstochastic, information transmitting environment. For each time-stage, this\nenvironment randomly transmits Shannon type information-packets to the\ndecision-maker, who examines each of them for relevancy and then determines his\noptimal choices. Using this set of relevant information-packets, the\ndecision-maker adapts, over time, to the stochastic nature of his environment,\nand optimizes the subjective expected rate-of-growth of knowledge. The\ndecision-maker's optimal actions, lead to a decision function that involves,\nover time, his view of the subjective entropy of the environmental process and\nother important parameters at each time-stage of the process. Using this model\nof human behavior, one could create psychometric experiments using computer\nsimulation and real decision-makers, to play programmed games to measure the\nresulting human performance.\n",
          "  Cooperative decision making is a vision of future network management and\ncontrol. Distributed connection preemption is an important example where nodes\ncan make intelligent decisions on allocating resources and controlling traffic\nflows for multi-class service networks. A challenge is that nodal decisions are\nspatially dependent as traffic flows trespass multiple nodes in a network.\nHence the performance-complexity trade-off becomes important, i.e., how\naccurate decisions are versus how much information is exchanged among nodes.\nConnection preemption is known to be NP-complete. Centralized preemption is\noptimal but computationally intractable. Decentralized preemption is\ncomputationally efficient but may result in a poor performance. This work\ninvestigates distributed preemption where nodes decide whether and which flows\nto preempt using only local information exchange with neighbors. We develop,\nbased on the probabilistic graphical models, a near-optimal distributed\nalgorithm. The algorithm is used by each node to make collectively near-optimal\npreemption decisions. We study trade-offs between near-optimal performance and\ncomplexity that corresponds to the amount of information-exchange of the\ndistributed algorithm. The algorithm is validated by both analysis and\nsimulation.\n",
          "  This paper presents the formulation of a combinatorial optimization problem\nwith the following characteristics: i.the search space is the power set of a\nfinite set structured as a Boolean lattice; ii.the cost function forms a\nU-shaped curve when applied to any lattice chain. This formulation applies for\nfeature selection in the context of pattern recognition. The known approaches\nfor this problem are branch-and-bound algorithms and heuristics, that explore\npartially the search space. Branch-and-bound algorithms are equivalent to the\nfull search, while heuristics are not. This paper presents a branch-and-bound\nalgorithm that differs from the others known by exploring the lattice structure\nand the U-shaped chain curves of the search space. The main contribution of\nthis paper is the architecture of this algorithm that is based on the\nrepresentation and exploration of the search space by new lattice properties\nproven here. Several experiments, with well known public data, indicate the\nsuperiority of the proposed method to SFFS, which is a popular heuristic that\ngives good results in very short computational time. In all experiments, the\nproposed method got better or equal results in similar or even smaller\ncomputational time.\n",
          "  This paper presents a new hybrid learning algorithm for unsupervised\nclassification tasks. We combined Fuzzy c-means learning algorithm and a\nsupervised version of Minimerror to develop a hybrid incremental strategy\nallowing unsupervised classifications. We applied this new approach to a\nreal-world database in order to know if the information contained in unlabeled\nfeatures of a Geographic Information System (GIS), allows to well classify it.\nFinally, we compared our results to a classical supervised classification\nobtained by a multilayer perceptron.\n",
          "  On-line learning of a hierarchical learning model is studied by a method from\nstatistical mechanics. In our model a student of a simple perceptron learns\nfrom not a true teacher directly, but ensemble teachers who learn from the true\nteacher with a perceptron learning rule. Since the true teacher and the\nensemble teachers are expressed as non-monotonic perceptron and simple ones,\nrespectively, the ensemble teachers go around the unlearnable true teacher with\nthe distance between them fixed in an asymptotic steady state. The\ngeneralization performance of the student is shown to exceed that of the\nensemble teachers in a transient state, as was shown in similar\nensemble-teachers models. Further, it is found that moving the ensemble\nteachers even in the steady state, in contrast to the fixed ensemble teachers,\nis efficient for the performance of the student.\n",
          "  For a wide variety of regularization methods, algorithms computing the entire\nsolution path have been developed recently. Solution path algorithms do not\nonly compute the solution for one particular value of the regularization\nparameter but the entire path of solutions, making the selection of an optimal\nparameter much easier. Most of the currently used algorithms are not robust in\nthe sense that they cannot deal with general or degenerate input. Here we\npresent a new robust, generic method for parametric quadratic programming. Our\nalgorithm directly applies to nearly all machine learning applications, where\nso far every application required its own different algorithm.\n  We illustrate the usefulness of our method by applying it to a very low rank\nproblem which could not be solved by existing path tracking methods, namely to\ncompute part-worth values in choice based conjoint analysis, a popular\ntechnique from market research to estimate consumers preferences on a class of\nparameterized options.\n",
          "  This article describes an approach to designing a distributed and modular\nneural classifier. This approach introduces a new hierarchical clustering that\nenables one to determine reliable regions in the representation space by\nexploiting supervised information. A multilayer perceptron is then associated\nwith each of these detected clusters and charged with recognizing elements of\nthe associated cluster while rejecting all others. The obtained global\nclassifier is comprised of a set of cooperating neural networks and completed\nby a K-nearest neighbor classifier charged with treating elements rejected by\nall the neural networks. Experimental results for the handwritten digit\nrecognition problem and comparison with neural and statistical nonmodular\nclassifiers are given.\n",
          "  Approximate message passing algorithms proved to be extremely effective in\nreconstructing sparse signals from a small number of incoherent linear\nmeasurements. Extensive numerical experiments further showed that their\ndynamics is accurately tracked by a simple one-dimensional iteration termed\nstate evolution. In this paper we provide the first rigorous foundation to\nstate evolution. We prove that indeed it holds asymptotically in the large\nsystem limit for sensing matrices with independent and identically distributed\ngaussian entries.\n  While our focus is on message passing algorithms for compressed sensing, the\nanalysis extends beyond this setting, to a general class of algorithms on dense\ngraphs. In this context, state evolution plays the role that density evolution\nhas for sparse graphs.\n  The proof technique is fundamentally different from the standard approach to\ndensity evolution, in that it copes with large number of short loops in the\nunderlying factor graph. It relies instead on a conditioning technique recently\ndeveloped by Erwin Bolthausen in the context of spin glass theory.\n",
          "  We present three related ways of using Transfer Learning to improve feature\nselection. The three methods address different problems, and hence share\ndifferent kinds of information between tasks or feature classes, but all three\nare based on the information theoretic Minimum Description Length (MDL)\nprinciple and share the same underlying Bayesian interpretation. The first\nmethod, MIC, applies when predictive models are to be built simultaneously for\nmultiple tasks (``simultaneous transfer'') that share the same set of features.\nMIC allows each feature to be added to none, some, or all of the task models\nand is most beneficial for selecting a small set of predictive features from a\nlarge pool of features, as is common in genomic and biological datasets. Our\nsecond method, TPC (Three Part Coding), uses a similar methodology for the case\nwhen the features can be divided into feature classes. Our third method,\nTransfer-TPC, addresses the ``sequential transfer'' problem in which the task\nto which we want to transfer knowledge may not be known in advance and may have\ndifferent amounts of data than the other tasks. Transfer-TPC is most beneficial\nwhen we want to transfer knowledge between tasks which have unequal amounts of\nlabeled data, for example the data for disambiguating the senses of different\nverbs. We demonstrate the effectiveness of these approaches with experimental\nresults on real world data pertaining to genomics and to Word Sense\nDisambiguation (WSD).\n",
          "  This paper has been retracted.\n",
          "  We consider privacy preserving decision tree induction via ID3 in the case\nwhere the training data is horizontally or vertically distributed. Furthermore,\nwe consider the same problem in the case where the data is both horizontally\nand vertically distributed, a situation we refer to as grid partitioned data.\nWe give an algorithm for privacy preserving ID3 over horizontally partitioned\ndata involving more than two parties. For grid partitioned data, we discuss two\ndifferent evaluation methods for preserving privacy ID3, namely, first merging\nhorizontally and developing vertically or first merging vertically and next\ndeveloping horizontally. Next to introducing privacy preserving data mining\nover grid-partitioned data, the main contribution of this paper is that we\nshow, by means of a complexity analysis that the former evaluation method is\nthe more efficient.\n",
          "  We identify the classical Perceptron algorithm with margin as a member of a\nbroader family of large margin classifiers which we collectively call the\nMargitron. The Margitron, (despite its) sharing the same update rule with the\nPerceptron, is shown in an incremental setting to converge in a finite number\nof updates to solutions possessing any desirable fraction of the maximum\nmargin. Experiments comparing the Margitron with decomposition SVMs on tasks\ninvolving linear kernels and 2-norm soft margin are also reported.\n",
          "  This paper introduces a model based upon games on an evolving network, and\ndevelops three clustering algorithms according to it. In the clustering\nalgorithms, data points for clustering are regarded as players who can make\ndecisions in games. On the network describing relationships among data points,\nan edge-removing-and-rewiring (ERR) function is employed to explore in a\nneighborhood of a data point, which removes edges connecting to neighbors with\nsmall payoffs, and creates new edges to neighbors with larger payoffs. As such,\nthe connections among data points vary over time. During the evolution of\nnetwork, some strategies are spread in the network. As a consequence, clusters\nare formed automatically, in which data points with the same evolutionarily\nstable strategy are collected as a cluster, so the number of evolutionarily\nstable strategies indicates the number of clusters. Moreover, the experimental\nresults have demonstrated that data points in datasets are clustered reasonably\nand efficiently, and the comparison with other algorithms also provides an\nindication of the effectiveness of the proposed algorithms.\n",
          "  Multi-class classification is one of the most important tasks in machine\nlearning. In this paper we consider two online multi-class classification\nproblems: classification by a linear model and by a kernelized model. The\nquality of predictions is measured by the Brier loss function. We suggest two\ncomputationally efficient algorithms to work with these problems and prove\ntheoretical guarantees on their losses. We kernelize one of the algorithms and\nprove theoretical guarantees on its loss. We perform experiments and compare\nour algorithms with logistic regression.\n",
          "  We propose a unified framework for deriving and studying soft-in-soft-out\n(SISO) detection in interference channels using the concept of variational\ninference. The proposed framework may be used in multiple-access interference\n(MAI), inter-symbol interference (ISI), and multiple-input multiple-outpu\n(MIMO) channels. Without loss of generality, we will focus our attention on\nturbo multiuser detection, to facilitate a more concrete discussion. It is\nshown that, with some loss of optimality, variational inference avoids the\nexponential complexity of a posteriori probability (APP) detection by\noptimizing a closely-related, but much more manageable, objective function\ncalled variational free energy. In addition to its systematic appeal, there are\nseveral other advantages to this viewpoint. First of all, it provides unified\nand rigorous justifications for numerous detectors that were proposed on\nradically different grounds, and facilitates convenient joint detection and\ndecoding (utilizing the turbo principle) when error-control codes are\nincorporated. Secondly, efficient joint parameter estimation and data detection\nis possible via the variational expectation maximization (EM) algorithm, such\nthat the detrimental effect of inaccurate channel knowledge at the receiver may\nbe dealt with systematically. We are also able to extend BPSK-based SISO\ndetection schemes to arbitrary square QAM constellations in a rigorous manner\nusing a variational argument.\n",
          "  Models for near-rigid shape matching are typically based on distance-related\nfeatures, in order to infer matches that are consistent with the isometric\nassumption. However, real shapes from image datasets, even when expected to be\nrelated by \"almost isometric\" transformations, are actually subject not only to\nnoise but also, to some limited degree, to variations in appearance and scale.\nIn this paper, we introduce a graphical model that parameterises appearance,\ndistance, and angle features and we learn all of the involved parameters via\nstructured prediction. The outcome is a model for near-rigid shape matching\nwhich is robust in the sense that it is able to capture the possibly limited\nbut still important scale and appearance variations. Our experimental results\nreveal substantial improvements upon recent successful models, while\nmaintaining similar running times.\n",
          "  We introduce a modified model of random walk, and then develop two novel\nclustering algorithms based on it. In the algorithms, each data point in a\ndataset is considered as a particle which can move at random in space according\nto the preset rules in the modified model. Further, this data point may be also\nviewed as a local control subsystem, in which the controller adjusts its\ntransition probability vector in terms of the feedbacks of all data points, and\nthen its transition direction is identified by an event-generating function.\nFinally, the positions of all data points are updated. As they move in space,\ndata points collect gradually and some separating parts emerge among them\nautomatically. As a consequence, data points that belong to the same class are\nlocated at a same position, whereas those that belong to different classes are\naway from one another. Moreover, the experimental results have demonstrated\nthat data points in the test datasets are clustered reasonably and efficiently,\nand the comparison with other algorithms also provides an indication of the\neffectiveness of the proposed algorithms.\n",
          "  Bayesian model averaging, model selection and its approximations such as BIC\nare generally statistically consistent, but sometimes achieve slower rates og\nconvergence than other methods such as AIC and leave-one-out cross-validation.\nOn the other hand, these other methods can br inconsistent. We identify the\n\"catch-up phenomenon\" as a novel explanation for the slow convergence of\nBayesian methods. Based on this analysis we define the switch distribution, a\nmodification of the Bayesian marginal distribution. We show that, under broad\nconditions,model selection and prediction based on the switch distribution is\nboth consistent and achieves optimal convergence rates, thereby resolving the\nAIC-BIC dilemma. The method is practical; we give an efficient implementation.\nThe switch distribution has a data compression interpretation, and can thus be\nviewed as a \"prequential\" or MDL method; yet it is different from the MDL\nmethods that are usually considered in the literature. We compare the switch\ndistribution to Bayes factor model selection and leave-one-out\ncross-validation.\n",
          "  Artificial intelligence offers superior techniques and methods by which\nproblems from diverse domains may find an optimal solution. The Machine\nLearning technologies refer to the domain of artificial intelligence aiming to\ndevelop the techniques allowing the computers to \"learn\". Some systems based on\nMachine Learning technologies tend to eliminate the necessity of the human\nintelligence while the others adopt a man-machine collaborative approach.\n",
          "  This paper is withdrawn due to some errors, which are corrected in\narXiv:0912.0071v4 [cs.LG].\n",
          "  We investigate the performance of a simple signed distance function (SDF)\nbased method by direct comparison with standard SVM packages, as well as\nK-nearest neighbor and RBFN methods. We present experimental results comparing\nthe SDF approach with other classifiers on both synthetic geometric problems\nand five benchmark clinical microarray data sets. On both geometric problems\nand microarray data sets, the non-optimized SDF based classifiers perform just\nas well or slightly better than well-developed, standard SVM methods. These\nresults demonstrate the potential accuracy of SDF-based methods on some types\nof problems.\n",
          "  This paper presents a framework aimed at monitoring the behavior of aircraft\nin a given airspace. Nominal trajectories are determined and learned using data\ndriven methods. Standard procedures are used by air traffic controllers (ATC)\nto guide aircraft, ensure the safety of the airspace, and to maximize the\nrunway occupancy. Even though standard procedures are used by ATC, the control\nof the aircraft remains with the pilots, leading to a large variability in the\nflight patterns observed. Two methods to identify typical operations and their\nvariability from recorded radar tracks are presented. This knowledge base is\nthen used to monitor the conformance of current operations against operations\npreviously identified as standard. A tool called AirTrajectoryMiner is\npresented, aiming at monitoring the instantaneous health of the airspace, in\nreal time. The airspace is \"healthy\" when all aircraft are flying according to\nthe nominal procedures. A measure of complexity is introduced, measuring the\nconformance of current flight to nominal flight patterns. When an aircraft does\nnot conform, the complexity increases as more attention from ATC is required to\nensure a safe separation between aircraft.\n",
          null
         ],
         "marker": {
          "color": "#CFD8DC",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "other",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          6.13862943649292,
          1.7188555002212524,
          6.259122848510742,
          3.5537023544311523,
          2.2166495323181152,
          2.7593448162078857,
          5.129063606262207,
          4.10253381729126,
          1.5277433395385742,
          2.5855209827423096,
          1.8019578456878662,
          2.953547239303589,
          7.563567638397217,
          1.6792385578155518,
          1.7379573583602905,
          2.510934352874756,
          2.1499953269958496,
          0.7605852484703064,
          0.7038392424583435,
          3.2603840827941895,
          1.7826625108718872,
          3.784839153289795,
          4.190096855163574,
          2.6016480922698975,
          2.6377365589141846,
          3.404362678527832,
          2.594588041305542,
          2.6349964141845703,
          6.155203342437744,
          1.699839472770691,
          4.060608863830566,
          5.72531270980835,
          4.137933731079102,
          2.2735953330993652,
          6.900486946105957,
          4.21076774597168,
          2.8276383876800537,
          5.943004608154297,
          3.8309361934661865,
          4.0533833503723145,
          1.2263447046279907,
          5.160822868347168,
          7.250852584838867,
          5.18057918548584,
          1.173345685005188,
          4.459115505218506,
          5.77581262588501,
          6.133796215057373,
          3.1444547176361084,
          2.1841635704040527,
          5.632339000701904,
          1.9841421842575073,
          0.7119867205619812,
          2.3174009323120117,
          1.761565089225769,
          1.6284458637237549,
          2.0119125843048096,
          3.057870626449585,
          3.1083309650421143,
          3.84315824508667,
          2.069481611251831,
          1.7521862983703613,
          2.0983612537384033,
          1.7154629230499268,
          5.792295455932617,
          4.053793907165527,
          2.190248966217041,
          5.240325927734375,
          2.584747076034546,
          2.7309248447418213,
          1.0575636625289917,
          3.8036839962005615,
          7.395144939422607,
          2.074772596359253,
          2.0383918285369873,
          2.3421449661254883,
          7.445953845977783,
          5.667885780334473,
          3.3363308906555176,
          2.7217040061950684,
          6.695915699005127,
          1.6840314865112305,
          1.6735148429870605,
          6.532612323760986,
          1.998884677886963,
          6.1033220291137695,
          1.4424610137939453,
          2.876678943634033,
          1.9127206802368164,
          0.7326059937477112,
          0.8133427500724792,
          5.395330429077148,
          2.0712594985961914,
          4.0425801277160645,
          5.336868762969971,
          6.067416191101074,
          6.959477424621582,
          7.633128643035889,
          2.0752007961273193,
          1.6673524379730225,
          5.408269882202148,
          1.5994033813476562,
          1.7372628450393677,
          2.7135212421417236,
          0.8403459191322327,
          5.149454116821289,
          1.7584428787231445,
          1.4027148485183716,
          3.947916030883789,
          1.6940442323684692,
          5.149118423461914,
          2.81732439994812,
          3.8572115898132324,
          5.276151657104492,
          2.068655490875244,
          5.1459126472473145,
          1.7437382936477661,
          3.695755958557129,
          3.4079713821411133
         ],
         "y": [
          6.547318935394287,
          9.446529388427734,
          6.500439643859863,
          4.409639358520508,
          8.863118171691895,
          5.956254482269287,
          8.83517074584961,
          7.873986721038818,
          7.944622993469238,
          7.052482604980469,
          6.609358787536621,
          4.395708084106445,
          7.5390214920043945,
          7.782416820526123,
          5.253305435180664,
          7.903298377990723,
          8.792684555053711,
          7.629904747009277,
          7.737026691436768,
          5.720931529998779,
          5.04727029800415,
          8.326273918151855,
          6.185138702392578,
          7.165393352508545,
          7.948257923126221,
          9.310302734375,
          7.147777557373047,
          6.485863208770752,
          7.405516147613525,
          8.965435981750488,
          7.1506829261779785,
          6.486638069152832,
          7.091297149658203,
          4.9215264320373535,
          7.295664310455322,
          4.42416524887085,
          4.629428386688232,
          6.431421279907227,
          5.254847526550293,
          4.28105354309082,
          5.569730758666992,
          8.869482040405273,
          5.742273807525635,
          8.885652542114258,
          6.4552741050720215,
          6.97156286239624,
          6.624174118041992,
          6.067781448364258,
          6.696353912353516,
          9.643397331237793,
          7.461650848388672,
          4.547192096710205,
          7.656403541564941,
          4.97675085067749,
          6.899485111236572,
          8.802244186401367,
          5.876082420349121,
          8.519835472106934,
          8.029746055603027,
          4.697698593139648,
          5.3027801513671875,
          6.953774929046631,
          7.892355442047119,
          5.159485816955566,
          6.615457534790039,
          4.421304702758789,
          8.822237968444824,
          6.344164848327637,
          7.152528285980225,
          4.5322089195251465,
          5.838592529296875,
          8.548856735229492,
          6.840102195739746,
          5.478147506713867,
          6.3677778244018555,
          5.175342559814453,
          7.764278888702393,
          6.410373210906982,
          4.29701566696167,
          6.395833492279053,
          6.340489387512207,
          5.1316938400268555,
          8.868949890136719,
          6.191130638122559,
          5.651233196258545,
          7.357550144195557,
          8.094287872314453,
          8.411975860595703,
          5.321349143981934,
          7.621099948883057,
          6.387355327606201,
          7.4504594802856445,
          7.8405046463012695,
          4.565269947052002,
          6.429139614105225,
          6.509778022766113,
          6.986302852630615,
          5.852266788482666,
          6.5405168533325195,
          5.117161273956299,
          7.450963497161865,
          8.207112312316895,
          5.068819999694824,
          9.220015525817871,
          7.480164527893066,
          8.858380317687988,
          6.897808074951172,
          7.163211345672607,
          5.2876787185668945,
          6.405657768249512,
          7.821405410766602,
          8.013069152832031,
          5.292238712310791,
          6.321427822113037,
          5.660507678985596,
          8.854578971862793,
          7.367833614349365,
          4.253695011138916,
          6.765641212463379
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  We consider the problem of boosting the accuracy of weak learning algorithms\nin the agnostic learning framework of Haussler (1992) and Kearns et al. (1992).\nKnown algorithms for this problem (Ben-David et al., 2001; Gavinsky, 2002;\nKalai et al., 2008) follow the same strategy as boosting algorithms in the PAC\nmodel: the weak learner is executed on the same target function but over\ndifferent distributions on the domain. We demonstrate boosting algorithms for\nthe agnostic learning framework that only modify the distribution on the labels\nof the points (or, equivalently, modify the target function). This allows\nboosting a distribution-specific weak agnostic learner to a strong agnostic\nlearner with respect to the same distribution.\n  When applied to the weak agnostic parity learning algorithm of Goldreich and\nLevin (1989) our algorithm yields a simple PAC learning algorithm for DNF and\nan agnostic learning algorithm for decision trees over the uniform distribution\nusing membership queries. These results substantially simplify Jackson's famous\nDNF learning algorithm (1994) and the recent result of Gopalan et al. (2008).\n  We also strengthen the connection to hard-core set constructions discovered\nby Klivans and Servedio (1999) by demonstrating that hard-core set\nconstructions that achieve the optimal hard-core set size (given by Holenstein\n(2005) and Barak et al. (2009)) imply distribution-specific agnostic boosting\nalgorithms. Conversely, our boosting algorithm gives a simple hard-core set\nconstruction with an (almost) optimal hard-core set size.\n",
          "  Solomonoff's central result on induction is that the posterior of a universal\nsemimeasure M converges rapidly and with probability 1 to the true sequence\ngenerating posterior mu, if the latter is computable. Hence, M is eligible as a\nuniversal sequence predictor in case of unknown mu. Despite some nearby results\nand proofs in the literature, the stronger result of convergence for all\n(Martin-Loef) random sequences remained open. Such a convergence result would\nbe particularly interesting and natural, since randomness can be defined in\nterms of M itself. We show that there are universal semimeasures M which do not\nconverge for all random sequences, i.e. we give a partial negative answer to\nthe open problem. We also provide a positive answer for some non-universal\nsemimeasures. We define the incomputable measure D as a mixture over all\ncomputable measures and the enumerable semimeasure W as a mixture over all\nenumerable nearly-measures. We show that W converges to D and D to mu on all\nrandom sequences. The Hellinger distance measuring closeness of two\ndistributions plays a central role.\n",
          "  While statistics focusses on hypothesis testing and on estimating (properties\nof) the true sampling distribution, in machine learning the performance of\nlearning algorithms on future data is the primary issue. In this paper we\nbridge the gap with a general principle (PHI) that identifies hypotheses with\nbest predictive performance. This includes predictive point and interval\nestimation, simple and composite hypothesis testing, (mixture) model selection,\nand others as special cases. For concrete instantiations we will recover\nwell-known methods, variations thereof, and new ones. PHI nicely justifies,\nreconciles, and blends (a reparametrization invariant variation of) MAP, ML,\nMDL, and moment estimation. One particular feature of PHI is that it can\ngenuinely deal with nested hypotheses.\n",
          "  Consider a class $\\mH$ of binary functions $h: X\\to\\{-1, +1\\}$ on a finite\ninterval $X=[0, B]\\subset \\Real$. Define the {\\em sample width} of $h$ on a\nfinite subset (a sample) $S\\subset X$ as $\\w_S(h) \\equiv \\min_{x\\in S}\n|\\w_h(x)|$, where $\\w_h(x) = h(x) \\max\\{a\\geq 0: h(z)=h(x), x-a\\leq z\\leq\nx+a\\}$. Let $\\mathbb{S}_\\ell$ be the space of all samples in $X$ of cardinality\n$\\ell$ and consider sets of wide samples, i.e., {\\em hypersets} which are\ndefined as $A_{\\beta, h} = \\{S\\in \\mathbb{S}_\\ell: \\w_{S}(h) \\geq \\beta\\}$.\nThrough an application of the Sauer-Shelah result on the density of sets an\nupper estimate is obtained on the growth function (or trace) of the class\n$\\{A_{\\beta, h}: h\\in\\mH\\}$, $\\beta>0$, i.e., on the number of possible\ndichotomies obtained by intersecting all hypersets with a fixed collection of\nsamples $S\\in\\mathbb{S}_\\ell$ of cardinality $m$. The estimate is\n$2\\sum_{i=0}^{2\\lfloor B/(2\\beta)\\rfloor}{m-\\ell\\choose i}$.\n",
          "  We introduce the Reduced-Rank Hidden Markov Model (RR-HMM), a generalization\nof HMMs that can model smooth state evolution as in Linear Dynamical Systems\n(LDSs) as well as non-log-concave predictive distributions as in\ncontinuous-observation HMMs. RR-HMMs assume an m-dimensional latent state and n\ndiscrete observations, with a transition matrix of rank k <= m. This implies\nthe dynamics evolve in a k-dimensional subspace, while the shape of the set of\npredictive distributions is determined by m. Latent state belief is represented\nwith a k-dimensional state vector and inference is carried out entirely in R^k,\nmaking RR-HMMs as computationally efficient as k-state HMMs yet more\nexpressive. To learn RR-HMMs, we relax the assumptions of a recently proposed\nspectral learning algorithm for HMMs (Hsu, Kakade and Zhang 2009) and apply it\nto learn k-dimensional observable representations of rank-k RR-HMMs. The\nalgorithm is consistent and free of local optima, and we extend its performance\nguarantees to cover the RR-HMM case. We show how this algorithm can be used in\nconjunction with a kernel density estimator to efficiently model\nhigh-dimensional multivariate continuous data. We also relax the assumption\nthat single observations are sufficient to disambiguate state, and extend the\nalgorithm accordingly. Experiments on synthetic data and a toy video, as well\nas on a difficult robot vision modeling problem, yield accurate models that\ncompare favorably with standard alternatives in simulation quality and\nprediction capability.\n",
          "  We prove that mutual information is actually negative copula entropy, based\non which a method for mutual information estimation is proposed.\n",
          "  Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).\n",
          "  Let $\\XX$ be a compact, smooth, connected, Riemannian manifold without\nboundary, $G:\\XX\\times\\XX\\to \\RR$ be a kernel. Analogous to a radial basis\nfunction network, an eignet is an expression of the form $\\sum_{j=1}^M\na_jG(\\circ,y_j)$, where $a_j\\in\\RR$, $y_j\\in\\XX$, $1\\le j\\le M$. We describe a\ndeterministic, universal algorithm for constructing an eignet for approximating\nfunctions in $L^p(\\mu;\\XX)$ for a general class of measures $\\mu$ and kernels\n$G$. Our algorithm yields linear operators. Using the minimal separation\namongst the centers $y_j$ as the cost of approximation, we give modulus of\nsmoothness estimates for the degree of approximation by our eignets, and show\nby means of a converse theorem that these are the best possible for every\n\\emph{individual function}. We also give estimates on the coefficients $a_j$ in\nterms of the norm of the eignet. Finally, we demonstrate that if any sequence\nof eignets satisfies the optimal estimates for the degree of approximation of a\nsmooth function, measured in terms of the minimal separation, then the\nderivatives of the eignets also approximate the corresponding derivatives of\nthe target function in an optimal manner.\n",
          "  Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms.\n",
          "  We give the first non-trivial upper bounds on the average sensitivity and\nnoise sensitivity of polynomial threshold functions. More specifically, for a\nBoolean function f on n variables equal to the sign of a real, multivariate\npolynomial of total degree d we prove\n  1) The average sensitivity of f is at most O(n^{1-1/(4d+6)}) (we also give a\ncombinatorial proof of the bound O(n^{1-1/2^d}).\n  2) The noise sensitivity of f with noise rate \\delta is at most\nO(\\delta^{1/(4d+6)}).\n  Previously, only bounds for the linear case were known. Along the way we show\nnew structural theorems about random restrictions of polynomial threshold\nfunctions obtained via hypercontractivity. These structural results may be of\nindependent interest as they provide a generic template for transforming\nproblems related to polynomial threshold functions defined on the Boolean\nhypercube to polynomial threshold functions defined in Gaussian space.\n",
          "  We propose a framework for analyzing and comparing distributions, allowing us\nto design statistical tests to determine if two samples are drawn from\ndifferent distributions. Our test statistic is the largest difference in\nexpectations over functions in the unit ball of a reproducing kernel Hilbert\nspace (RKHS). We present two tests based on large deviation bounds for the test\nstatistic, while a third is based on the asymptotic distribution of this\nstatistic. The test statistic can be computed in quadratic time, although\nefficient linear time approximations are available. Several classical metrics\non distributions are recovered when the function space used to compute the\ndifference in expectations is allowed to be more general (eg. a Banach space).\nWe apply our two-sample tests to a variety of problems, including attribute\nmatching for databases using the Hungarian marriage method, where they perform\nstrongly. Excellent performance is also obtained when comparing distributions\nover graphs, for which these are the first such tests.\n",
          "  We consider the problem of joint universal variable-rate lossy coding and\nidentification for parametric classes of stationary $\\beta$-mixing sources with\ngeneral (Polish) alphabets. Compression performance is measured in terms of\nLagrangians, while identification performance is measured by the variational\ndistance between the true source and the estimated source. Provided that the\nsources are mixing at a sufficiently fast rate and satisfy certain smoothness\nand Vapnik-Chervonenkis learnability conditions, it is shown that, for bounded\nmetric distortions, there exist universal schemes for joint lossy compression\nand identification whose Lagrangian redundancies converge to zero as $\\sqrt{V_n\n\\log n /n}$ as the block length $n$ tends to infinity, where $V_n$ is the\nVapnik-Chervonenkis dimension of a certain class of decision regions defined by\nthe $n$-dimensional marginal distributions of the sources; furthermore, for\neach $n$, the decoder can identify $n$-dimensional marginal of the active\nsource up to a ball of radius $O(\\sqrt{V_n\\log n/n})$ in variational distance,\neventually with probability one. The results are supplemented by several\nexamples of parametric sources satisfying the regularity conditions.\n",
          "  Given $n$ points in a $d$ dimensional Euclidean space, the Minimum Enclosing\nBall (MEB) problem is to find the ball with the smallest radius which contains\nall $n$ points. We give a $O(nd\\Qcal/\\sqrt{\\epsilon})$ approximation algorithm\nfor producing an enclosing ball whose radius is at most $\\epsilon$ away from\nthe optimum (where $\\Qcal$ is an upper bound on the norm of the points). This\nimproves existing results using \\emph{coresets}, which yield a $O(nd/\\epsilon)$\ngreedy algorithm. Finding the Minimum Enclosing Convex Polytope (MECP) is a\nrelated problem wherein a convex polytope of a fixed shape is given and the aim\nis to find the smallest magnification of the polytope which encloses the given\npoints. For this problem we present a $O(mnd\\Qcal/\\epsilon)$ approximation\nalgorithm, where $m$ is the number of faces of the polytope. Our algorithms\nborrow heavily from convex duality and recently developed techniques in\nnon-smooth optimization, and are in contrast with existing methods which rely\non geometric arguments. In particular, we specialize the excessive gap\nframework of \\citet{Nesterov05a} to obtain our results.\n",
          "  We introduce algorithmic information theory, also known as the theory of\nKolmogorov complexity. We explain the main concepts of this quantitative\napproach to defining `information'. We discuss the extent to which Kolmogorov's\nand Shannon's information theory have a common purpose, and where they are\nfundamentally different. We indicate how recent developments within the theory\nallow one to formally distinguish between `structural' (meaningful) and\n`random' information as measured by the Kolmogorov structure function, which\nleads to a mathematical formalization of Occam's razor in inductive inference.\nWe end by discussing some of the philosophical implications of the theory.\n",
          "  The Bethe approximation, or loopy belief propagation algorithm is a\nsuccessful method for approximating partition functions of probabilistic models\nassociated with a graph. Chertkov and Chernyak derived an interesting formula\ncalled Loop Series Expansion, which is an expansion of the partition function.\nThe main term of the series is the Bethe approximation while other terms are\nlabeled by subgraphs called generalized loops. In our recent paper, we derive\nthe loop series expansion in form of a polynomial with coefficients positive\nintegers, and extend the result to the expansion of marginals. In this paper,\nwe give more clear derivation for the results and discuss the properties of the\npolynomial which is introduced in the paper.\n",
          "  We study probability distributions over free algebras of trees. Probability\ndistributions can be seen as particular (formal power) tree series [Berstel et\nal 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely\nstudied class of tree series is the class of rational (or recognizable) tree\nseries which can be defined either in an algebraic way or by means of\nmultiplicity tree automata. We argue that the algebraic representation is very\nconvenient to model probability distributions over a free algebra of trees.\nFirst, as in the string case, the algebraic representation allows to design\nlearning algorithms for the whole class of probability distributions defined by\nrational tree series. Note that learning algorithms for rational tree series\ncorrespond to learning algorithms for weighted tree automata where both the\nstructure and the weights are learned. Second, the algebraic representation can\nbe easily extended to deal with unranked trees (like XML trees where a symbol\nmay have an unbounded number of children). Both properties are particularly\nrelevant for applications: nondeterministic automata are required for the\ninference problem to be relevant (recall that Hidden Markov Models are\nequivalent to nondeterministic string automata); nowadays applications for Web\nInformation Extraction, Web Services and document processing consider unranked\ntrees.\n",
          "  The Baum-Welsh algorithm together with its derivatives and variations has\nbeen the main technique for learning Hidden Markov Models (HMM) from\nobservational data. We present an HMM learning algorithm based on the\nnon-negative matrix factorization (NMF) of higher order Markovian statistics\nthat is structurally different from the Baum-Welsh and its associated\napproaches. The described algorithm supports estimation of the number of\nrecurrent states of an HMM and iterates the non-negative matrix factorization\n(NMF) algorithm to improve the learned HMM parameters. Numerical examples are\nprovided as well.\n",
          "  Next to the shortest path distance, the second most popular distance function\nbetween vertices in a graph is the commute distance (resistance distance). For\ntwo vertices u and v, the hitting time H_{uv} is the expected time it takes a\nrandom walk to travel from u to v. The commute time is its symmetrized version\nC_{uv} = H_{uv} + H_{vu}. In our paper we study the behavior of hitting times\nand commute distances when the number n of vertices in the graph is very large.\nWe prove that as n converges to infinty, hitting times and commute distances\nconverge to expressions that do not take into account the global structure of\nthe graph at all. Namely, the hitting time H_{uv} converges to 1/d_v and the\ncommute time to 1/d_u + 1/d_v where d_u and d_v denote the degrees of vertices\nu and v. In these cases, the hitting and commute times are misleading in the\nsense that they do not provide information about the structure of the graph. We\nfocus on two major classes of random graphs: random geometric graphs (k-nearest\nneighbor graphs, epsilon-graphs, Gaussian similarity graphs) and random graphs\nwith given expected degrees (in particular, Erdos-Renyi graphs with and without\nplanted partitions)\n",
          "  In this paper we propose a novel algorithm, factored value iteration (FVI),\nfor the approximate solution of factored Markov decision processes (fMDPs). The\ntraditional approximate value iteration algorithm is modified in two ways. For\none, the least-squares projection operator is modified so that it does not\nincrease max-norm, and thus preserves convergence. The other modification is\nthat we uniformly sample polynomially many samples from the (exponentially\nlarge) state space. This way, the complexity of our algorithm becomes\npolynomial in the size of the fMDP description length. We prove that the\nalgorithm is convergent. We also derive an upper bound on the difference\nbetween our approximate solution and the optimal one, and also on the error\nintroduced by sampling. We analyze various projection operators with respect to\ntheir computation complexity and their convergence when combined with\napproximate value iteration.\n",
          "  Walley's Imprecise Dirichlet Model (IDM) for categorical i.i.d. data extends\nthe classical Dirichlet model to a set of priors. It overcomes several\nfundamental problems which other approaches to uncertainty suffer from. Yet, to\nbe useful in practice, one needs efficient ways for computing the\nimprecise=robust sets or intervals. The main objective of this work is to\nderive exact, conservative, and approximate, robust and credible interval\nestimates under the IDM for a large class of statistical estimators, including\nthe entropy and mutual information.\n",
          "  The purpose of this note is to show how the method of maximum entropy in the\nmean (MEM) may be used to improve parametric estimation when the measurements\nare corrupted by large level of noise. The method is developed in the context\non a concrete example: that of estimation of the parameter in an exponential\ndistribution. We compare the performance of our method with the bayesian and\nmaximum likelihood approaches.\n",
          "  We present a novel graphical framework for modeling non-negative sequential\ndata with hierarchical structure. Our model corresponds to a network of coupled\nnon-negative matrix factorization (NMF) modules, which we refer to as a\npositive factor network (PFN). The data model is linear, subject to\nnon-negativity constraints, so that observation data consisting of an additive\ncombination of individually representable observations is also representable by\nthe network. This is a desirable property for modeling problems in\ncomputational auditory scene analysis, since distinct sound sources in the\nenvironment are often well-modeled as combining additively in the corresponding\nmagnitude spectrogram. We propose inference and learning algorithms that\nleverage existing NMF algorithms and that are straightforward to implement. We\npresent a target tracking example and provide results for synthetic observation\ndata which serve to illustrate the interesting properties of PFNs and motivate\ntheir potential usefulness in applications such as music transcription, source\nseparation, and speech recognition. We show how a target process characterized\nby a hierarchical state transition model can be represented as a PFN. Our\nresults illustrate that a PFN which is defined in terms of a single target\nobservation can then be used to effectively track the states of multiple\nsimultaneous targets. Our results show that the quality of the inferred target\nstates degrades gradually as the observation noise is increased. We also\npresent results for an example in which meaningful hierarchical features are\nextracted from a spectrogram. Such a hierarchical representation could be\nuseful for music transcription and source separation applications. We also\npropose a network for language modeling.\n",
          "  We show that learning a convex body in $\\RR^d$, given random samples from the\nbody, requires $2^{\\Omega(\\sqrt{d/\\eps})}$ samples. By learning a convex body\nwe mean finding a set having at most $\\eps$ relative symmetric difference with\nthe input body. To prove the lower bound we construct a hard to learn family of\nconvex bodies. Our construction of this family is very simple and based on\nerror correcting codes.\n",
          "  In this paper, we propose a unified algorithmic framework for solving many\nknown variants of \\mds. Our algorithm is a simple iterative scheme with\nguaranteed convergence, and is \\emph{modular}; by changing the internals of a\nsingle subroutine in the algorithm, we can switch cost functions and target\nspaces easily. In addition to the formal guarantees of convergence, our\nalgorithms are accurate; in most cases, they converge to better quality\nsolutions than existing methods, in comparable time. We expect that this\nframework will be useful for a number of \\mds variants that have not yet been\nstudied.\n  Our framework extends to embedding high-dimensional points lying on a sphere\nto points on a lower dimensional sphere, preserving geodesic distances. As a\ncompliment to this result, we also extend the Johnson-Lindenstrauss Lemma to\nthis spherical setting, where projecting to a random $O((1/\\eps^2) \\log\nn)$-dimensional sphere causes $\\eps$-distortion.\n",
          "  We provide asymptotically sharp bounds for the Gaussian surface area and the\nGaussian noise sensitivity of polynomial threshold functions. In particular we\nshow that if $f$ is a degree-$d$ polynomial threshold function, then its\nGaussian sensitivity at noise rate $\\epsilon$ is less than some quantity\nasymptotic to $\\frac{d\\sqrt{2\\epsilon}}{\\pi}$ and the Gaussian surface area is\nat most $\\frac{d}{\\sqrt{2\\pi}}$. Furthermore these bounds are asymptotically\ntight as $\\epsilon\\to 0$ and $f$ the threshold function of a product of $d$\ndistinct homogeneous linear functions.\n",
          "  Background: Hidden Markov models are widely employed by numerous\nbioinformatics programs used today. Applications range widely from comparative\ngene prediction to time-series analyses of micro-array data. The parameters of\nthe underlying models need to be adjusted for specific data sets, for example\nthe genome of a particular species, in order to maximize the prediction\naccuracy. Computationally efficient algorithms for parameter training are thus\nkey to maximizing the usability of a wide range of bioinformatics applications.\n  Results: We introduce two computationally efficient training algorithms, one\nfor Viterbi training and one for stochastic expectation maximization (EM)\ntraining, which render the memory requirements independent of the sequence\nlength. Unlike the existing algorithms for Viterbi and stochastic EM training\nwhich require a two-step procedure, our two new algorithms require only one\nstep and scan the input sequence in only one direction. We also implement these\ntwo new algorithms and the already published linear-memory algorithm for EM\ntraining into the hidden Markov model compiler HMM-Converter and examine their\nrespective practical merits for three small example models.\n  Conclusions: Bioinformatics applications employing hidden Markov models can\nuse the two algorithms in order to make Viterbi training and stochastic EM\ntraining more computationally efficient. Using these algorithms, parameter\ntraining can thus be attempted for more complex models and longer training\nsequences. The two new algorithms have the added advantage of being easier to\nimplement than the corresponding default algorithms for Viterbi training and\nstochastic EM training.\n",
          "  We consider the problem of minimal correction of the training set to make it\nconsistent with monotonic constraints. This problem arises during analysis of\ndata sets via techniques that require monotone data. We show that this problem\nis NP-hard in general and is equivalent to finding a maximal independent set in\nspecial orgraphs. Practically important cases of that problem considered in\ndetail. These are the cases when a partial order given on the replies set is a\ntotal order or has a dimension 2. We show that the second case can be reduced\nto maximization of a quadratic convex function on a convex set. For this case\nwe construct an approximate polynomial algorithm based on convex optimization.\n",
          "  It is hard to exaggerate the role of economic aggregators -- functions that\nsummarize numerous and / or heterogeneous data -- in economic models since the\nearly XX$^{th}$ century. In many cases, as witnessed by the pioneering works of\nCobb and Douglas, these functions were information quantities tailored to\neconomic theories, i.e. they were built to fit economic phenomena. In this\npaper, we look at these functions from the complementary side: information. We\nuse a recent toolbox built on top of a vast class of distortions coined by\nBregman, whose application field rivals metrics' in various subfields of\nmathematics. This toolbox makes it possible to find the quality of an\naggregator (for consumptions, prices, labor, capital, wages, etc.), from the\nstandpoint of the information it carries. We prove a rather striking result.\n  From the informational standpoint, well-known economic aggregators do belong\nto the \\textit{optimal} set. As common economic assumptions enter the analysis,\nthis large set shrinks, and it essentially ends up \\textit{exactly fitting}\neither CES, or Cobb-Douglas, or both. To summarize, in the relevant economic\ncontexts, one could not have crafted better some aggregator from the\ninformation standpoint. We also discuss global economic behaviors of optimal\ninformation aggregators in general, and present a brief panorama of the links\nbetween economic and information aggregators.\n  Keywords: Economic Aggregators, CES, Cobb-Douglas, Bregman divergences\n",
          "  We extend the Chow-Liu algorithm for general random variables while the\nprevious versions only considered finite cases. In particular, this paper\napplies the generalization to Suzuki's learning algorithm that generates from\ndata forests rather than trees based on the minimum description length by\nbalancing the fitness of the data to the forest and the simplicity of the\nforest. As a result, we successfully obtain an algorithm when both of the\nGaussian and finite random variables are present.\n",
          "  In the constraint satisfaction problem ($CSP$), the aim is to find an\nassignment of values to a set of variables subject to specified constraints. In\nthe minimum cost homomorphism problem ($MinHom$), one is additionally given\nweights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find\nan assignment $f$ to the variables that minimizes $\\sum_{v} c_{vf(v)}$. Let\n$MinHom(\\Gamma)$ denote the $MinHom$ problem parameterized by the set of\npredicates allowed for constraints. $MinHom(\\Gamma)$ is related to many\nwell-studied combinatorial optimization problems, and concrete applications can\nbe found in, for instance, defence logistics and machine learning. We show that\n$MinHom(\\Gamma)$ can be studied by using algebraic methods similar to those\nused for CSPs. With the aid of algebraic techniques, we classify the\ncomputational complexity of $MinHom(\\Gamma)$ for all choices of $\\Gamma$. Our\nresult settles a general dichotomy conjecture previously resolved only for\ncertain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of\nCombinatorics, 2008].\n",
          "  We consider the problem of PAC-learning decision trees, i.e., learning a\ndecision tree over the n-dimensional hypercube from independent random labeled\nexamples. Despite significant effort, no polynomial-time algorithm is known for\nlearning polynomial-sized decision trees (even trees of any super-constant\nsize), even when examples are assumed to be drawn from the uniform distribution\non {0,1}^n. We give an algorithm that learns arbitrary polynomial-sized\ndecision trees for {\\em most product distributions}. In particular, consider a\nrandom product distribution where the bias of each bit is chosen independently\nand uniformly from, say, [.49,.51]. Then with high probability over the\nparameters of the product distribution and the random examples drawn from it,\nthe algorithm will learn any tree. More generally, in the spirit of smoothed\nanalysis, we consider an arbitrary product distribution whose parameters are\nspecified only up to a [-c,c] accuracy (perturbation), for an arbitrarily small\npositive constant c.\n",
          "  The method of stable random projections is a tool for efficiently computing\nthe $l_\\alpha$ distances using low memory, where $0<\\alpha \\leq 2$ is a tuning\nparameter. The method boils down to a statistical estimation task and various\nestimators have been proposed, based on the geometric mean, the harmonic mean,\nand the fractional power etc.\n  This study proposes the optimal quantile estimator, whose main operation is\nselecting, which is considerably less expensive than taking fractional power,\nthe main operation in previous estimators. Our experiments report that the\noptimal quantile estimator is nearly one order of magnitude more\ncomputationally efficient than previous estimators. For large-scale learning\ntasks in which storing and computing pairwise distances is a serious\nbottleneck, this estimator should be desirable.\n  In addition to its computational advantages, the optimal quantile estimator\nexhibits nice theoretical properties. It is more accurate than previous\nestimators when $\\alpha>1$. We derive its theoretical error bounds and\nestablish the explicit (i.e., no hidden constants) sample complexity bound.\n",
          "  This paper presents a theoretical analysis of sample selection bias\ncorrection. The sample bias correction technique commonly used in machine\nlearning consists of reweighting the cost of an error on each training point of\na biased sample to more closely reflect the unbiased distribution. This relies\non weights derived by various estimation techniques based on finite samples. We\nanalyze the effect of an error in that estimation on the accuracy of the\nhypothesis returned by the learning algorithm for two estimation techniques: a\ncluster-based estimation technique and kernel mean matching. We also report the\nresults of sample bias correction experiments with several data sets using\nthese techniques. Our analysis is based on the novel concept of distributional\nstability which generalizes the existing concept of point-based stability. Much\nof our work and proof techniques can be used to analyze other importance\nweighting techniques and their effect on accuracy when using a distributionally\nstable algorithm.\n",
          "  We consider inapproximability of the correlation clustering problem defined\nas follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\"\n(similar) or \"-\" (dissimilar), correlation clustering seeks to partition the\nvertices into clusters so that the number of pairs correctly (resp.\nincorrectly) classified with respect to the labels is maximized (resp.\nminimized). The two complementary problems are called MaxAgree and MinDisagree,\nrespectively, and have been studied on complete graphs, where every edge is\nlabeled, and general graphs, where some edge might not have been labeled.\nNatural edge-weighted versions of both problems have been studied as well. Let\nS-MaxAgree denote the weighted problem where all weights are taken from set S,\nwe show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$\nessentially belongs to the same hardness class in the following sense: if there\nis a polynomial time algorithm that approximates S-MaxAgree within a factor of\n$\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S',\nS'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda\n+ \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high\nprobability. A similar statement also holds for $S-MinDisagree. This result\nimplies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree\nwithin a factor of $80/79-\\epsilon$, improving upon a previous known factor of\n$116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.\n",
          "  The paper studies the asymptotic behavior of Random Algebraic Riccati\nEquations (RARE) arising in Kalman filtering when the arrival of the\nobservations is described by a Bernoulli i.i.d. process. We model the RARE as\nan order-preserving, strongly sublinear random dynamical system (RDS). Under a\nsufficient condition, stochastic boundedness, and using a limit-set dichotomy\nresult for order-preserving, strongly sublinear RDS, we establish the\nasymptotic properties of the RARE: the sequence of random prediction error\ncovariance matrices converges weakly to a unique invariant distribution, whose\nsupport exhibits fractal behavior. In particular, this weak convergence holds\nunder broad conditions and even when the observations arrival rate is below the\ncritical probability for mean stability. We apply the weak-Feller property of\nthe Markov process governing the RARE to characterize the support of the\nlimiting invariant distribution as the topological closure of a countable set\nof points, which, in general, is not dense in the set of positive semi-definite\nmatrices. We use the explicit characterization of the support of the invariant\ndistribution and the almost sure ergodicity of the sample paths to easily\ncompute the moments of the invariant distribution. A one dimensional example\nillustrates that the support is a fractured subset of the non-negative reals\nwith self-similarity properties.\n",
          "  The problem of statistical learning is to construct an accurate predictor of\na random variable as a function of a correlated random variable on the basis of\nan i.i.d. training sample from their joint distribution. Allowable predictors\nare constrained to lie in some specified class, and the goal is to approach\nasymptotically the performance of the best predictor in the class. We consider\ntwo settings in which the learning agent only has access to rate-limited\ndescriptions of the training data, and present information-theoretic bounds on\nthe predictor performance achievable in the presence of these communication\nconstraints. Our proofs do not assume any separation structure between\ncompression and learning and rely on a new class of operational criteria\nspecifically tailored to joint design of encoders and learning algorithms in\nrate-constrained settings.\n",
          "  In this paper, I expand Shannon's definition of entropy into a new form of\nentropy that allows integration of information from different random events.\nShannon's notion of entropy is a special case of my more general definition of\nentropy. I define probability using a so-called performance function, which is\nde facto an exponential distribution. Assuming that my general notion of\nentropy reflects the true uncertainty about a probabilistic event, I understand\nthat our perceived uncertainty differs. I claim that our perception is the\nresult of two opposing forces similar to the two famous antagonists in Chinese\nphilosophy: Yin and Yang. Based on this idea, I show that our perceived\nuncertainty matches the true uncertainty in points determined by the golden\nratio. I demonstrate that the well-known sigmoid function, which we typically\nemploy in artificial neural networks as a non-linear threshold function,\ndescribes the actual performance. Furthermore, I provide a motivation for the\ntime dilation in Einstein's Special Relativity, basically claiming that\nalthough time dilation conforms with our perception, it does not correspond to\nreality. At the end of the paper, I show how to apply this theoretical\nframework to practical applications. I present recognition rates for a pattern\nrecognition problem, and also propose a network architecture that can take\nadvantage of general entropy to solve complex decision problems.\n",
          "  In this paper, we have established a unified framework of multistage\nparameter estimation. We demonstrate that a wide variety of statistical\nproblems such as fixed-sample-size interval estimation, point estimation with\nerror control, bounded-width confidence intervals, interval estimation\nfollowing hypothesis testing, construction of confidence sequences, can be cast\ninto the general framework of constructing sequential random intervals with\nprescribed coverage probabilities. We have developed exact methods for the\nconstruction of such sequential random intervals in the context of multistage\nsampling. In particular, we have established inclusion principle and coverage\ntuning techniques to control and adjust the coverage probabilities of\nsequential random intervals. We have obtained concrete sampling schemes which\nare unprecedentedly efficient in terms of sampling effort as compared to\nexisting procedures.\n",
          "  In this paper, we show a connection between a certain online low-congestion\nrouting problem and an online prediction of graph labeling. More specifically,\nwe prove that if there exists a routing scheme that guarantees a congestion of\n$\\alpha$ on any edge, there exists an online prediction algorithm with mistake\nbound $\\alpha$ times the cut size, which is the size of the cut induced by the\nlabel partitioning of graph vertices. With previous known bound of $O(\\log n)$\nfor $\\alpha$ for the routing problem on trees with $n$ vertices, we obtain an\nimproved prediction algorithm for graphs with high effective resistance.\n  In contrast to previous approaches that move the graph problem into problems\nin vector space using graph Laplacian and rely on the analysis of the\nperceptron algorithm, our proof are purely combinatorial. Further more, our\napproach directly generalizes to the case where labels are not binary.\n",
          "  We show how rate-distortion theory provides a mechanism for automated theory\nbuilding by naturally distinguishing between regularity and randomness. We\nstart from the simple principle that model variables should, as much as\npossible, render the future and past conditionally independent. From this, we\nconstruct an objective function for model making whose extrema embody the\ntrade-off between a model's structural complexity and its predictive power. The\nsolutions correspond to a hierarchy of models that, at each level of\ncomplexity, achieve optimal predictive power at minimal cost. In the limit of\nmaximal prediction the resulting optimal model identifies a process's intrinsic\norganization by extracting the underlying causal states. In this limit, the\nmodel's complexity is given by the statistical complexity, which is known to be\nminimal for achieving maximum prediction. Examples show how theory building can\nprofit from analyzing a process's causal compressibility, which is reflected in\nthe optimal models' rate-distortion curve--the process's characteristic for\noptimally balancing structure and noise at different levels of representation.\n",
          "  Given i.i.d. data from an unknown distribution, we consider the problem of\npredicting future items. An adaptive way to estimate the probability density is\nto recursively subdivide the domain to an appropriate data-dependent\ngranularity. A Bayesian would assign a data-independent prior probability to\n\"subdivide\", which leads to a prior over infinite(ly many) trees. We derive an\nexact, fast, and simple inference algorithm for such a prior, for the data\nevidence, the predictive distribution, the effective model dimension, moments,\nand other quantities. We prove asymptotic convergence and consistency results,\nand illustrate the behavior of our model on some prototypical functions.\n",
          "  Let X be randomly chosen from {-1,1}^n, and let Y be randomly chosen from the\nstandard spherical Gaussian on R^n. For any (possibly unbounded) polytope P\nformed by the intersection of k halfspaces, we prove that\n  |Pr [X belongs to P] - Pr [Y belongs to P]| < log^{8/5}k * Delta, where Delta\nis a parameter that is small for polytopes formed by the intersection of\n\"regular\" halfspaces (i.e., halfspaces with low influence). The novelty of our\ninvariance principle is the polylogarithmic dependence on k. Previously, only\nbounds that were at least linear in k were known. We give two important\napplications of our main result: (1) A polylogarithmic in k bound on the\nBoolean noise sensitivity of intersections of k \"regular\" halfspaces (previous\nwork gave bounds linear in k). (2) A pseudorandom generator (PRG) with seed\nlength O((log n)*poly(log k,1/delta)) that delta-fools all polytopes with k\nfaces with respect to the Gaussian distribution. We also obtain PRGs with\nsimilar parameters that fool polytopes formed by intersection of regular\nhalfspaces over the hypercube. Using our PRG constructions, we obtain the first\ndeterministic quasi-polynomial time algorithms for approximately counting the\nnumber of solutions to a broad class of integer programs, including dense\ncovering problems and contingency tables.\n",
          "  Markov random fields are used to model high dimensional distributions in a\nnumber of applied areas. Much recent interest has been devoted to the\nreconstruction of the dependency structure from independent samples from the\nMarkov random fields. We analyze a simple algorithm for reconstructing the\nunderlying graph defining a Markov random field on $n$ nodes and maximum degree\n$d$ given observations. We show that under mild non-degeneracy conditions it\nreconstructs the generating graph with high probability using $\\Theta(d\n\\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the\nlocal interactions. For most local interaction $\\eps,\\delta$ are of order\n$\\exp(-O(d))$.\n  Our results are optimal as a function of $n$ up to a multiplicative constant\ndepending on $d$ and the strength of the local interactions. Our results seem\nto be the first results for general models that guarantee that {\\em the}\ngenerating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}\n\\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the\nmeasure on the graph has correlation decay, the running time is $O(n^2 \\log n)$\nfor all fixed $d$. We also discuss the effect of observing noisy samples and\nshow that as long as the noise level is low, our algorithm is effective. On the\nother hand, we construct an example where large noise implies\nnon-identifiability even for generic noise and interactions. Finally, we\nbriefly show that in some simple cases, models with hidden nodes can also be\nrecovered.\n",
          "  In this paper, we have established a general framework of multistage\nhypothesis tests which applies to arbitrarily many mutually exclusive and\nexhaustive composite hypotheses. Within the new framework, we have constructed\nspecific multistage tests which rigorously control the risk of committing\ndecision errors and are more efficient than previous tests in terms of average\nsample number and the number of sampling operations. Without truncation, the\nsample numbers of our testing plans are absolutely bounded.\n",
          "  Fitting probabilistic models to data is often difficult, due to the general\nintractability of the partition function and its derivatives. Here we propose a\nnew parameter estimation technique that does not require computing an\nintractable normalization factor or sampling from the equilibrium distribution\nof the model. This is achieved by establishing dynamics that would transform\nthe observed data distribution into the model distribution, and then setting as\nthe objective the minimization of the KL divergence between the data\ndistribution and the distribution produced by running the dynamics for an\ninfinitesimal time. Score matching, minimum velocity learning, and certain\nforms of contrastive divergence are shown to be special cases of this learning\ntechnique. We demonstrate parameter estimation in Ising models, deep belief\nnetworks and an independent component analysis model of natural scenes. In the\nIsing model case, current state of the art techniques are outperformed by at\nleast an order of magnitude in learning time, with lower error in recovered\ncoupling parameters.\n",
          "  We study the problem of partitioning a small sample of $n$ individuals from a\nmixture of $k$ product distributions over a Boolean cube $\\{0, 1\\}^K$ according\nto their distributions. Each distribution is described by a vector of allele\nfrequencies in $\\R^K$. Given two distributions, we use $\\gamma$ to denote the\naverage $\\ell_2^2$ distance in frequencies across $K$ dimensions, which\nmeasures the statistical divergence between them. We study the case assuming\nthat bits are independently distributed across $K$ dimensions. This work\ndemonstrates that, for a balanced input instance for $k = 2$, a certain\ngraph-based optimization function returns the correct partition with high\nprobability, where a weighted graph $G$ is formed over $n$ individuals, whose\npairwise hamming distances between their corresponding bit vectors define the\nedge weights, so long as $K = \\Omega(\\ln n/\\gamma)$ and $Kn = \\tilde\\Omega(\\ln\nn/\\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where\nthe weight of a cut is the sum of the weights across all edges in the cut. This\nresult demonstrates a nice property in the high-dimensional feature space: one\ncan trade off the number of features that are required with the size of the\nsample to accomplish certain tasks like clustering.\n",
          "  The problem of graphical model selection is to correctly estimate the graph\nstructure of a Markov random field given samples from the underlying\ndistribution. We analyze the information-theoretic limitations of the problem\nof graph selection for binary Markov random fields under high-dimensional\nscaling, in which the graph size $p$ and the number of edges $k$, and/or the\nmaximal node degree $d$ are allowed to increase to infinity as a function of\nthe sample size $n$. For pairwise binary Markov random fields, we derive both\nnecessary and sufficient conditions for correct graph selection over the class\n$\\mathcal{G}_{p,k}$ of graphs on $p$ vertices with at most $k$ edges, and over\nthe class $\\mathcal{G}_{p,d}$ of graphs on $p$ vertices with maximum degree at\nmost $d$. For the class $\\mathcal{G}_{p, k}$, we establish the existence of\nconstants $c$ and $c'$ such that if $\\numobs < c k \\log p$, any method has\nerror probability at least 1/2 uniformly over the family, and we demonstrate a\ngraph decoder that succeeds with high probability uniformly over the family for\nsample sizes $\\numobs > c' k^2 \\log p$. Similarly, for the class\n$\\mathcal{G}_{p,d}$, we exhibit constants $c$ and $c'$ such that for $n < c d^2\n\\log p$, any method fails with probability at least 1/2, and we demonstrate a\ngraph decoder that succeeds with high probability for $n > c' d^3 \\log p$.\n",
          "  The paper proposes a new message passing algorithm for cycle-free factor\ngraphs. The proposed \"entropy message passing\" (EMP) algorithm may be viewed as\nsum-product message passing over the entropy semiring, which has previously\nappeared in automata theory. The primary use of EMP is to compute the entropy\nof a model. However, EMP can also be used to compute expressions that appear in\nexpectation maximization and in gradient descent algorithms.\n",
          "  Gaussian processes (GPs) provide a probabilistic nonparametric representation\nof functions in regression, classification, and other problems. Unfortunately,\nexact learning with GPs is intractable for large datasets. A variety of\napproximate GP methods have been proposed that essentially map the large\ndataset into a small set of basis points. The most advanced of these, the\nvariable-sigma GP (VSGP) (Walder et al., 2008), allows each basis point to have\nits own length scale. However, VSGP was only derived for regression. We\ndescribe how VSGP can be applied to classification and other problems, by\nderiving it as an expectation propagation algorithm. In this view, sparse GP\napproximations correspond to a KL-projection of the true posterior onto a\ncompact exponential family of GPs. VSGP constitutes one such family, and we\nshow how to enlarge this family to get additional accuracy. In particular, we\nshow that endowing each basis point with its own full covariance matrix\nprovides a significant increase in approximation power.\n",
          "  Gaussian belief propagation (GaBP) is an iterative message-passing algorithm\nfor inference in Gaussian graphical models. It is known that when GaBP\nconverges it converges to the correct MAP estimate of the Gaussian random\nvector and simple sufficient conditions for its convergence have been\nestablished. In this paper we develop a double-loop algorithm for forcing\nconvergence of GaBP. Our method computes the correct MAP estimate even in cases\nwhere standard GaBP would not have converged. We further extend this\nconstruction to compute least-squares solutions of over-constrained linear\nsystems. We believe that our construction has numerous applications, since the\nGaBP algorithm is linked to solution of linear systems of equations, which is a\nfundamental problem in computer science and engineering. As a case study, we\ndiscuss the linear detection problem. We show that using our new construction,\nwe are able to force convergence of Montanari's linear detection algorithm, in\ncases where it would originally fail. As a consequence, we are able to increase\nsignificantly the number of users that can transmit concurrently.\n",
          "  Conformal prediction uses past experience to determine precise levels of\nconfidence in new predictions. Given an error probability $\\epsilon$, together\nwith a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a\nset of labels, typically containing $\\hat{y}$, that also contains $y$ with\nprobability $1-\\epsilon$. Conformal prediction can be applied to any method for\nproducing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge\nregression, etc.\n  Conformal prediction is designed for an on-line setting in which labels are\npredicted successively, each one being revealed before the next is predicted.\nThe most novel and valuable feature of conformal prediction is that if the\nsuccessive examples are sampled independently from the same distribution, then\nthe successive predictions will be right $1-\\epsilon$ of the time, even though\nthey are based on an accumulating dataset rather than on independent datasets.\n  In addition to the model under which successive examples are sampled\nindependently, other on-line compression models can also use conformal\nprediction. The widely used Gaussian linear model is one of these.\n  This tutorial presents a self-contained account of the theory of conformal\nprediction and works through several numerical examples. A more comprehensive\ntreatment of the topic is provided in \"Algorithmic Learning in a Random World\",\nby Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).\n",
          "  Given a random binary sequence $X^{(n)}$ of random variables, $X_{t},$\n$t=1,2,...,n$, for instance, one that is generated by a Markov source (teacher)\nof order $k^{*}$ (each state represented by $k^{*}$ bits). Assume that the\nprobability of the event $X_{t}=1$ is constant and denote it by $\\beta$.\nConsider a learner which is based on a parametric model, for instance a Markov\nmodel of order $k$, who trains on a sequence $x^{(m)}$ which is randomly drawn\nby the teacher. Test the learner's performance by giving it a sequence\n$x^{(n)}$ (generated by the teacher) and check its predictions on every bit of\n$x^{(n)}.$ An error occurs at time $t$ if the learner's prediction $Y_{t}$\ndiffers from the true bit value $X_{t}$. Denote by $\\xi^{(n)}$ the sequence of\nerrors where the error bit $\\xi_{t}$ at time $t$ equals 1 or 0 according to\nwhether the event of an error occurs or not, respectively. Consider the\nsubsequence $\\xi^{(\\nu)}$ of $\\xi^{(n)}$ which corresponds to the errors of\npredicting a 0, i.e., $\\xi^{(\\nu)}$ consists of the bits of $\\xi^{(n)}$ only at\ntimes $t$ such that $Y_{t}=0.$ In this paper we compute an estimate on the\ndeviation of the frequency of 1s of $\\xi^{(\\nu)}$ from $\\beta$. The result\nshows that the level of randomness of $\\xi^{(\\nu)}$ decreases relative to an\nincrease in the complexity of the learner.\n",
          "  We present a novel approach for learning nonlinear dynamic models, which\nleads to a new set of tools capable of solving problems that are otherwise\ndifficult. We provide theory showing this new approach is consistent for models\nwith long range structure, and apply the approach to motion capture and\nhigh-dimensional video data, yielding results superior to standard\nalternatives.\n",
          "  We give a characterization of Maximum Entropy/Minimum Relative Entropy\ninference by providing two `strong entropy concentration' theorems. These\ntheorems unify and generalize Jaynes' `concentration phenomenon' and Van\nCampenhout and Cover's `conditional limit theorem'. The theorems characterize\nexactly in what sense a prior distribution Q conditioned on a given constraint,\nand the distribution P, minimizing the relative entropy D(P ||Q) over all\ndistributions satisfying the constraint, are `close' to each other. We then\napply our theorems to establish the relationship between entropy concentration\nand a game-theoretic characterization of Maximum Entropy Inference due to\nTopsoe and others.\n",
          "  This work describes a method of approximating matrix permanents efficiently\nusing belief propagation. We formulate a probability distribution whose\npartition function is exactly the permanent, then use Bethe free energy to\napproximate this partition function. After deriving some speedups to standard\nbelief propagation, the resulting algorithm requires $(n^2)$ time per\niteration. Finally, we demonstrate the advantages of using this approximation.\n",
          "  We show that Boolean functions expressible as monotone disjunctive normal\nforms are PAC-evolvable under a uniform distribution on the Boolean cube if the\nhypothesis size is allowed to remain fixed. We further show that this result is\ninsufficient to prove the PAC-learnability of monotone Boolean functions,\nthereby demonstrating a counter-example to a recent claim to the contrary. We\nfurther discuss scenarios wherein evolvability and learnability will coincide\nas well as scenarios under which they differ. The implications of the latter\ncase on the prospects of learning in complex hypothesis spaces is briefly\nexamined.\n",
          "  In statistical problems, a set of parameterized probability distributions is\nused to estimate the true probability distribution. If Fisher information\nmatrix at the true distribution is singular, then it has been left unknown what\nwe can estimate about the true distribution from random samples. In this paper,\nwe study a singular regression problem and prove a limit theorem which shows\nthe relation between the singular regression problem and two birational\ninvariants, a real log canonical threshold and a singular fluctuation. The\nobtained theorem has an important application to statistics, because it enables\nus to estimate the generalization error from the training error without any\nknowledge of the true probability distribution.\n",
          "  Statistical query (SQ) learning model of Kearns (1993) is a natural\nrestriction of the PAC learning model in which a learning algorithm is allowed\nto obtain estimates of statistical properties of the examples but cannot see\nthe examples themselves. We describe a new and simple characterization of the\nquery complexity of learning in the SQ learning model. Unlike the previously\nknown bounds on SQ learning our characterization preserves the accuracy and the\nefficiency of learning. The preservation of accuracy implies that that our\ncharacterization gives the first characterization of SQ learning in the\nagnostic learning framework. The preservation of efficiency is achieved using a\nnew boosting technique and allows us to derive a new approach to the design of\nevolutionary algorithms in Valiant's (2006) model of evolvability. We use this\napproach to demonstrate the existence of a large class of monotone evolutionary\nlearning algorithms based on square loss performance estimation. These results\ndiffer significantly from the few known evolutionary algorithms and give\nevidence that evolvability in Valiant's model is a more versatile phenomenon\nthan there had been previous reason to suspect.\n",
          "  We consider the problem of choosing a density estimate from a set of\ndistributions F, minimizing the L1-distance to an unknown distribution\n(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the\nproblem: Scheffe tournament winner and minimum distance estimate. The Scheffe\ntournament estimate requires fewer computations than the minimum distance\nestimate, but has strictly weaker guarantees than the latter.\n  We focus on the computational aspect of density estimation. We present two\nalgorithms, both with the same guarantee as the minimum distance estimate. The\nfirst one, a modification of the minimum distance estimate, uses the same\nnumber (quadratic in |F|) of computations as the Scheffe tournament. The second\none, called ``efficient minimum loss-weight estimate,'' uses only a linear\nnumber of computations, assuming that F is preprocessed.\n  We also give examples showing that the guarantees of the algorithms cannot be\nimproved and explore randomized algorithms for density estimation.\n",
          "  The problem is sequence prediction in the following setting. A sequence\n$x_1,...,x_n,...$ of discrete-valued observations is generated according to\nsome unknown probabilistic law (measure) $\\mu$. After observing each outcome,\nit is required to give the conditional probabilities of the next observation.\nThe measure $\\mu$ belongs to an arbitrary but known class $C$ of stochastic\nprocess measures. We are interested in predictors $\\rho$ whose conditional\nprobabilities converge (in some sense) to the \"true\" $\\mu$-conditional\nprobabilities if any $\\mu\\in C$ is chosen to generate the sequence. The\ncontribution of this work is in characterizing the families $C$ for which such\npredictors exist, and in providing a specific and simple form in which to look\nfor a solution. We show that if any predictor works, then there exists a\nBayesian predictor, whose prior is discrete, and which works too. We also find\nseveral sufficient and necessary conditions for the existence of a predictor,\nin terms of topological characterizations of the family $C$, as well as in\nterms of local behaviour of the measures in $C$, which in some cases lead to\nprocedures for constructing such predictors. It should be emphasized that the\nframework is completely general: the stochastic processes considered are not\nrequired to be i.i.d., stationary, or to belong to any parametric or countable\nfamily.\n",
          "  The Minimum Description Length (MDL) principle selects the model that has the\nshortest code for data plus model. We show that for a countable class of\nmodels, MDL predictions are close to the true distribution in a strong sense.\nThe result is completely general. No independence, ergodicity, stationarity,\nidentifiability, or other assumption on the model class need to be made. More\nformally, we show that for any countable class of models, the distributions\nselected by MDL (or MAP) asymptotically predict (merge with) the true measure\nin the class in total variation distance. Implications for non-i.i.d. domains\nlike time-series forecasting, discriminative learning, and reinforcement\nlearning are discussed.\n",
          "  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n",
          "  Most generalization bounds in learning theory are based on some measure of\nthe complexity of the hypothesis class used, independently of any algorithm. In\ncontrast, the notion of algorithmic stability can be used to derive tight\ngeneralization bounds that are tailored to specific learning algorithms by\nexploiting their particular properties. However, as in much of learning theory,\nexisting stability analyses and bounds apply only in the scenario where the\nsamples are independently and identically distributed. In many machine learning\napplications, however, this assumption does not hold. The observations received\nby the learning algorithm often have some inherent temporal dependence.\n  This paper studies the scenario where the observations are drawn from a\nstationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\nthe study of non-i.i.d. processes that implies a dependence between\nobservations weakening over time. We prove novel and distinct stability-based\ngeneralization bounds for stationary phi-mixing and beta-mixing sequences.\nThese bounds strictly generalize the bounds given in the i.i.d. case and apply\nto all stable learning algorithms, thereby extending the use of\nstability-bounds to non-i.i.d. scenarios.\n  We also illustrate the application of our phi-mixing generalization bounds to\ngeneral classes of learning algorithms, including Support Vector Regression,\nKernel Ridge Regression, and Support Vector Machines, and many other kernel\nregularization-based and relative entropy-based regularization algorithms.\nThese novel bounds can thus be viewed as the first theoretical basis for the\nuse of these algorithms in non-i.i.d. scenarios.\n",
          "  Kolmogorov argued that the concept of information exists also in problems\nwith no underlying stochastic model (as Shannon's information representation)\nfor instance, the information contained in an algorithm or in the genome. He\nintroduced a combinatorial notion of entropy and information $I(x:\\sy)$\nconveyed by a binary string $x$ about the unknown value of a variable $\\sy$.\nThe current paper poses the following questions: what is the relationship\nbetween the information conveyed by $x$ about $\\sy$ to the description\ncomplexity of $x$ ? is there a notion of cost of information ? are there limits\non how efficient $x$ conveys information ?\n  To answer these questions Kolmogorov's definition is extended and a new\nconcept termed {\\em information width} which is similar to $n$-widths in\napproximation theory is introduced. Information of any input source, e.g.,\nsample-based, general side-information or a hybrid of both can be evaluated by\na single common formula. An application to the space of binary functions is\nconsidered.\n",
          "  Current methods for determining whether a time series exhibits fractal\nstructure (FS) rely on subjective assessments on estimators of the Hurst\nexponent (H). Here, I introduce the Bayesian Assessment of Scaling, an\nanalytical framework for drawing objective and accurate inferences on the FS of\ntime series. The technique exploits the scaling property of the diffusion\nassociated to a time series. The resulting criterion is simple to compute and\nrepresents an accurate characterization of the evidence supporting different\nhypotheses on the scaling regime of a time series. Additionally, a closed-form\nMaximum Likelihood estimator of H is derived from the criterion, and this\nestimator outperforms the best available estimators.\n",
          "  Starting with a similarity function between objects, it is possible to define\na distance metric on pairs of objects, and more generally on probability\ndistributions over them. These distance metrics have a deep basis in functional\nanalysis, measure theory and geometric measure theory, and have a rich\nstructure that includes an isometric embedding into a (possibly infinite\ndimensional) Hilbert space. They have recently been applied to numerous\nproblems in machine learning and shape analysis.\n  In this paper, we provide the first algorithmic analysis of these distance\nmetrics. Our main contributions are as follows: (i) We present fast\napproximation algorithms for computing the kernel distance between two point\nsets P and Q that runs in near-linear time in the size of (P cup Q) (note that\nan explicit calculation would take quadratic time). (ii) We present\npolynomial-time algorithms for approximately minimizing the kernel distance\nunder rigid transformation; they run in time O(n + poly(1/epsilon, log n)).\n(iii) We provide several general techniques for reducing complex objects to\nconvenient sparse representations (specifically to point sets or sets of points\nsets) which approximately preserve the kernel distance. In particular, this\nallows us to reduce problems of computing the kernel distance between various\ntypes of objects such as curves, surfaces, and distributions to computing the\nkernel distance between point sets. These take advantage of the reproducing\nkernel Hilbert space and a new relation linking binary range spaces to\ncontinuous range spaces with bounded fat-shattering dimension.\n",
          "  The versatility of exponential families, along with their attendant convexity\nproperties, make them a popular and effective statistical model. A central\nissue is learning these models in high-dimensions, such as when there is some\nsparsity pattern of the optimal parameter. This work characterizes a certain\nstrong convexity property of general exponential families, which allow their\ngeneralization ability to be quantified. In particular, we show how this\nproperty can be used to analyze generic exponential families under L_1\nregularization.\n",
          "  Security protocols often use randomization to achieve probabilistic\nnon-determinism. This non-determinism, in turn, is used in obfuscating the\ndependence of observable values on secret data. Since the correctness of\nsecurity protocols is very important, formal analysis of security protocols has\nbeen widely studied in literature. Randomized security protocols have also been\nanalyzed using formal techniques such as process-calculi and probabilistic\nmodel checking. In this paper, we consider the problem of validating\nimplementations of randomized protocols. Unlike previous approaches which treat\nthe protocol as a white-box, our approach tries to verify an implementation\nprovided as a black box. Our goal is to infer the secrecy guarantees provided\nby a security protocol through statistical techniques. We learn the\nprobabilistic dependency of the observable outputs on secret inputs using\nBayesian network. This is then used to approximate the leakage of secret. In\norder to evaluate the accuracy of our statistical approach, we compare our\ntechnique with the probabilistic model checking technique on two examples:\ncrowds protocol and dining crypotgrapher's protocol.\n",
          "  Many learning machines that have hierarchical structure or hidden variables\nare now being used in information science, artificial intelligence, and\nbioinformatics. However, several learning machines used in such fields are not\nregular but singular statistical models, hence their generalization performance\nis still left unknown. To overcome these problems, in the previous papers, we\nproved new equations in statistical learning, by which we can estimate the\nBayes generalization loss from the Bayes training loss and the functional\nvariance, on the condition that the true distribution is a singularity\ncontained in a learning machine. In this paper, we prove that the same\nequations hold even if a true distribution is not contained in a parametric\nmodel. Also we prove that, the proposed equations in a regular case are\nasymptotically equivalent to the Takeuchi information criterion. Therefore, the\nproposed equations are always applicable without any condition on the unknown\ntrue distribution.\n",
          "  We consider computation of permanent of a positive $(N\\times N)$ non-negative\nmatrix, $P=(P_i^j|i,j=1,\\cdots,N)$, or equivalently the problem of weighted\ncounting of the perfect matchings over the complete bipartite graph $K_{N,N}$.\nThe problem is known to be of likely exponential complexity. Stated as the\npartition function $Z$ of a graphical model, the problem allows exact Loop\nCalculus representation [Chertkov, Chernyak '06] in terms of an interior\nminimum of the Bethe Free Energy functional over non-integer doubly stochastic\nmatrix of marginal beliefs, $\\beta=(\\beta_i^j|i,j=1,\\cdots,N)$, also\ncorrespondent to a fixed point of the iterative message-passing algorithm of\nthe Belief Propagation (BP) type. Our main result is an explicit expression of\nthe exact partition function (permanent) in terms of the matrix of BP\nmarginals, $\\beta$, as $Z=\\mbox{Perm}(P)=Z_{BP}\n\\mbox{Perm}(\\beta_i^j(1-\\beta_i^j))/\\prod_{i,j}(1-\\beta_i^j)$, where $Z_{BP}$\nis the BP expression for the permanent stated explicitly in terms if $\\beta$.\nWe give two derivations of the formula, a direct one based on the Bethe Free\nEnergy and an alternative one combining the Ihara graph-$\\zeta$ function and\nthe Loop Calculus approaches. Assuming that the matrix $\\beta$ of the Belief\nPropagation marginals is calculated, we provide two lower bounds and one\nupper-bound to estimate the multiplicative term. Two complementary lower bounds\nare based on the Gurvits-van der Waerden theorem and on a relation between the\nmodified permanent and determinant respectively.\n",
          "  In this article, we derive a new generalization of Chebyshev inequality for\nrandom vectors. We demonstrate that the new generalization is much less\nconservative than the classical generalization.\n",
          "  In the study of computer codes, filling space as uniformly as possible is\nimportant to describe the complexity of the investigated phenomenon. However,\nthis property is not conserved by reducing the dimension. Some numeric\nexperiment designs are conceived in this sense as Latin hypercubes or\northogonal arrays, but they consider only the projections onto the axes or the\ncoordinate planes. In this article we introduce a statistic which allows\nstudying the good distribution of points according to all 1-dimensional\nprojections. By angularly scanning the domain, we obtain a radar type\nrepresentation, allowing the uniformity defects of a design to be identified\nwith respect to its projections onto straight lines. The advantages of this new\ntool are demonstrated on usual examples of space-filling designs (SFD) and a\nglobal statistic independent of the angle of rotation is studied.\n",
          "  This paper concerns the construction of tests for universal hypothesis\ntesting problems, in which the alternate hypothesis is poorly modeled and the\nobservation space is large. The mismatched universal test is a feature-based\ntechnique for this purpose. In prior work it is shown that its\nfinite-observation performance can be much better than the (optimal) Hoeffding\ntest, and good performance depends crucially on the choice of features. The\ncontributions of this paper include: 1) We obtain bounds on the number of\n\\epsilon distinguishable distributions in an exponential family. 2) This\nmotivates a new framework for feature extraction, cast as a rank-constrained\noptimization problem. 3) We obtain a gradient-based algorithm to solve the\nrank-constrained optimization problem and prove its local convergence.\n",
          "  We consider the problem of learning the structure of Ising models (pairwise\nbinary Markov random fields) from i.i.d. samples. While several methods have\nbeen proposed to accomplish this task, their relative merits and limitations\nremain somewhat obscure. By analyzing a number of concrete examples, we show\nthat low-complexity algorithms systematically fail when the Markov random field\ndevelops long-range correlations. More precisely, this phenomenon appears to be\nrelated to the Ising model phase transition (although it does not coincide with\nit).\n",
          "  Recently Kutin and Niyogi investigated several notions of algorithmic\nstability--a property of a learning map conceptually similar to\ncontinuity--showing that training-stability is sufficient for consistency of\nEmpirical Risk Minimization while distribution-free CV-stability is necessary\nand sufficient for having finite VC-dimension. This paper concerns a phase\ntransition in the training stability of ERM, conjectured by the same authors.\nKutin and Niyogi proved that ERM on finite hypothesis spaces containing a\nunique risk minimizer has training stability that scales exponentially with\nsample size, and conjectured that the existence of multiple risk minimizers\nprevents even super-quadratic convergence. We prove this result for the\nstrictly weaker notion of CV-stability, positively resolving the conjecture.\n",
          "  We give polynomial-time algorithms for the exact computation of lowest-energy\n(ground) states, worst margin violators, log partition functions, and marginal\nedge probabilities in certain binary undirected graphical models. Our approach\nprovides an interesting alternative to the well-known graph cut paradigm in\nthat it does not impose any submodularity constraints; instead we require\nplanarity to establish a correspondence with perfect matchings (dimer\ncoverings) in an expanded dual graph. We implement a unified framework while\ndelegating complex but well-understood subproblems (planar embedding,\nmaximum-weight perfect matching) to established algorithms for which efficient\nimplementations are freely available. Unlike graph cut methods, we can perform\npenalized maximum-likelihood as well as maximum-margin parameter estimation in\nthe associated conditional random fields (CRFs), and employ marginal posterior\nprobabilities as well as maximum a posteriori (MAP) states for prediction.\nMaximum-margin CRF parameter estimation on image denoising and segmentation\nproblems shows our approach to be efficient and effective. A C++ implementation\nis available from http://nic.schraudolph.org/isinf/\n",
          "  In this paper, we consider the nonasymptotic sequential estimation of means\nof random variables bounded in between zero and one. We have rigorously\ndemonstrated that, in order to guarantee prescribed relative precision and\nconfidence level, it suffices to continue sampling until the sample sum is no\nless than a certain bound and then take the average of samples as an estimate\nfor the mean of the bounded random variable. We have developed an explicit\nformula and a bisection search method for the determination of such bound of\nsample sum, without any knowledge of the bounded variable. Moreover, we have\nderived bounds for the distribution of sample size. In the special case of\nBernoulli random variables, we have established analytical and numerical\nmethods to further reduce the bound of sample sum and thus improve the\nefficiency of sampling. Furthermore, the fallacy of existing results are\ndetected and analyzed.\n",
          "  The problem of joint universal source coding and modeling, treated in the\ncontext of lossless codes by Rissanen, was recently generalized to fixed-rate\nlossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We\nextend these results to variable-rate lossy block coding of stationary ergodic\nsources and show that, for bounded metric distortion measures, any finitely\nparametrized family of stationary sources satisfying suitable mixing,\nsmoothness and Vapnik-Chervonenkis learnability conditions admits universal\nschemes for joint lossy source coding and identification. We also give several\nexplicit examples of parametric sources satisfying the regularity conditions.\n",
          "  We study the empirical meaning of randomness with respect to a family of\nprobability distributions $P_\\theta$, where $\\theta$ is a real parameter, using\nalgorithmic randomness theory. In the case when for a computable probability\ndistribution $P_\\theta$ an effectively strongly consistent estimate exists, we\nshow that the Levin's a priory semicomputable semimeasure of the set of all\n$P_\\theta$-random sequences is positive if and only if the parameter $\\theta$\nis a computable real number. The different methods for generating\n``meaningful'' $P_\\theta$-random sequences with noncomputable $\\theta$ are\ndiscussed.\n",
          "  In this paper, we examine the CE method in the broad context of Monte Carlo\nOptimization (MCO) and Parametric Learning (PL), a type of machine learning. A\nwell-known overarching principle used to improve the performance of many PL\nalgorithms is the bias-variance tradeoff. This tradeoff has been used to\nimprove PL algorithms ranging from Monte Carlo estimation of integrals, to\nlinear estimation, to general statistical estimation. Moreover, as described\nby, MCO is very closely related to PL. Owing to this similarity, the\nbias-variance tradeoff affects MCO performance, just as it does PL performance.\n  In this article, we exploit the bias-variance tradeoff to enhance the\nperformance of MCO algorithms. We use the technique of cross-validation, a\ntechnique based on the bias-variance tradeoff, to significantly improve the\nperformance of the Cross Entropy (CE) method, which is an MCO algorithm. In\nprevious work we have confirmed that other PL techniques improve the perfomance\nof other MCO algorithms. We conclude that the many techniques pioneered in PL\ncould be investigated as ways to improve MCO algorithms in general, and the CE\nmethod in particular.\n",
          "  Inferring the sequence of states from observations is one of the most\nfundamental problems in Hidden Markov Models. In statistical physics language,\nthis problem is equivalent to computing the marginals of a one-dimensional\nmodel with a random external field. While this task can be accomplished through\ntransfer matrix methods, it becomes quickly intractable when the underlying\nstate space is large.\n  This paper develops several low-complexity approximate algorithms to address\nthis inference problem when the state space becomes large. The new algorithms\nare based on various mean-field approximations of the transfer matrix. Their\nperformances are studied in detail on a simple realistic model for DNA\npyrosequencing.\n",
          "  The problem of statistical learning is to construct a predictor of a random\nvariable $Y$ as a function of a related random variable $X$ on the basis of an\ni.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\npredictors are drawn from some specified class, and the goal is to approach\nasymptotically the performance (expected loss) of the best predictor in the\nclass. We consider the setting in which one has perfect observation of the\n$X$-part of the sample, while the $Y$-part has to be communicated at some\nfinite bit rate. The encoding of the $Y$-values is allowed to depend on the\n$X$-values. Under suitable regularity conditions on the admissible predictors,\nthe underlying family of probability distributions and the loss function, we\ngive an information-theoretic characterization of achievable predictor\nperformance in terms of conditional distortion-rate functions. The ideas are\nillustrated on the example of nonparametric regression in Gaussian noise.\n",
          "  We prove existence and uniqueness of the minimizer for the average geodesic\ndistance to the points of a geodesically convex set on the sphere. This implies\na corresponding existence and uniqueness result for an optimal algorithm for\nhalfspace learning, when data and target functions are drawn from the uniform\ndistribution.\n",
          "  Bounds on the risk play a crucial role in statistical learning theory. They\nusually involve as capacity measure of the model studied the VC dimension or\none of its extensions. In classification, such \"VC dimensions\" exist for models\ntaking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations\nappropriate for the missing case, the one of models with values in R^Q. This\nprovides us with a new guaranteed risk for M-SVMs which appears superior to the\nexisting one.\n",
          "  For the universal hypothesis testing problem, where the goal is to decide\nbetween the known null hypothesis distribution and some other unknown\ndistribution, Hoeffding proposed a universal test in the nineteen sixties.\nHoeffding's universal test statistic can be written in terms of\nKullback-Leibler (K-L) divergence between the empirical distribution of the\nobservations and the null hypothesis distribution. In this paper a modification\nof Hoeffding's test is considered based on a relaxation of the K-L divergence\ntest statistic, referred to as the mismatched divergence. The resulting\nmismatched test is shown to be a generalized likelihood-ratio test (GLRT) for\nthe case where the alternate distribution lies in a parametric family of the\ndistributions characterized by a finite dimensional parameter, i.e., it is a\nsolution to the corresponding composite hypothesis testing problem. For certain\nchoices of the alternate distribution, it is shown that both the Hoeffding test\nand the mismatched test have the same asymptotic performance in terms of error\nexponents. A consequence of this result is that the GLRT is optimal in\ndifferentiating a particular distribution from others in an exponential family.\nIt is also shown that the mismatched test has a significant advantage over the\nHoeffding test in terms of finite sample size performance. This advantage is\ndue to the difference in the asymptotic variances of the two test statistics\nunder the null hypothesis. In particular, the variance of the K-L divergence\ngrows linearly with the alphabet size, making the test impractical for\napplications involving large alphabet distributions. The variance of the\nmismatched divergence on the other hand grows linearly with the dimension of\nthe parameter space, and can hence be controlled through a prudent choice of\nthe function class defining the mismatched divergence.\n",
          "  There has been a tremendous growth in publicly available digital video\nfootage over the past decade. This has necessitated the development of new\ntechniques in computer vision geared towards efficient analysis, storage and\nretrieval of such data. Many mid-level computer vision tasks such as\nsegmentation, object detection, tracking, etc. involve an inference problem\nbased on the video data available. Video data has a high degree of spatial and\ntemporal coherence. The property must be intelligently leveraged in order to\nobtain better results.\n  Graphical models, such as Markov Random Fields, have emerged as a powerful\ntool for such inference problems. They are naturally suited for expressing the\nspatial dependencies present in video data, It is however, not clear, how to\nextend the existing techniques for the problem of inference over time. This\nthesis explores the Path Probability Method, a variational technique in\nstatistical mechanics, in the context of graphical models and approximate\ninference problems. It extends the method to a general framework for problems\ninvolving inference in time, resulting in an algorithm, \\emph{DynBP}. We\nexplore the relation of the algorithm with existing techniques, and find the\nalgorithm competitive with existing approaches.\n  The main contribution of this thesis are the extended GBP algorithm, the\nextension of Path Probability Methods to the DynBP algorithm and the\nrelationship between them. We have also explored some applications in computer\nvision involving temporal evolution with promising results.\n",
          "  We study the problem of learning k-juntas given access to examples drawn from\na number of different product distributions. Thus we wish to learn a function f\n: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best\nknown algorithms for the general problem of learning a k-junta require running\ntime of n^k * poly(n,2^k), we show that given access to k different product\ndistributions with biases separated by \\gamma>0, the functions may be learned\nin time poly(n,2^k,\\gamma^{-k}). More generally, given access to t <= k\ndifferent product distributions, the functions may be learned in time n^{k/t} *\npoly(n,2^k,\\gamma^{-k}). Our techniques involve novel results in Fourier\nanalysis relating Fourier expansions with respect to different biases and a\ngeneralization of Russo's formula.\n",
          "  Information distance is a parameter-free similarity measure based on\ncompression, used in pattern recognition, data mining, phylogeny, clustering,\nand classification. The notion of information distance is extended from pairs\nto multiples (finite lists). We study maximal overlap, metricity, universality,\nminimal overlap, additivity, and normalized information distance in multiples.\nWe use the theoretical notion of Kolmogorov complexity which for practical\npurposes is approximated by the length of the compressed version of the file\ninvolved, using a real-world compression program.\n  {\\em Index Terms}-- Information distance, multiples, pattern recognition,\ndata mining, similarity, Kolmogorov complexity\n",
          "  We describe a novel approach to statistical learning from particles tracked\nwhile moving in a random environment. The problem consists in inferring\nproperties of the environment from recorded snapshots. We consider here the\ncase of a fluid seeded with identical passive particles that diffuse and are\nadvected by a flow. Our approach rests on efficient algorithms to estimate the\nweighted number of possible matchings among particles in two consecutive\nsnapshots, the partition function of the underlying graphical model. The\npartition function is then maximized over the model parameters, namely\ndiffusivity and velocity gradient. A Belief Propagation (BP) scheme is the\nbackbone of our algorithm, providing accurate results for the flow parameters\nwe want to learn. The BP estimate is additionally improved by incorporating\nLoop Series (LS) contributions. For the weighted matching problem, LS is\ncompactly expressed as a Cauchy integral, accurately estimated by a saddle\npoint approximation. Numerical experiments show that the quality of our\nimproved BP algorithm is comparable to the one of a fully polynomial randomized\napproximation scheme, based on the Markov Chain Monte Carlo (MCMC) method,\nwhile the BP-based scheme is substantially faster than the MCMC scheme.\n",
          "  In this paper we consider the problem of reconstructing a hidden weighted\nhypergraph of constant rank using additive queries. We prove the following: Let\n$G$ be a weighted hidden hypergraph of constant rank with n vertices and $m$\nhyperedges. For any $m$ there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log n}{\\log m}) $$\nadditive queries. This solves the open problem in [S. Choi, J. H. Kim. Optimal\nQuery Complexity Bounds for Finding Graphs. {\\em STOC}, 749--758,~2008].\n  When the weights of the hypergraph are integers that are less than\n$O(poly(n^d/m))$ where $d$ is the rank of the hypergraph (and therefore for\nunweighted hypergraphs) there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log \\frac{n^d}{m}}{\\log\nm}). $$ additive queries.\n  Using the information theoretic bound the above query complexities are tight.\n",
          "  In this paper, we have established a new framework of truncated inverse\nsampling for estimating mean values of non-negative random variables such as\nbinomial, Poisson, hyper-geometrical, and bounded variables. We have derived\nexplicit formulas and computational methods for designing sampling schemes to\nensure prescribed levels of precision and confidence for point estimators.\nMoreover, we have developed interval estimation methods.\n",
          "  In the context of inference with expectation constraints, we propose an\napproach based on the \"loopy belief propagation\" algorithm LBP, as a surrogate\nto an exact Markov Random Field MRF modelling. A prior information composed of\ncorrelations among a large set of N variables, is encoded into a graphical\nmodel; this encoding is optimized with respect to an approximate decoding\nprocedure LBP, which is used to infer hidden variables from an observed subset.\nWe focus on the situation where the underlying data have many different\nstatistical components, representing a variety of independent patterns.\nConsidering a single parameter family of models we show how LBP may be used to\nencode and decode efficiently such information, without solving the NP hard\ninverse problem yielding the optimal MRF. Contrary to usual practice, we work\nin the non-convex Bethe free energy minimization framework, and manage to\nassociate a belief propagation fixed point to each component of the underlying\nprobabilistic mixture. The mean field limit is considered and yields an exact\nconnection with the Hopfield model at finite temperature and steady state, when\nthe number of mixture components is proportional to the number of variables. In\naddition, we provide an enhanced learning procedure, based on a straightforward\nmulti-parameter extension of the model in conjunction with an effective\ncontinuous optimization procedure. This is performed using the stochastic\nsearch heuristic CMAES and yields a significant improvement with respect to the\nsingle parameter basic model.\n",
          "  We introduce an approach to inferring the causal architecture of stochastic\ndynamical systems that extends rate distortion theory to use causal\nshielding---a natural principle of learning. We study two distinct cases of\ncausal inference: optimal causal filtering and optimal causal estimation.\n  Filtering corresponds to the ideal case in which the probability distribution\nof measurement sequences is known, giving a principled method to approximate a\nsystem's causal structure at a desired level of representation. We show that,\nin the limit in which a model complexity constraint is relaxed, filtering finds\nthe exact causal architecture of a stochastic dynamical system, known as the\ncausal-state partition. From this, one can estimate the amount of historical\ninformation the process stores. More generally, causal filtering finds a graded\nmodel-complexity hierarchy of approximations to the causal architecture. Abrupt\nchanges in the hierarchy, as a function of approximation, capture distinct\nscales of structural organization.\n  For nonideal cases with finite data, we show how the correct number of\nunderlying causal states can be found by optimal causal estimation. A\npreviously derived model complexity control term allows us to correct for the\neffect of statistical fluctuations in probability estimates and thereby avoid\nover-fitting.\n",
          "  Hidden Markov Models (HMMs) are one of the most fundamental and widely used\nstatistical tools for modeling discrete time series. In general, learning HMMs\nfrom data is computationally hard (under cryptographic assumptions), and\npractitioners typically resort to search heuristics which suffer from the usual\nlocal optima issues. We prove that under a natural separation condition (bounds\non the smallest singular value of the HMM parameters), there is an efficient\nand provably correct algorithm for learning HMMs. The sample complexity of the\nalgorithm does not explicitly depend on the number of distinct (discrete)\nobservations---it implicitly depends on this quantity through spectral\nproperties of the underlying HMM. This makes the algorithm particularly\napplicable to settings with a large number of observations, such as those in\nnatural language processing where the space of observation is sometimes the\nwords in a language. The algorithm is also simple, employing only a singular\nvalue decomposition and matrix multiplications.\n",
          "  Learning machines which have hierarchical structures or hidden variables are\nsingular statistical models because they are nonidentifiable and their Fisher\ninformation matrices are singular. In singular statistical models, neither the\nBayes a posteriori distribution converges to the normal distribution nor the\nmaximum likelihood estimator satisfies asymptotic normality. This is the main\nreason why it has been difficult to predict their generalization performances\nfrom trained states. In this paper, we study four errors, (1) Bayes\ngeneralization error, (2) Bayes training error, (3) Gibbs generalization error,\nand (4) Gibbs training error, and prove that there are mathematical relations\namong these errors. The formulas proved in this paper are equations of states\nin statistical estimation because they hold for any true distribution, any\nparametric model, and any a priori distribution. Also we show that Bayes and\nGibbs generalization errors are estimated by Bayes and Gibbs training errors,\nand propose widely applicable information criteria which can be applied to both\nregular and singular statistical models.\n",
          "  We study the problem of estimating the time delay between two signals\nrepresenting delayed, irregularly sampled and noisy versions of the same\nunderlying pattern. We propose and demonstrate an evolutionary algorithm for\nthe (hyper)parameter estimation of a kernel-based technique in the context of\nan astronomical problem, namely estimating the time delay between two\ngravitationally lensed signals from a distant quasar. Mixed types (integer and\nreal) are used to represent variables within the evolutionary algorithm. We\ntest the algorithm on several artificial data sets, and also on real\nastronomical observations of quasar Q0957+561. By carrying out a statistical\nanalysis of the results we present a detailed comparison of our method with the\nmost popular methods for time delay estimation in astrophysics. Our method\nyields more accurate and more stable time delay estimates: for Q0957+561, we\nobtain 419.6 days for the time delay between images A and B. Our methodology\ncan be readily applied to current state-of-the-art optical monitoring data in\nastronomy, but can also be applied in other disciplines involving similar time\nseries data.\n",
          "  The Sample Compression Conjecture of Littlestone & Warmuth has remained\nunsolved for over two decades. This paper presents a systematic geometric\ninvestigation of the compression of finite maximum concept classes. Simple\narrangements of hyperplanes in Hyperbolic space, and Piecewise-Linear\nhyperplane arrangements, are shown to represent maximum classes, generalizing\nthe corresponding Euclidean result. A main result is that PL arrangements can\nbe swept by a moving hyperplane to unlabeled d-compress any finite maximum\nclass, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary\nis that some d-maximal classes cannot be embedded into any maximum class of VC\ndimension d+k, for any constant k. The construction of the PL sweeping involves\nPachner moves on the one-inclusion graph, corresponding to moves of a\nhyperplane across the intersection of d other hyperplanes. This extends the\nwell known Pachner moves for triangulations to cubical complexes.\n",
          "  We analyse the prequential plug-in codes relative to one-parameter\nexponential families M. We show that if data are sampled i.i.d. from some\ndistribution outside M, then the redundancy of any plug-in prequential code\ngrows at rate larger than 1/2 ln(n) in the worst case. This means that plug-in\ncodes, such as the Rissanen-Dawid ML code, may behave inferior to other\nimportant universal codes such as the 2-part MDL, Shtarkov and Bayes codes, for\nwhich the redundancy is always 1/2 ln(n) + O(1). However, we also show that a\nslight modification of the ML plug-in code, \"almost\" in the model, does achieve\nthe optimal redundancy even if the the true distribution is outside M.\n",
          "  Least squares (LS) fitting is one of the most fundamental techniques in\nscience and engineering. It is used to estimate parameters from multiple noisy\nobservations. In many problems the parameters are known a-priori to be bounded\ninteger valued, or they come from a finite set of values on an arbitrary finite\nlattice. In this case finding the closest vector becomes NP-Hard problem. In\nthis paper we propose a novel algorithm, the Tomographic Least Squares Decoder\n(TLSD), that not only solves the ILS problem, better than other sub-optimal\ntechniques, but also is capable of providing the a-posteriori probability\ndistribution for each element in the solution vector. The algorithm is based on\nreconstruction of the vector from multiple two-dimensional projections. The\nprojections are carefully chosen to provide low computational complexity.\nUnlike other iterative techniques, such as the belief propagation, the proposed\nalgorithm has ensured convergence. We also provide simulated experiments\ncomparing the algorithm to other sub-optimal algorithms.\n",
          "  In this paper, we consider the coherent theory of (epistemic) uncertainty of\nWalley, in which beliefs are represented through sets of probability\ndistributions, and we focus on the problem of modeling prior ignorance about a\ncategorical random variable. In this setting, it is a known result that a state\nof prior ignorance is not compatible with learning. To overcome this problem,\nanother state of beliefs, called \\emph{near-ignorance}, has been proposed.\nNear-ignorance resembles ignorance very closely, by satisfying some principles\nthat can arguably be regarded as necessary in a state of ignorance, and allows\nlearning to take place. What this paper does, is to provide new and substantial\nevidence that also near-ignorance cannot be really regarded as a way out of the\nproblem of starting statistical inference in conditions of very weak beliefs.\nThe key to this result is focusing on a setting characterized by a variable of\ninterest that is \\emph{latent}. We argue that such a setting is by far the most\ncommon case in practice, and we provide, for the case of categorical latent\nvariables (and general \\emph{manifest} variables) a condition that, if\nsatisfied, prevents learning to take place under prior near-ignorance. This\ncondition is shown to be easily satisfied even in the most common statistical\nproblems. We regard these results as a strong form of evidence against the\npossibility to adopt a condition of prior near-ignorance in real statistical\nproblems.\n",
          "  This document describes concisely the ubiquitous class of exponential family\ndistributions met in statistics. The first part recalls definitions and\nsummarizes main properties and duality with Bregman divergences (all proofs are\nskipped). The second part lists decompositions and related formula of common\nexponential family distributions. We recall the Fisher-Rao-Riemannian\ngeometries and the dual affine connection information geometries of statistical\nmanifolds. It is intended to maintain and update this document and catalog by\nadding new distribution items.\n",
          "  As a fundamental problem in pattern recognition, graph matching has\napplications in a variety of fields, from computer vision to computational\nbiology. In graph matching, patterns are modeled as graphs and pattern\nrecognition amounts to finding a correspondence between the nodes of different\ngraphs. Many formulations of this problem can be cast in general as a quadratic\nassignment problem, where a linear term in the objective function encodes node\ncompatibility and a quadratic term encodes edge compatibility. The main\nresearch focus in this theme is about designing efficient algorithms for\napproximately solving the quadratic assignment problem, since it is NP-hard. In\nthis paper we turn our attention to a different question: how to estimate\ncompatibility functions such that the solution of the resulting graph matching\nproblem best matches the expected solution that a human would manually provide.\nWe present a method for learning graph matching: the training examples are\npairs of graphs and the `labels' are matches between them. Our experimental\nresults reveal that learning can substantially improve the performance of\nstandard graph matching algorithms. In particular, we find that simple linear\nassignment with such a learning scheme outperforms Graduated Assignment with\nbistochastic normalisation, a state-of-the-art quadratic assignment relaxation\nalgorithm.\n",
          "  Bayes statistics and statistical physics have the common mathematical\nstructure, where the log likelihood function corresponds to the random\nHamiltonian. Recently, it was discovered that the asymptotic learning curves in\nBayes estimation are subject to a universal law, even if the log likelihood\nfunction can not be approximated by any quadratic form. However, it is left\nunknown what mathematical property ensures such a universal law. In this paper,\nwe define a renormalizable condition of the statistical estimation problem, and\nshow that, under such a condition, the asymptotic learning curves are ensured\nto be subject to the universal law, even if the true distribution is\nunrealizable and singular for a statistical model. Also we study a\nnonrenormalizable case, in which the learning curves have the different\nasymptotic behaviors from the universal law.\n",
          "  We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend\non an unknown subset of k<<n variables (so-called k-juntas) is agnostically\nlearnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k},\nand log(1/delta). In other words, there is an algorithm with the claimed\nrunning time that, given epsilon, delta > 0 and access to a random walk on\n{-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with\nprobability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f,\nwhere opt(f) denotes the distance of a closest k-junta to f.\n",
          "  This paper uncovers and explores the close relationship between Monte Carlo\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\ncontributions. First, we prove that MCO is mathematically identical to a broad\nclass of PL problems. This identity potentially provides a new application\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\nfor blackbox optimization. Immediate sampling transforms the original BO\nproblem into an MCO problem. Accordingly, by combining these first two\ncontributions, we can apply all PL techniques to BO. In our third contribution\nwe validate this way of improving BO by demonstrating that cross-validation and\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\nignore the relationship between the sample point locations and the associated\nvalues of the integrand; only the values of the integrand at those locations\nare considered. We demonstrate that one can exploit the sample location\ninformation using PL techniques, for example by forming a fit of the sample\nlocations to the associated values of the integrand. This provides an\nadditional way to apply PL techniques to improve MCO.\n",
          "  We present several theoretical contributions which allow Lie groups to be fit\nto high dimensional datasets. Transformation operators are represented in their\neigen-basis, reducing the computational complexity of parameter estimation to\nthat of training a linear transformation model. A transformation specific\n\"blurring\" operator is introduced that allows inference to escape local minima\nvia a smoothing of the transformation space. A penalty on traversed manifold\ndistance is added which encourages the discovery of sparse, minimal distance,\ntransformations between states. Both learning and inference are demonstrated\nusing these methods for the full set of affine transformations on natural image\npatches. Transformation operators are then trained on natural video sequences.\nIt is shown that the learned video transformations provide a better description\nof inter-frame differences than the standard motion model based on rigid\ntranslation.\n",
          "  Statistical learning theory chiefly studies restricted hypothesis classes,\nparticularly those with finite Vapnik-Chervonenkis (VC) dimension. The\nfundamental quantity of interest is the sample complexity: the number of\nsamples required to learn to a specified level of accuracy. Here we consider\nlearning over the set of all computable labeling functions. Since the\nVC-dimension is infinite and a priori (uniform) bounds on the number of samples\nare impossible, we let the learning algorithm decide when it has seen\nsufficient samples to have learned. We first show that learning in this setting\nis indeed possible, and develop a learning algorithm. We then show, however,\nthat bounding sample complexity independently of the distribution is\nimpossible. Notably, this impossibility is entirely due to the requirement that\nthe learning algorithm be computable, and not due to the statistical nature of\nthe problem.\n",
          "  In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "0_generalization_models_markov",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "0_generalization_models_markov"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          4.3634467124938965,
          5.145861625671387,
          4.456152439117432,
          3.761728048324585,
          3.905818462371826,
          4.761734485626221,
          4.261229991912842,
          2.8932244777679443,
          1.806530237197876,
          3.8067073822021484,
          3.709998607635498,
          5.050747394561768,
          2.8850057125091553,
          5.271956443786621,
          3.413604259490967,
          4.452112674713135,
          3.9083006381988525,
          3.7905280590057373,
          3.011533260345459,
          5.0080437660217285,
          4.893697261810303,
          3.5489418506622314,
          3.605297327041626,
          2.989039659500122,
          3.807736396789551,
          3.9402620792388916,
          3.3073887825012207,
          5.472387313842773,
          4.398123264312744,
          3.4322659969329834,
          4.292411804199219,
          4.257917881011963,
          4.178359508514404,
          3.492339611053467,
          5.459867000579834,
          4.803164958953857,
          5.311474800109863,
          4.332207202911377,
          3.8137030601501465,
          5.313658237457275,
          5.155739784240723,
          3.7850849628448486,
          3.736818313598633,
          4.17557430267334,
          3.700730800628662,
          3.6375551223754883,
          3.716599225997925,
          3.3396801948547363,
          2.9261155128479004,
          2.9501094818115234,
          4.38878870010376,
          5.205348968505859,
          3.322537899017334,
          4.862344264984131,
          3.33764910697937,
          4.5200514793396,
          4.839232444763184,
          4.501583099365234,
          3.919428586959839,
          5.312506198883057,
          4.788656711578369,
          3.3859803676605225,
          4.907074451446533,
          5.251570224761963,
          4.369256019592285,
          2.648247003555298,
          5.039183139801025,
          1.6338742971420288,
          4.802367687225342,
          3.4491467475891113,
          3.85778546333313,
          3.1449837684631348,
          3.6640241146087646,
          3.9048802852630615,
          4.5982441902160645,
          3.395416259765625,
          4.389398097991943,
          4.972628593444824,
          5.173701763153076,
          4.688488960266113,
          3.888563632965088,
          4.914838790893555,
          3.1334152221679688,
          4.082228183746338,
          3.9631218910217285,
          3.470106363296509,
          4.1001973152160645,
          5.240050315856934,
          3.713549852371216,
          3.6064603328704834,
          4.35324764251709,
          3.6837635040283203,
          5.361417293548584,
          3.9667739868164062,
          4.842081546783447,
          2.7355144023895264,
          3.4772000312805176,
          5.0449957847595215,
          2.8434231281280518,
          5.048433303833008,
          4.916285037994385,
          3.154965400695801,
          4.856678009033203,
          4.000666618347168,
          4.528656959533691,
          3.1343655586242676,
          4.352524757385254,
          3.3194034099578857,
          4.087461471557617
         ],
         "y": [
          7.145566463470459,
          6.724524021148682,
          6.572010040283203,
          7.63712739944458,
          9.102938652038574,
          6.795375823974609,
          6.953436851501465,
          7.635371685028076,
          6.839983940124512,
          7.539761066436768,
          6.951223850250244,
          7.6367669105529785,
          7.633218288421631,
          6.885811805725098,
          8.81020450592041,
          7.044114112854004,
          9.148823738098145,
          8.067032814025879,
          8.945351600646973,
          6.439461708068848,
          6.5505781173706055,
          9.245950698852539,
          7.584689140319824,
          7.645162582397461,
          7.5518035888671875,
          9.13479995727539,
          7.8322014808654785,
          6.675786018371582,
          7.15917444229126,
          7.975696086883545,
          7.1558661460876465,
          7.31020975112915,
          6.650010108947754,
          7.865749835968018,
          7.365853309631348,
          7.278332710266113,
          6.896151542663574,
          6.475751876831055,
          7.981051445007324,
          6.908955097198486,
          6.48028564453125,
          7.5098042488098145,
          8.421796798706055,
          6.598416328430176,
          8.742048263549805,
          7.821240425109863,
          8.248994827270508,
          8.842022895812988,
          8.232254981994629,
          8.674646377563477,
          6.525372505187988,
          7.441253662109375,
          8.589088439941406,
          6.848601818084717,
          8.880108833312988,
          7.300721168518066,
          7.448217868804932,
          7.120548248291016,
          7.606194019317627,
          6.656773567199707,
          7.150290489196777,
          8.039167404174805,
          7.354597091674805,
          6.907458782196045,
          7.733904838562012,
          7.758325576782227,
          7.541192531585693,
          6.69832181930542,
          7.4761643409729,
          8.758401870727539,
          7.515727996826172,
          7.636531352996826,
          6.891966342926025,
          8.491347312927246,
          7.381916046142578,
          8.415135383605957,
          6.483927249908447,
          7.659796237945557,
          6.778717994689941,
          6.420903205871582,
          9.085967063903809,
          7.393725872039795,
          7.549602508544922,
          7.1964006423950195,
          6.794145584106445,
          8.578733444213867,
          7.398618698120117,
          6.801314353942871,
          8.637446403503418,
          8.02762222290039,
          6.489463806152344,
          8.676933288574219,
          6.927376747131348,
          9.149042129516602,
          7.442593097686768,
          7.426705360412598,
          7.413855075836182,
          7.626134872436523,
          8.785968780517578,
          6.466955661773682,
          7.466377258300781,
          7.97160005569458,
          7.394524097442627,
          7.426892280578613,
          6.468868255615234,
          8.56221866607666,
          7.244844436645508,
          8.772211074829102,
          7.574169158935547
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  Adaptive control problems are notoriously difficult to solve even in the\npresence of plant-specific controllers. One way to by-pass the intractable\ncomputation of the optimal policy is to restate the adaptive control as the\nminimization of the relative entropy of a controller that ignores the true\nplant dynamics from an informed controller. The solution is given by the\nBayesian control rule-a set of equations characterizing a stochastic adaptive\ncontroller for the class of possible plant dynamics. Here, the Bayesian control\nrule is applied to derive BCR-MDP, a controller to solve undiscounted Markov\ndecision processes with finite state and action spaces and unknown dynamics. In\nparticular, we derive a non-parametric conjugate prior distribution over the\npolicy space that encapsulates the agent's whole relevant history and we\npresent a Gibbs sampler to draw random policies from this distribution.\nPreliminary results show that BCR-MDP successfully avoids sub-optimal limit\ncycles due to its built-in mechanism to balance exploration versus\nexploitation.\n",
          "  The aim of this work is to address the question of whether we can in\nprinciple design rational decision-making agents or artificial intelligences\nembedded in computable physics such that their decisions are optimal in\nreasonable mathematical senses. Recent developments in rare event probability\nestimation, recursive bayesian inference, neural networks, and probabilistic\nplanning are sufficient to explicitly approximate reinforcement learners of the\nAIXI style with non-trivial model classes (here, the class of resource-bounded\nTuring machines). Consideration of the effects of resource limitations in a\nconcrete implementation leads to insights about possible architectures for\nlearning systems using optimal decision makers as components.\n",
          "  General-purpose, intelligent, learning agents cycle through sequences of\nobservations, actions, and rewards that are complex, uncertain, unknown, and\nnon-Markovian. On the other hand, reinforcement learning is well-developed for\nsmall finite state Markov decision processes (MDPs). Up to now, extracting the\nright state representations out of bare observations, that is, reducing the\ngeneral agent setup to the MDP framework, is an art that involves significant\neffort by designers. The primary goal of this work is to automate the reduction\nprocess and thereby significantly expand the scope of many existing\nreinforcement learning algorithms and the agents that employ them. Before we\ncan think of mechanizing this search for suitable MDPs, we need a formal\nobjective criterion. The main contribution of this article is to develop such a\ncriterion. I also integrate the various parts into one learning algorithm.\nExtensions to more realistic dynamic Bayesian networks are developed in Part\nII. The role of POMDPs is also considered there.\n",
          "  We use co-evolutionary genetic algorithms to model the players' learning\nprocess in several Cournot models, and evaluate them in terms of their\nconvergence to the Nash Equilibrium. The \"social-learning\" versions of the two\nco-evolutionary algorithms we introduce, establish Nash Equilibrium in those\nmodels, in contrast to the \"individual learning\" versions which, as we see\nhere, do not imply the convergence of the players' strategies to the Nash\noutcome. When players use \"canonical co-evolutionary genetic algorithms\" as\nlearning algorithms, the process of the game is an ergodic Markov Chain, and\ntherefore we analyze simulation results using both the relevant methodology and\nmore general statistical tests, to find that in the \"social\" case, states\nleading to NE play are highly frequent at the stationary distribution of the\nchain, in contrast to the \"individual learning\" case, when NE is not reached at\nall in our simulations; to find that the expected Hamming distance of the\nstates at the limiting distribution from the \"NE state\" is significantly\nsmaller in the \"social\" than in the \"individual learning case\"; to estimate the\nexpected time that the \"social\" algorithms need to get to the \"NE state\" and\nverify their robustness and finally to show that a large fraction of the games\nplayed are indeed at the Nash Equilibrium.\n",
          "  Research in reinforcement learning has produced algorithms for optimal\ndecision making under uncertainty that fall within two main types. The first\nemploys a Bayesian framework, where optimality improves with increased\ncomputational time. This is because the resulting planning task takes the form\nof a dynamic programming problem on a belief tree with an infinite number of\nstates. The second type employs relatively simple algorithm which are shown to\nsuffer small regret within a distribution-free framework. This paper presents a\nlower bound and a high probability upper bound on the optimal value function\nfor the nodes in the Bayesian belief tree, which are analogous to similar\nbounds in POMDPs. The bounds are then used to create more efficient strategies\nfor exploring the tree. The resulting algorithms are compared with the\ndistribution-free algorithm UCB1, as well as a simpler baseline algorithm on\nmulti-armed bandit problems.\n",
          "  Feature Markov Decision Processes (PhiMDPs) are well-suited for learning\nagents in general environments. Nevertheless, unstructured (Phi)MDPs are\nlimited to relatively simple environments. Structured MDPs like Dynamic\nBayesian Networks (DBNs) are used for large-scale real-world problems. In this\narticle I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost\ncriterion that allows to automatically extract the most relevant features from\nthe environment, leading to the \"best\" DBN representation. I discuss all\nbuilding blocks required for a complete general learning algorithm.\n",
          "  We consider an agent interacting with an unmodeled environment. At each time,\nthe agent makes an observation, takes an action, and incurs a cost. Its actions\ncan influence future observations and costs. The goal is to minimize the\nlong-term average cost. We propose a novel algorithm, known as the active LZ\nalgorithm, for optimal control based on ideas from the Lempel-Ziv scheme for\nuniversal data compression and prediction. We establish that, under the active\nLZ algorithm, if there exists an integer $K$ such that the future is\nconditionally independent of the past given a window of $K$ consecutive actions\nand observations, then the average cost converges to the optimum. Experimental\nresults involving the game of Rock-Paper-Scissors illustrate merits of the\nalgorithm.\n",
          "  Several researchers have recently investigated the connection between\nreinforcement learning and classification. We are motivated by proposals of\napproximate policy iteration schemes without value functions which focus on\npolicy representation using classifiers and address policy learning as a\nsupervised learning problem. This paper proposes variants of an improved policy\niteration scheme which addresses the core sampling problem in evaluating a\npolicy through simulation as a multi-armed bandit machine. The resulting\nalgorithm offers comparable performance to the previous algorithm achieved,\nhowever, with significantly less computational effort. An order of magnitude\nimprovement is demonstrated experimentally in two standard reinforcement\nlearning domains: inverted pendulum and mountain-car.\n",
          "  Many reinforcement learning exploration techniques are overly optimistic and\ntry to explore every state. Such exploration is impossible in environments with\nthe unlimited number of states. I propose to use simulated exploration with an\noptimistic model to discover promising paths for real exploration. This reduces\nthe needs for the real exploration.\n",
          "  A central problem in artificial intelligence is that of planning to maximize\nfuture reward under uncertainty in a partially observable environment. In this\npaper we propose and demonstrate a novel algorithm which accurately learns a\nmodel of such an environment directly from sequences of action-observation\npairs. We then close the loop from observations to actions by planning in the\nlearned model and recovering a policy which is near-optimal in the original\nenvironment. Specifically, we present an efficient and statistically consistent\nspectral algorithm for learning the parameters of a Predictive State\nRepresentation (PSR). We demonstrate the algorithm by learning a model of a\nsimulated high-dimensional, vision-based mobile robot planning task, and then\nperform approximate point-based planning in the learned PSR. Analysis of our\nresults shows that the algorithm learns a state space which efficiently\ncaptures the essential features of the environment. This representation allows\naccurate prediction with a small number of parameters, and enables successful\nand efficient planning.\n",
          "  In this paper we propose an algorithm for polynomial-time reinforcement\nlearning in factored Markov decision processes (FMDPs). The factored optimistic\ninitial model (FOIM) algorithm, maintains an empirical model of the FMDP in a\nconventional way, and always follows a greedy policy with respect to its model.\nThe only trick of the algorithm is that the model is initialized\noptimistically. We prove that with suitable initialization (i) FOIM converges\nto the fixed point of approximate value iteration (AVI); (ii) the number of\nsteps when the agent makes non-near-optimal decisions (with respect to the\nsolution of AVI) is polynomial in all relevant quantities; (iii) the per-step\ncosts of the algorithm are also polynomial. To our best knowledge, FOIM is the\nfirst algorithm with these properties. This extended version contains the\nrigorous proofs of the main theorem. A version of this paper appeared in\nICML'09.\n",
          "  A technique for speeding up reinforcement learning algorithms by using time\nmanipulation is proposed. It is applicable to failure-avoidance control\nproblems running in a computer simulation. Turning the time of the simulation\nbackwards on failure events is shown to speed up the learning by 260% and\nimprove the state space exploration by 12% on the cart-pole balancing task,\ncompared to the conventional Q-learning and Actor-Critic algorithms.\n",
          "  We describe a preliminary investigation into learning a Chess player's style\nfrom game records. The method is based on attempting to learn features of a\nplayer's individual evaluation function using the method of temporal\ndifferences, with the aid of a conventional Chess engine architecture. Some\nencouraging results were obtained in learning the styles of two recent Chess\nworld champions, and we report on our attempt to use the learnt styles to\ndiscriminate between the players from game records by trying to detect who was\nplaying white and who was playing black. We also discuss some limitations of\nour approach and propose possible directions for future research. The method we\nhave presented may also be applicable to other strategic games, and may even be\ngeneralisable to other domains where sequences of agents' actions are recorded.\n",
          "  Explaining adaptive behavior is a central problem in artificial intelligence\nresearch. Here we formalize adaptive agents as mixture distributions over\nsequences of inputs and outputs (I/O). Each distribution of the mixture\nconstitutes a `possible world', but the agent does not know which of the\npossible worlds it is actually facing. The problem is to adapt the I/O stream\nin a way that is compatible with the true world. A natural measure of\nadaptation can be obtained by the Kullback-Leibler (KL) divergence between the\nI/O distribution of the true world and the I/O distribution expected by the\nagent that is uncertain about possible worlds. In the case of pure input\nstreams, the Bayesian mixture provides a well-known solution for this problem.\nWe show, however, that in the case of I/O streams this solution breaks down,\nbecause outputs are issued by the agent itself and require a different\nprobabilistic syntax as provided by intervention calculus. Based on this\ncalculus, we obtain a Bayesian control rule that allows modeling adaptive\nbehavior with mixture distributions over I/O streams. This rule might allow for\na novel approach to adaptive control based on a minimum KL-principle.\n",
          "  Recently, new approaches to adaptive control have sought to reformulate the\nproblem as a minimization of a relative entropy criterion to obtain tractable\nsolutions. In particular, it has been shown that minimizing the expected\ndeviation from the causal input-output dependencies of the true plant leads to\na new promising stochastic control rule called the Bayesian control rule. This\nwork proves the convergence of the Bayesian control rule under two sufficient\nassumptions: boundedness, which is an ergodicity condition; and consistency,\nwhich is an instantiation of the sure-thing principle.\n",
          "  This paper introduces a principled approach for the design of a scalable\ngeneral reinforcement learning agent. Our approach is based on a direct\napproximation of AIXI, a Bayesian optimality notion for general reinforcement\nlearning agents. Previously, it has been unclear whether the theory of AIXI\ncould motivate the design of practical algorithms. We answer this hitherto open\nquestion in the affirmative, by providing the first computationally feasible\napproximation to the AIXI agent. To develop our approximation, we introduce a\nnew Monte-Carlo Tree Search algorithm along with an agent-specific extension to\nthe Context Tree Weighting algorithm. Empirically, we present a set of\nencouraging results on a variety of stochastic and partially observable\ndomains. We conclude by proposing a number of directions for future research.\n",
          "  Serious Games (SGs) have experienced a tremendous outburst these last years.\nVideo game companies have been producing fun, user-friendly SGs, but their\neducational value has yet to be proven. Meanwhile, cognition research scientist\nhave been developing SGs in such a way as to guarantee an educational gain, but\nthe fun and attractive characteristics featured often would not meet the\npublic's expectations. The ideal SG must combine these two aspects while still\nbeing economically viable. In this article, we propose a production chain model\nto efficiently conceive and produce SGs that are certified for their\neducational gain and fun qualities. Each step of this chain will be described\nalong with the human actors, the tools and the documents that intervene.\n",
          "  We derive an equation for temporal difference learning from statistical\nprinciples. Specifically, we start with the variational principle and then\nbootstrap to produce an updating rule for discounted state value estimates. The\nresulting equation is similar to the standard equation for temporal difference\nlearning with eligibility traces, so called TD(lambda), however it lacks the\nparameter alpha that specifies the learning rate. In the place of this free\nparameter there is now an equation for the learning rate that is specific to\neach state transition. We experimentally test this new learning rule against\nTD(lambda) and find that it offers superior performance in various settings.\nFinally, we make some preliminary investigations into how to extend our new\ntemporal difference algorithm to reinforcement learning. To do this we combine\nour update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find\nthat it again offers superior performance without a learning rate parameter.\n",
          "  Specialized intelligent systems can be found everywhere: finger print,\nhandwriting, speech, and face recognition, spam filtering, chess and other game\nprograms, robots, et al. This decade the first presumably complete mathematical\ntheory of artificial intelligence based on universal\ninduction-prediction-decision-action has been proposed. This\ninformation-theoretic approach solidifies the foundations of inductive\ninference and artificial intelligence. Getting the foundations right usually\nmarks a significant progress and maturing of a field. The theory provides a\ngold standard and guidance for researchers working on intelligent algorithms.\nThe roots of universal induction have been laid exactly half-a-century ago and\nthe roots of universal intelligence exactly one decade ago. So it is timely to\ntake stock of what has been achieved and what remains to be done. Since there\nare already good recent surveys, I describe the state-of-the-art only in\npassing and refer the reader to the literature. This article concentrates on\nthe open problems in universal induction and its extension to universal\nintelligence.\n",
          "  Actor-Critic based approaches were among the first to address reinforcement\nlearning in a general setting. Recently, these algorithms have gained renewed\ninterest due to their generality, good convergence properties, and possible\nbiological relevance. In this paper, we introduce an online temporal difference\nbased actor-critic algorithm which is proved to converge to a neighborhood of a\nlocal maximum of the average reward. Linear function approximation is used by\nthe critic in order estimate the value function, and the temporal difference\nsignal, which is passed from the critic to the actor. The main distinguishing\nfeature of the present convergence proof is that both the actor and the critic\noperate on a similar time scale, while in most current convergence proofs they\nare required to have very different time scales in order to converge. Moreover,\nthe same temporal difference signal is used to update the parameters of both\nthe actor and the critic. A limitation of the proposed approach, compared to\nresults available for two time scale convergence, is that convergence is\nguaranteed only to a neighborhood of an optimal value, rather to an optimal\nvalue itself. The single time scale and identical temporal difference signal\nused by the actor and the critic, may provide a step towards constructing more\nbiologically realistic models of reinforcement learning in the brain.\n",
          "  A mechanism called Eligibility Propagation is proposed to speed up the Time\nHopping technique used for faster Reinforcement Learning in simulations.\nEligibility Propagation provides for Time Hopping similar abilities to what\neligibility traces provide for conventional Reinforcement Learning. It\npropagates values from one state to all of its temporal predecessors using a\nstate transitions graph. Experiments on a simulated biped crawling robot\nconfirm that Eligibility Propagation accelerates the learning process more than\n3 times.\n",
          "  The exploration-exploitation dilemma has been an intriguing and unsolved\nproblem within the framework of reinforcement learning. \"Optimism in the face\nof uncertainty\" and model building play central roles in advanced exploration\nmethods. Here, we integrate several concepts and obtain a fast and simple\nalgorithm. We show that the proposed algorithm finds a near-optimal policy in\npolynomial time, and give experimental evidence that it is robust and efficient\ncompared to its ascendants.\n",
          "  In large systems, it is important for agents to learn to act effectively, but\nsophisticated multi-agent learning algorithms generally do not scale. An\nalternative approach is to find restricted classes of games where simple,\nefficient algorithms converge. It is shown that stage learning efficiently\nconverges to Nash equilibria in large anonymous games if best-reply dynamics\nconverge. Two features are identified that improve convergence. First, rather\nthan making learning more difficult, more agents are actually beneficial in\nmany settings. Second, providing agents with statistical information about the\nbehavior of others can significantly reduce the number of observations needed.\n",
          "  The problem of multi-agent learning and adaptation has attracted a great deal\nof attention in recent years. It has been suggested that the dynamics of multi\nagent learning can be studied using replicator equations from population\nbiology. Most existing studies so far have been limited to discrete strategy\nspaces with a small number of available actions. In many cases, however, the\nchoices available to agents are better characterized by continuous spectra.\nThis paper suggests a generalization of the replicator framework that allows to\nstudy the adaptive dynamics of Q-learning agents with continuous strategy\nspaces. Instead of probability vectors, agents strategies are now characterized\nby probability measures over continuous variables. As a result, the ordinary\ndifferential equations for the discrete case are replaced by a system of\ncoupled integral--differential replicator equations that describe the mutual\nevolution of individual agent strategies. We derive a set of functional\nequations describing the steady state of the replicator dynamics, examine their\nsolutions for several two-player games, and confirm our analytical results\nusing simulations.\n",
          "  General purpose intelligent learning agents cycle through (complex,non-MDP)\nsequences of observations, actions, and rewards. On the other hand,\nreinforcement learning is well-developed for small finite state Markov Decision\nProcesses (MDPs). So far it is an art performed by human designers to extract\nthe right state representation out of the bare observations, i.e. to reduce the\nagent setup to the MDP framework. Before we can think of mechanizing this\nsearch for suitable MDPs, we need a formal objective criterion. The main\ncontribution of this article is to develop such a criterion. I also integrate\nthe various parts into one learning algorithm. Extensions to more realistic\ndynamic Bayesian networks are developed in a companion article.\n",
          "  This paper proposes a method to construct an adaptive agent that is universal\nwith respect to a given class of experts, where each expert is an agent that\nhas been designed specifically for a particular environment. This adaptive\ncontrol problem is formalized as the problem of minimizing the relative entropy\nof the adaptive agent from the expert that is most suitable for the unknown\nenvironment. If the agent is a passive observer, then the optimal solution is\nthe well-known Bayesian predictor. However, if the agent is active, then its\npast actions need to be treated as causal interventions on the I/O stream\nrather than normal probability conditions. Here it is shown that the solution\nto this new variational problem is given by a stochastic controller called the\nBayesian control rule, which implements adaptive behavior as a mixture of\nexperts. Furthermore, it is shown that under mild assumptions, the Bayesian\ncontrol rule converges to the control law of the most suitable expert.\n",
          "  Experimental verification has been the method of choice for verifying the\nstability of a multi-agent reinforcement learning (MARL) algorithm as the\nnumber of agents grows and theoretical analysis becomes prohibitively complex.\nFor cooperative agents, where the ultimate goal is to optimize some global\nmetric, the stability is usually verified by observing the evolution of the\nglobal performance metric over time. If the global metric improves and\neventually stabilizes, it is considered a reasonable verification of the\nsystem's stability.\n  The main contribution of this note is establishing the need for better\nexperimental frameworks and measures to assess the stability of large-scale\nadaptive cooperative systems. We show an experimental case study where the\nstability of the global performance metric can be rather deceiving, hiding an\nunderlying instability in the system that later leads to a significant drop in\nperformance. We then propose an alternative metric that relies on agents' local\npolicies and show, experimentally, that our proposed metric is more effective\n(than the traditional global performance metric) in exposing the instability of\nMARL algorithms.\n",
          "  Using virtual stock markets with artificial interacting software investors,\naka agent-based models (ABMs), we present a method to reverse engineer\nreal-world financial time series. We model financial markets as made of a large\nnumber of interacting boundedly rational agents. By optimizing the similarity\nbetween the actual data and that generated by the reconstructed virtual stock\nmarket, we obtain parameters and strategies, which reveal some of the inner\nworkings of the target stock market. We validate our approach by out-of-sample\npredictions of directional moves of the Nasdaq Composite Index.\n",
          "  We address the problem of reinforcement learning in which observations may\nexhibit an arbitrary form of stochastic dependence on past observations and\nactions, i.e. environments more general than (PO)MDPs. The task for an agent is\nto attain the best possible asymptotic reward where the true generating\nenvironment is unknown but belongs to a known countable family of environments.\nWe find some sufficient conditions on the class of environments under which an\nagent exists which attains the best asymptotic reward for any environment in\nthe class. We analyze how tight these conditions are and how they relate to\ndifferent probabilistic assumptions known in reinforcement learning and related\nfields, such as Markov Decision Processes and mixing conditions.\n",
          "  Despite the conventional wisdom that proactive security is superior to\nreactive security, we show that reactive security can be competitive with\nproactive security as long as the reactive defender learns from past attacks\ninstead of myopically overreacting to the last attack. Our game-theoretic model\nfollows common practice in the security literature by making worst-case\nassumptions about the attacker: we grant the attacker complete knowledge of the\ndefender's strategy and do not require the attacker to act rationally. In this\nmodel, we bound the competitive ratio between a reactive defense algorithm\n(which is inspired by online learning theory) and the best fixed proactive\ndefense. Additionally, we show that, unlike proactive defenses, this reactive\nstrategy is robust to a lack of information about the attacker's incentives and\nknowledge.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "1_adaptive_reinforcement_markov",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "1_adaptive_reinforcement_markov"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          7.180612087249756,
          7.418622016906738,
          7.513530731201172,
          6.7390456199646,
          7.515465259552002,
          7.5265045166015625,
          7.319569110870361,
          7.708988666534424,
          7.680290222167969,
          7.624436378479004,
          7.605845928192139,
          7.6470561027526855,
          6.810018062591553,
          7.143362998962402,
          7.140048027038574,
          7.489786624908447,
          6.874648571014404,
          7.692661285400391,
          6.686028957366943,
          7.698391914367676,
          7.723378658294678,
          7.65177059173584,
          6.858709335327148,
          6.931496620178223,
          7.540301322937012,
          7.173312664031982,
          7.017714977264404,
          6.3229498863220215,
          7.226530075073242,
          6.9023284912109375,
          7.278779983520508
         ],
         "y": [
          7.641040802001953,
          7.7905192375183105,
          7.619743824005127,
          6.954135894775391,
          7.724709987640381,
          7.764438152313232,
          7.57611608505249,
          7.458431243896484,
          7.493047714233398,
          7.618399143218994,
          7.594536781311035,
          7.56531286239624,
          6.935932636260986,
          7.665376663208008,
          7.659344673156738,
          7.740277290344238,
          6.982062816619873,
          7.527096271514893,
          6.926801681518555,
          7.555907726287842,
          7.497618198394775,
          7.475599765777588,
          7.023777961730957,
          7.322235107421875,
          7.707714080810547,
          7.675088405609131,
          7.2005157470703125,
          6.386262893676758,
          7.631340026855469,
          6.537313938140869,
          7.408356189727783
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  We address the problem of learning in an online, bandit setting where the\nlearner must repeatedly select among $K$ actions, but only receives partial\nfeedback based on its choices. We establish two new facts: First, using a new\nalgorithm called Exp4.P, we show that it is possible to compete with the best\nin a set of $N$ experts with probability $1-\\delta$ while incurring regret at\nmost $O(\\sqrt{KT\\ln(N/\\delta)})$ over $T$ time steps. The new algorithm is\ntested empirically in a large-scale, real-world dataset. Second, we give a new\nalgorithm called VE that competes with a possibly infinite set of policies of\nVC-dimension $d$ while incurring regret at most $O(\\sqrt{T(d\\ln(T) + \\ln\n(1/\\delta))})$ with probability $1-\\delta$. These guarantees improve on those\nof all previous algorithms, whether in a stochastic or adversarial environment,\nand bring us closer to providing supervised learning type guarantees for the\ncontextual bandit setting.\n",
          "  The Lipschitz multi-armed bandit (MAB) problem generalizes the classical\nmulti-armed bandit problem by assuming one is given side information consisting\nof a priori upper bounds on the difference in expected payoff between certain\npairs of strategies. Classical results of (Lai and Robbins 1985) and (Auer et\nal. 2002) imply a logarithmic regret bound for the Lipschitz MAB problem on\nfinite metric spaces. Recent results on continuum-armed bandit problems and\ntheir generalizations imply lower bounds of $\\sqrt{t}$, or stronger, for many\ninfinite metric spaces such as the unit interval. Is this dichotomy universal?\nWe prove that the answer is yes: for every metric space, the optimal regret of\na Lipschitz MAB algorithm is either bounded above by any $f\\in \\omega(\\log t)$,\nor bounded below by any $g\\in o(\\sqrt{t})$. Perhaps surprisingly, this\ndichotomy does not coincide with the distinction between finite and infinite\nmetric spaces; instead it depends on whether the completion of the metric space\nis compact and countable. Our proof connects upper and lower bound techniques\nin online learning with classical topological notions such as perfect sets and\nthe Cantor-Bendixson theorem. Among many other results, we show a similar\ndichotomy for the \"full-feedback\" (a.k.a., \"best-expert\") version.\n",
          "  Traffic forecasting from past observed traffic data with small calculation\ncomplexity is one of important problems for planning of servers and networks.\nFocusing on World Wide Web (WWW) traffic as fundamental investigation, this\npaper would deal with Bayesian forecasting of network traffic on the time\nvarying Poisson model from a viewpoint from statistical decision theory. Under\nthis model, we would show that the estimated forecasting value is obtained by\nsimple arithmetic calculation and expresses real WWW traffic well from both\ntheoretical and empirical points of view.\n",
          "  We discuss multi-task online learning when a decision maker has to deal\nsimultaneously with M tasks. The tasks are related, which is modeled by\nimposing that the M-tuple of actions taken by the decision maker needs to\nsatisfy certain constraints. We give natural examples of such restrictions and\nthen discuss a general class of tractable constraints, for which we introduce\ncomputationally efficient ways of selecting actions, essentially by reducing to\nan on-line shortest path problem. We briefly discuss \"tracking\" and \"bandit\"\nversions of the problem and extend the model in various ways, including\nnon-additive global losses and uncountably infinite sets of tasks.\n",
          "  We consider a generalization of stochastic bandits where the set of arms,\n$\\cX$, is allowed to be a generic measurable space and the mean-payoff function\nis \"locally Lipschitz\" with respect to a dissimilarity function that is known\nto the decision maker. Under this condition we construct an arm selection\npolicy, called HOO (hierarchical optimistic optimization), with improved regret\nbounds compared to previous results for a large class of problems. In\nparticular, our results imply that if $\\cX$ is the unit hypercube in a\nEuclidean space and the mean-payoff function has a finite number of global\nmaxima around which the behavior of the function is locally continuous with a\nknown smoothness degree, then the expected regret of HOO is bounded up to a\nlogarithmic factor by $\\sqrt{n}$, i.e., the rate of growth of the regret is\nindependent of the dimension of the space. We also prove the minimax optimality\nof our algorithm when the dissimilarity is a metric. Our basic strategy has\nquadratic computational complexity as a function of the number of time steps\nand does not rely on the doubling trick. We also introduce a modified strategy,\nwhich relies on the doubling trick but runs in linearithmic time. Both results\nare improvements with respect to previous approaches.\n",
          "  In a multi-armed bandit problem, an online algorithm chooses from a set of\nstrategies in a sequence of trials so as to maximize the total payoff of the\nchosen strategies. While the performance of bandit algorithms with a small\nfinite strategy set is quite well understood, bandit problems with large\nstrategy sets are still a topic of very active investigation, motivated by\npractical applications such as online auctions and web advertisement. The goal\nof such research is to identify broad and natural classes of strategy sets and\npayoff functions which enable the design of efficient solutions. In this work\nwe study a very general setting for the multi-armed bandit problem in which the\nstrategies form a metric space, and the payoff function satisfies a Lipschitz\ncondition with respect to the metric. We refer to this problem as the\n\"Lipschitz MAB problem\". We present a complete solution for the multi-armed\nproblem in this setting. That is, for every metric space (L,X) we define an\nisometry invariant which bounds from below the performance of Lipschitz MAB\nalgorithms for X, and we present an algorithm which comes arbitrarily close to\nmeeting this bound. Furthermore, our technique gives even better results for\nbenign payoff functions.\n",
          "  We study the problem of decision-theoretic online learning (DTOL). Motivated\nby practical applications, we focus on DTOL when the number of actions is very\nlarge. Previous algorithms for learning in this framework have a tunable\nlearning rate parameter, and a barrier to using online-learning in practical\napplications is that it is not understood how to set this parameter optimally,\nparticularly when the number of actions is large.\n  In this paper, we offer a clean solution by proposing a novel and completely\nparameter-free algorithm for DTOL. We introduce a new notion of regret, which\nis more natural for applications with a large number of actions. We show that\nour algorithm achieves good performance with respect to this new notion of\nregret; in addition, it also achieves performance close to that of the best\nbounds achieved by previous algorithms with optimally-tuned parameters,\naccording to previous notions of regret.\n",
          "  We study the regret of optimal strategies for online convex optimization\ngames. Using von Neumann's minimax theorem, we show that the optimal regret in\nthis adversarial setting is closely related to the behavior of the empirical\nminimization algorithm in a stochastic process setting: it is equal to the\nmaximum, over joint distributions of the adversary's action sequence, of the\ndifference between a sum of minimal expected losses and the minimal empirical\nloss. We show that the optimal regret has a natural geometric interpretation,\nsince it can be viewed as the gap in Jensen's inequality for a concave\nfunctional--the minimizer over the player's actions of expected loss--defined\non a set of probability distributions. We use this expression to obtain upper\nand lower bounds on the regret of an optimal strategy for a variety of online\nlearning problems. Our method provides upper bounds without the need to\nconstruct a learning algorithm; the lower bounds provide explicit optimal\nstrategies for the adversary.\n",
          "  Personalized web services strive to adapt their services (advertisements,\nnews articles, etc) to individual users by making use of both content and user\ninformation. Despite a few recent advances, this problem remains challenging\nfor at least two reasons. First, web service is featured with dynamically\nchanging pools of content, rendering traditional collaborative filtering\nmethods inapplicable. Second, the scale of most web services of practical\ninterest calls for solutions that are both fast in learning and computation.\n  In this work, we model personalized recommendation of news articles as a\ncontextual bandit problem, a principled approach in which a learning algorithm\nsequentially selects articles to serve users based on contextual information\nabout the users and articles, while simultaneously adapting its\narticle-selection strategy based on user-click feedback to maximize total user\nclicks.\n  The contributions of this work are three-fold. First, we propose a new,\ngeneral contextual bandit algorithm that is computationally efficient and well\nmotivated from learning theory. Second, we argue that any bandit algorithm can\nbe reliably evaluated offline using previously recorded random traffic.\nFinally, using this offline evaluation method, we successfully applied our new\nalgorithm to a Yahoo! Front Page Today Module dataset containing over 33\nmillion events. Results showed a 12.5% click lift compared to a standard\ncontext-free bandit algorithm, and the advantage becomes even greater when data\ngets more scarce.\n",
          "  Many applications require optimizing an unknown, noisy function that is\nexpensive to evaluate. We formalize this task as a multi-armed bandit problem,\nwhere the payoff function is either sampled from a Gaussian process (GP) or has\nlow RKHS norm. We resolve the important open problem of deriving regret bounds\nfor this setting, which imply novel convergence rates for GP optimization. We\nanalyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its\ncumulative regret in terms of maximal information gain, establishing a novel\nconnection between GP optimization and experimental design. Moreover, by\nbounding the latter in terms of operator spectra, we obtain explicit sublinear\nregret bounds for many commonly used covariance functions. In some important\ncases, our bounds have surprisingly weak dependence on the dimensionality. In\nour experiments on real sensor data, GP-UCB compares favorably with other\nheuristical GP optimization approaches.\n",
          "  We consider the framework of stochastic multi-armed bandit problems and study\nthe possibilities and limitations of forecasters that perform an on-line\nexploration of the arms. These forecasters are assessed in terms of their\nsimple regret, a regret notion that captures the fact that exploration is only\nconstrained by the number of available rounds (not necessarily known in\nadvance), in contrast to the case when the cumulative regret is considered and\nwhen exploitation needs to be performed at the same time. We believe that this\nperformance criterion is suited to situations when the cost of pulling an arm\nis expressed in terms of resources rather than rewards. We discuss the links\nbetween the simple and the cumulative regret. One of the main results in the\ncase of a finite number of arms is a general lower bound on the simple regret\nof a forecaster in terms of its cumulative regret: the smaller the latter, the\nlarger the former. Keeping this result in mind, we then exhibit upper bounds on\nthe simple regret of some forecasters. The paper ends with a study devoted to\ncontinuous-armed bandit problems; we show that the simple regret can be\nminimized with respect to a family of probability distributions if and only if\nthe cumulative regret can be minimized for it. Based on this equivalence, we\nare able to prove that the separable metric spaces are exactly the metric\nspaces on which these regrets can be minimized with respect to the family of\nall probability distributions with continuous mean-payoff functions.\n",
          "  The on-line shortest path problem is considered under various models of\npartial monitoring. Given a weighted directed acyclic graph whose edge weights\ncan change in an arbitrary (adversarial) way, a decision maker has to choose in\neach round of a game a path between two distinguished vertices such that the\nloss of the chosen path (defined as the sum of the weights of its composing\nedges) be as small as possible. In a setting generalizing the multi-armed\nbandit problem, after choosing a path, the decision maker learns only the\nweights of those edges that belong to the chosen path. For this problem, an\nalgorithm is given whose average cumulative loss in n rounds exceeds that of\nthe best path, matched off-line to the entire sequence of the edge weights, by\na quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on\nthe number of edges of the graph. The algorithm can be implemented with linear\ncomplexity in the number of rounds n and in the number of edges. An extension\nto the so-called label efficient setting is also given, in which the decision\nmaker is informed about the weights of the edges corresponding to the chosen\npath at a total of m << n time instances. Another extension is shown where the\ndecision maker competes against a time-varying path, a generalization of the\nproblem of tracking the best expert. A version of the multi-armed bandit\nsetting for shortest path is also discussed where the decision maker learns\nonly the total weight of the chosen path but not the weights of the individual\nedges on the path. Applications to routing in packet switched networks along\nwith simulation results are also presented.\n",
          "  This paper describes a methodology for detecting anomalies from sequentially\nobserved and potentially noisy data. The proposed approach consists of two main\nelements: (1) {\\em filtering}, or assigning a belief or likelihood to each\nsuccessive measurement based upon our ability to predict it from previous noisy\nobservations, and (2) {\\em hedging}, or flagging potential anomalies by\ncomparing the current belief against a time-varying and data-adaptive\nthreshold. The threshold is adjusted based on the available feedback from an\nend user. Our algorithms, which combine universal prediction with recent work\non online convex programming, do not require computing posterior distributions\ngiven all current observations and involve simple primal-dual parameter\nupdates. At the heart of the proposed approach lie exponential-family models\nwhich can be used in a wide variety of contexts and applications, and which\nyield methods that achieve sublinear per-round regret against both static and\nslowly varying product distributions with marginals drawn from the same\nexponential family. Moreover, the regret against static distributions coincides\nwith the minimax value of the corresponding online strongly convex game. We\nalso prove bounds on the number of mistakes made during the hedging step\nrelative to the best offline choice of the threshold with access to all\nestimated beliefs and feedback signals. We validate the theory on synthetic\ndata drawn from a time-varying distribution over binary vectors of high\ndimensionality, as well as on the Enron email dataset.\n",
          "  We consider bandit problems involving a large (possibly infinite) collection\nof arms, in which the expected reward of each arm is a linear function of an\n$r$-dimensional random vector $\\mathbf{Z} \\in \\mathbb{R}^r$, where $r \\geq 2$.\nThe objective is to minimize the cumulative regret and Bayes risk. When the set\nof arms corresponds to the unit sphere, we prove that the regret and Bayes risk\nis of order $\\Theta(r \\sqrt{T})$, by establishing a lower bound for an\narbitrary policy, and showing that a matching upper bound is obtained through a\npolicy that alternates between exploration and exploitation phases. The\nphase-based policy is also shown to be effective if the set of arms satisfies a\nstrong convexity condition. For the case of a general set of arms, we describe\na near-optimal policy whose regret and Bayes risk admit upper bounds of the\nform $O(r \\sqrt{T} \\log^{3/2} T)$.\n",
          "  Algorithm selection is typically based on models of algorithm performance,\nlearned during a separate offline training sequence, which can be prohibitively\nexpensive. In recent work, we adopted an online approach, in which a\nperformance model is iteratively updated and used to guide selection on a\nsequence of problem instances. The resulting exploration-exploitation trade-off\nwas represented as a bandit problem with expert advice, using an existing\nsolver for this game, but this required the setting of an arbitrary bound on\nalgorithm runtimes, thus invalidating the optimal regret of the solver. In this\npaper, we propose a simpler framework for representing algorithm selection as a\nbandit problem, with partial information, and an unknown bound on losses. We\nadapt an existing solver to this game, proving a bound on its expected regret,\nwhich holds also for the resulting algorithm selection technique. We present\npreliminary experiments with a set of SAT solvers on a mixed SAT-UNSAT\nbenchmark.\n",
          "  A key problem in sensor networks is to decide which sensors to query when, in\norder to obtain the most useful information (e.g., for performing accurate\nprediction), subject to constraints (e.g., on power and bandwidth). In many\napplications the utility function is not known a priori, must be learned from\ndata, and can even change over time. Furthermore for large sensor networks\nsolving a centralized optimization problem to select sensors is not feasible,\nand thus we seek a fully distributed solution. In this paper, we present\nDistributed Online Greedy (DOG), an efficient, distributed algorithm for\nrepeatedly selecting sensors online, only receiving feedback about the utility\nof the selected sensors. We prove very strong theoretical no-regret guarantees\nthat apply whenever the (unknown) utility function satisfies a natural\ndiminishing returns property called submodularity. Our algorithm has extremely\nlow communication requirements, and scales well to large sensor deployments. We\nextend DOG to allow observation-dependent sensor selection. We empirically\ndemonstrate the effectiveness of our algorithm on several real-world sensing\ntasks.\n",
          "  Which ads should we display in sponsored search in order to maximize our\nrevenue? How should we dynamically rank information sources to maximize value\nof information? These applications exhibit strong diminishing returns:\nSelection of redundant ads and information sources decreases their marginal\nutility. We show that these and other problems can be formalized as repeatedly\nselecting an assignment of items to positions to maximize a sequence of\nmonotone submodular functions that arrive one by one. We present an efficient\nalgorithm for this general problem and analyze it in the no-regret model. Our\nalgorithm possesses strong theoretical guarantees, such as a performance ratio\nthat converges to the optimal constant of 1-1/e. We empirically evaluate our\nalgorithm on two real-world online optimization problems on the web: ad\nallocation with submodular utilities, and dynamically ranking blogs to detect\ninformation cascades.\n",
          "  We formulate and study a decentralized multi-armed bandit (MAB) problem.\nThere are M distributed players competing for N independent arms. Each arm,\nwhen played, offers i.i.d. reward according to a distribution with an unknown\nparameter. At each time, each player chooses one arm to play without exchanging\nobservations or any information with other players. Players choosing the same\narm collide, and, depending on the collision model, either no one receives\nreward or the colliding players share the reward in an arbitrary way. We show\nthat the minimum system regret of the decentralized MAB grows with time at the\nsame logarithmic order as in the centralized counterpart where players act\ncollectively as a single entity by exchanging observations and making decisions\njointly. A decentralized policy is constructed to achieve this optimal order\nwhile ensuring fairness among players and without assuming any pre-agreement or\ninformation exchange among players. Based on a Time Division Fair Sharing\n(TDFS) of the M best arms, the proposed policy is constructed and its order\noptimality is proven under a general reward model. Furthermore, the basic\nstructure of the TDFS policy can be used with any order-optimal single-player\npolicy to achieve order optimality in the decentralized setting. We also\nestablish a lower bound on the system regret growth rate for a general class of\ndecentralized polices, to which the proposed policy belongs. This problem finds\npotential applications in cognitive radio networks, multi-channel communication\nsystems, multi-agent systems, web search and advertising, and social networks.\n",
          "  We consider a multi-round auction setting motivated by pay-per-click auctions\nfor Internet advertising. In each round the auctioneer selects an advertiser\nand shows her ad, which is then either clicked or not. An advertiser derives\nvalue from clicks; the value of a click is her private information. Initially,\nneither the auctioneer nor the advertisers have any information about the\nlikelihood of clicks on the advertisements. The auctioneer's goal is to design\na (dominant strategies) truthful mechanism that (approximately) maximizes the\nsocial welfare.\n  If the advertisers bid their true private values, our problem is equivalent\nto the \"multi-armed bandit problem\", and thus can be viewed as a strategic\nversion of the latter. In particular, for both problems the quality of an\nalgorithm can be characterized by \"regret\", the difference in social welfare\nbetween the algorithm and the benchmark which always selects the same \"best\"\nadvertisement. We investigate how the design of multi-armed bandit algorithms\nis affected by the restriction that the resulting mechanism must be truthful.\nWe find that truthful mechanisms have certain strong structural properties --\nessentially, they must separate exploration from exploitation -- and they incur\nmuch higher regret than the optimal multi-armed bandit algorithms. Moreover, we\nprovide a truthful mechanism which (essentially) matches our lower bound on\nregret.\n",
          "  In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence\nof choices. In each round it chooses from a time-invariant set of alternatives\nand receives the payoff associated with this alternative. While the case of\nsmall strategy sets is by now well-understood, a lot of recent work has focused\non MAB problems with exponentially or infinitely large strategy sets, where one\nneeds to assume extra structure in order to make the problem tractable. In\nparticular, recent literature considered information on similarity between\narms.\n  We consider similarity information in the setting of \"contextual bandits\", a\nnatural extension of the basic MAB problem where before each round an algorithm\nis given the \"context\" -- a hint about the payoffs in this round. Contextual\nbandits are directly motivated by placing advertisements on webpages, one of\nthe crucial problems in sponsored search. A particularly simple way to\nrepresent similarity information in the contextual bandit setting is via a\n\"similarity distance\" between the context-arm pairs which gives an upper bound\non the difference between the respective expected payoffs.\n  Prior work on contextual bandits with similarity uses \"uniform\" partitions of\nthe similarity space, which is potentially wasteful. We design more efficient\nalgorithms that are based on adaptive partitions adjusted to \"popular\" context\nand \"high-payoff\" arms.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "2_bandits_bandit_optimal",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "2_bandits_bandit_optimal"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          7.370458602905273,
          7.508929252624512,
          7.76528263092041,
          7.377610206604004,
          7.534768104553223,
          7.5777130126953125,
          7.200623512268066,
          7.2253313064575195,
          7.572973728179932,
          7.428586483001709,
          7.543527603149414,
          7.345627307891846,
          7.262385845184326,
          7.579010963439941,
          7.453587055206299,
          7.565153121948242,
          7.604026794433594,
          7.671823978424072,
          7.6251935958862305,
          7.563943862915039,
          7.488828182220459
         ],
         "y": [
          6.538967609405518,
          6.508220195770264,
          5.692764759063721,
          6.732478618621826,
          6.46682596206665,
          6.491611480712891,
          6.589563846588135,
          6.508138179779053,
          6.542198657989502,
          6.405327320098877,
          6.478079795837402,
          6.636690616607666,
          6.360760688781738,
          6.448853015899658,
          6.556759357452393,
          6.107334136962891,
          6.298215866088867,
          6.414610385894775,
          6.396572589874268,
          6.467410564422607,
          6.432069301605225
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  This paper addresses the general problem of domain adaptation which arises in\na variety of applications where the distribution of the labeled sample\navailable somewhat differs from that of the test data. Building on previous\nwork by Ben-David et al. (2007), we introduce a novel distance between\ndistributions, discrepancy distance, that is tailored to adaptation problems\nwith arbitrary loss functions. We give Rademacher complexity bounds for\nestimating the discrepancy distance from finite samples for different loss\nfunctions. Using this distance, we derive novel generalization bounds for\ndomain adaptation for a wide family of loss functions. We also present a series\nof novel adaptation bounds for large classes of regularization-based\nalgorithms, including support vector machines and kernel ridge regression based\non the empirical discrepancy. This motivates our analysis of the problem of\nminimizing the empirical discrepancy for various loss functions for which we\nalso give novel algorithms. We report the results of preliminary experiments\nthat demonstrate the benefits of our discrepancy minimization algorithms for\ndomain adaptation.\n",
          "  We introduce a framework for filtering features that employs the\nHilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence\nbetween the features and the labels. The key idea is that good features should\nmaximise such dependence. Feature selection for various supervised learning\nproblems (including classification and regression) is unified under this\nframework, and the solutions can be approximated using a backward-elimination\nalgorithm. We demonstrate the usefulness of our method on both artificial and\nreal world datasets.\n",
          "  In multi-task learning several related tasks are considered simultaneously,\nwith the hope that by an appropriate sharing of information across tasks, each\ntask may benefit from the others. In the context of learning linear functions\nfor supervised classification or regression, this can be achieved by including\na priori information about the weight vectors associated with the tasks, and\nhow they are expected to be related to each other. In this paper, we assume\nthat tasks are clustered into groups, which are unknown beforehand, and that\ntasks within a group have similar weight vectors. We design a new spectral norm\nthat encodes this a priori assumption, without the prior knowledge of the\npartition of tasks into groups, resulting in a new convex optimization\nformulation for multi-task learning. We show in simulations on synthetic\nexamples and on the IEDB MHC-I binding dataset, that our approach outperforms\nwell-known convex methods for multi-task learning, as well as related non\nconvex methods dedicated to the same problem.\n",
          "  We consider a general class of regularization methods which learn a vector of\nparameters on the basis of linear measurements. It is well known that if the\nregularizer is a nondecreasing function of the inner product then the learned\nvector is a linear combination of the input data. This result, known as the\n{\\em representer theorem}, is at the basis of kernel-based methods in machine\nlearning. In this paper, we prove the necessity of the above condition, thereby\ncompleting the characterization of kernel methods based on regularization. We\nfurther extend our analysis to regularization methods which learn a matrix, a\nproblem which is motivated by the application to multi-task learning. In this\ncontext, we study a more general representer theorem, which holds for a larger\nclass of regularizers. We provide a necessary and sufficient condition for\nthese class of matrix regularizers and highlight them with some concrete\nexamples of practical importance. Our analysis uses basic principles from\nmatrix theory, especially the useful notion of matrix nondecreasing function.\n",
          "  A client-server architecture to simultaneously solve multiple learning tasks\nfrom distributed datasets is described. In such architecture, each client is\nassociated with an individual learning task and the associated dataset of\nexamples. The goal of the architecture is to perform information fusion from\nmultiple datasets while preserving privacy of individual data. The role of the\nserver is to collect data in real-time from the clients and codify the\ninformation in a common database. The information coded in this database can be\nused by all the clients to solve their individual learning task, so that each\nclient can exploit the informative content of all the datasets without actually\nhaving access to private data of others. The proposed algorithmic framework,\nbased on regularization theory and kernel methods, uses a suitable class of\nmixed effect kernels. The new method is illustrated through a simulated music\nrecommendation system.\n",
          "  We propose a general method called truncated gradient to induce sparsity in\nthe weights of online learning algorithms with convex loss functions. This\nmethod has several essential properties: The degree of sparsity is continuous\n-- a parameter controls the rate of sparsification from no sparsification to\ntotal sparsification. The approach is theoretically motivated, and an instance\nof it can be regarded as an online counterpart of the popular\n$L_1$-regularization method in the batch setting. We prove that small rates of\nsparsification result in only small additional regret with respect to typical\nonline learning guarantees. The approach works well empirically. We apply the\napproach to several datasets and find that for datasets with large numbers of\nfeatures, substantial sparsity is discoverable.\n",
          "  Many regression problems involve not one but several response variables\n(y's). Often the responses are suspected to share a common underlying\nstructure, in which case it may be advantageous to share information across\nthem; this is known as multitask learning. As a special case, we can use\nmultiple responses to better identify shared predictive features -- a project\nwe might call multitask feature selection.\n  This thesis is organized as follows. Section 1 introduces feature selection\nfor regression, focusing on ell_0 regularization methods and their\ninterpretation within a Minimum Description Length (MDL) framework. Section 2\nproposes a novel extension of MDL feature selection to the multitask setting.\nThe approach, called the \"Multiple Inclusion Criterion\" (MIC), is designed to\nborrow information across regression tasks by more easily selecting features\nthat are associated with multiple responses. We show in experiments on\nsynthetic and real biological data sets that MIC can reduce prediction error in\nsettings where features are at least partially shared across responses. Section\n3 surveys hypothesis testing by regression with a single response, focusing on\nthe parallel between the standard Bonferroni correction and an MDL approach.\nMirroring the ideas in Section 2, Section 4 proposes a novel MIC approach to\nhypothesis testing with multiple responses and shows that on synthetic data\nwith significant sharing of features across responses, MIC sometimes\noutperforms standard FDR-controlling methods in terms of finding true positives\nfor a given level of false positives. Section 5 concludes.\n",
          "  This paper focuses on the problem of kernelizing an existing supervised\nMahalanobis distance learner. The following features are included in the paper.\nFirstly, three popular learners, namely, \"neighborhood component analysis\",\n\"large margin nearest neighbors\" and \"discriminant neighborhood embedding\",\nwhich do not have kernel versions are kernelized in order to improve their\nclassification performances. Secondly, an alternative kernelization framework\ncalled \"KPCA trick\" is presented. Implementing a learner in the new framework\ngains several advantages over the standard framework, e.g. no mathematical\nformulas and no reprogramming are required for a kernel implementation, the\nframework avoids troublesome problems such as singularity, etc. Thirdly, while\nthe truths of representer theorems are just assumptions in previous papers\nrelated to ours, here, representer theorems are formally proven. The proofs\nvalidate both the kernel trick and the KPCA trick in the context of Mahalanobis\ndistance learning. Fourthly, unlike previous works which always apply brute\nforce methods to select a kernel, we investigate two approaches which can be\nefficiently adopted to construct an appropriate kernel for a given dataset.\nFinally, numerical results on various real-world datasets are presented.\n",
          "  In this paper, the framework of kernel machines with two layers is\nintroduced, generalizing classical kernel methods. The new learning methodology\nprovide a formal connection between computational architectures with multiple\nlayers and the theme of kernel learning in standard regularization methods.\nFirst, a representer theorem for two-layer networks is presented, showing that\nfinite linear combinations of kernels on each layer are optimal architectures\nwhenever the corresponding functions solve suitable variational problems in\nreproducing kernel Hilbert spaces (RKHS). The input-output map expressed by\nthese architectures turns out to be equivalent to a suitable single-layer\nkernel machines in which the kernel function is also learned from the data.\nRecently, the so-called multiple kernel learning methods have attracted\nconsiderable attention in the machine learning literature. In this paper,\nmultiple kernel learning methods are shown to be specific cases of kernel\nmachines with two layers in which the second layer is linear. Finally, a simple\nand effective multiple kernel learning method called RLS2 (regularized least\nsquares with two layers) is introduced, and his performances on several\nlearning problems are extensively analyzed. An open source MATLAB toolbox to\ntrain and validate RLS2 models with a Graphic User Interface is available.\n",
          "  Maximum Variance Unfolding (MVU) and its variants have been very successful\nin embedding data-manifolds in lower dimensional spaces, often revealing the\ntrue intrinsic dimension. In this paper we show how to also incorporate\nsupervised class information into an MVU-like method without breaking its\nconvexity. We call this method the Isometric Separation Map and we show that\nthe resulting kernel matrix can be used as a binary/multiclass Support Vector\nMachine-like method in a semi-supervised (transductive) framework. We also show\nthat the method always finds a kernel matrix that linearly separates the\ntraining data exactly without projecting them in infinite dimensional spaces.\nIn traditional SVMs we choose a kernel and hope that the data become linearly\nseparable in the kernel space. In this paper we show how the hyperplane can be\nchosen ad-hoc and the kernel is trained so that data are always linearly\nseparable. Comparisons with Large Margin SVMs show comparable performance.\n",
          "  Support vector machines and kernel methods have recently gained considerable\nattention in chemoinformatics. They offer generally good performance for\nproblems of supervised classification or regression, and provide a flexible and\ncomputationally efficient framework to include relevant information and prior\nknowledge about the data and problems to be handled. In particular, with kernel\nmethods molecules do not need to be represented and stored explicitly as\nvectors or fingerprints, but only to be compared to each other through a\ncomparison function technically called a kernel. While classical kernels can be\nused to compare vector or fingerprint representations of molecules, completely\nnew kernels were developed in the recent years to directly compare the 2D or 3D\nstructures of molecules, without the need for an explicit vectorization step\nthrough the extraction of molecular descriptors. While still in their infancy,\nthese approaches have already demonstrated their relevance on several toxicity\nprediction and structure-activity relationship problems.\n",
          "  There is growing body of learning problems for which it is natural to\norganize the parameters into matrix, so as to appropriately regularize the\nparameters under some matrix norm (in order to impose some more sophisticated\nprior knowledge). This work describes and analyzes a systematic method for\nconstructing such matrix-based, regularization methods. In particular, we focus\non how the underlying statistical properties of a given problem can help us\ndecide which regularization function is appropriate.\n  Our methodology is based on the known duality fact: that a function is\nstrongly convex with respect to some norm if and only if its conjugate function\nis strongly smooth with respect to the dual norm. This result has already been\nfound to be a key component in deriving and analyzing several learning\nalgorithms. We demonstrate the potential of this framework by deriving novel\ngeneralization and regret bounds for multi-task learning, multi-class learning,\nand kernel learning.\n",
          "  Metric and kernel learning are important in several machine learning\napplications. However, most existing metric learning algorithms are limited to\nlearning metrics over low-dimensional data, while existing kernel learning\nalgorithms are often limited to the transductive setting and do not generalize\nto new data points. In this paper, we study metric learning as a problem of\nlearning a linear transformation of the input data. We show that for\nhigh-dimensional data, a particular framework for learning a linear\ntransformation of the data based on the LogDet divergence can be efficiently\nkernelized to learn a metric (or equivalently, a kernel function) over an\narbitrarily high dimensional space. We further demonstrate that a wide class of\nconvex loss functions for learning linear transformations can similarly be\nkernelized, thereby considerably expanding the potential applications of metric\nlearning. We demonstrate our learning approach by applying it to large-scale\nreal world problems in computer vision and text mining.\n",
          "  Learning linear combinations of multiple kernels is an appealing strategy\nwhen the right choice of features is unknown. Previous approaches to multiple\nkernel learning (MKL) promote sparse kernel combinations to support\ninterpretability and scalability. Unfortunately, this 1-norm MKL is rarely\nobserved to outperform trivial baselines in practical applications. To allow\nfor robust kernel mixtures, we generalize MKL to arbitrary norms. We devise new\ninsights on the connection between several existing MKL formulations and\ndevelop two efficient interleaved optimization strategies for arbitrary norms,\nlike p-norms with p>1. Empirically, we demonstrate that the interleaved\noptimization strategies are much faster compared to the commonly used wrapper\napproaches. A theoretical analysis and an experiment on controlled artificial\ndata experiment sheds light on the appropriateness of sparse, non-sparse and\n$\\ell_\\infty$-norm MKL in various scenarios. Empirical applications of p-norm\nMKL to three real-world problems from computational biology show that\nnon-sparse MKL achieves accuracies that go beyond the state-of-the-art.\n",
          "  For supervised and unsupervised learning, positive definite kernels allow to\nuse large and potentially infinite dimensional feature spaces with a\ncomputational cost that only depends on the number of observations. This is\nusually done through the penalization of predictor functions by Euclidean or\nHilbertian norms. In this paper, we explore penalizing by sparsity-inducing\nnorms such as the l1-norm or the block l1-norm. We assume that the kernel\ndecomposes into a large sum of individual basis kernels which can be embedded\nin a directed acyclic graph; we show that it is then possible to perform kernel\nselection through a hierarchical multiple kernel learning framework, in\npolynomial time in the number of selected kernels. This framework is naturally\napplied to non linear variable selection; our extensive simulations on\nsynthetic datasets and datasets from the UCI repository show that efficiently\nexploring the large feature space through sparsity-inducing norms leads to\nstate-of-the-art predictive performance.\n",
          "  KNN is one of the most popular classification methods, but it often fails to\nwork well with inappropriate choice of distance metric or due to the presence\nof numerous class-irrelevant features. Linear feature transformation methods\nhave been widely applied to extract class-relevant information to improve kNN\nclassification, which is very limited in many applications. Kernels have been\nused to learn powerful non-linear feature transformations, but these methods\nfail to scale to large datasets. In this paper, we present a scalable\nnon-linear feature mapping method based on a deep neural network pretrained\nwith restricted boltzmann machines for improving kNN classification in a\nlarge-margin framework, which we call DNet-kNN. DNet-kNN can be used for both\nclassification and for supervised dimensionality reduction. The experimental\nresults on two benchmark handwritten digit datasets show that DNet-kNN has much\nbetter performance than large-margin kNN using a linear mapping and kNN based\non a deep autoencoder pretrained with retricted boltzmann machines.\n",
          "  Ensemble methods, such as stacking, are designed to boost predictive accuracy\nby blending the predictions of multiple machine learning models. Recent work\nhas shown that the use of meta-features, additional inputs describing each\nexample in a dataset, can boost the performance of ensemble methods, but the\ngreatest reported gains have come from nonlinear procedures requiring\nsignificant tuning and training time. Here, we present a linear technique,\nFeature-Weighted Linear Stacking (FWLS), that incorporates meta-features for\nimproved accuracy while retaining the well-known virtues of linear regression\nregarding speed, stability, and interpretability. FWLS combines model\npredictions linearly using coefficients that are themselves linear functions of\nmeta-features. This technique was a key facet of the solution of the second\nplace team in the recently concluded Netflix Prize competition. Significant\nincreases in accuracy over standard linear stacking are demonstrated on the\nNetflix Prize collaborative filtering dataset.\n",
          "  Point clouds are sets of points in two or three dimensions. Most kernel\nmethods for learning on sets of points have not yet dealt with the specific\ngeometrical invariances and practical constraints associated with point clouds\nin computer vision and graphics. In this paper, we present extensions of graph\nkernels for point clouds, which allow to use kernel methods for such ob jects\nas shapes, line drawings, or any three-dimensional point clouds. In order to\ndesign rich and numerically efficient kernels with as few free parameters as\npossible, we use kernels between covariance matrices and their factorizations\non graphical models. We derive polynomial time dynamic programming recursions\nand present applications to recognition of handwritten digits and Chinese\ncharacters from few training examples.\n",
          "  We learn multiple hypotheses for related tasks under a latent hierarchical\nrelationship between tasks. We exploit the intuition that for domain\nadaptation, we wish to share classifier structure, but for multitask learning,\nwe wish to share covariance structure. Our hierarchical model is seen to\nsubsume several previously proposed multitask learning models and performs well\non three distinct real-world data sets.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "3_kernelized_kernels_supervised",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "3_kernelized_kernels_supervised"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          3.4676156044006348,
          0.9806116223335266,
          1.131701946258545,
          1.6004939079284668,
          1.788646936416626,
          1.1629008054733276,
          0.8789559602737427,
          1.857492208480835,
          1.5968496799468994,
          1.8434841632843018,
          1.7597137689590454,
          1.5120655298233032,
          2.0819599628448486,
          1.4143661260604858,
          1.48337984085083,
          1.849647045135498,
          0.8472163677215576,
          2.0827605724334717,
          1.015843391418457,
          1.5976686477661133
         ],
         "y": [
          6.719610691070557,
          7.78406286239624,
          7.619654655456543,
          7.8043341636657715,
          7.998818874359131,
          7.861606121063232,
          7.479567527770996,
          7.642892837524414,
          7.679281711578369,
          8.078160285949707,
          7.41013240814209,
          7.845830917358398,
          7.825616359710693,
          7.826513767242432,
          7.908908367156982,
          7.8526458740234375,
          7.26772928237915,
          7.781176567077637,
          7.434916019439697,
          7.674813747406006
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  One of the most utilized data mining tasks is the search for association\nrules. Association rules represent significant relationships between items in\ntransactions. We extend the concept of association rule to represent a much\nbroader class of associations, which we refer to as \\emph{entity-relationship\nrules.} Semantically, entity-relationship rules express associations between\nproperties of related objects. Syntactically, these rules are based on a broad\nsubclass of safe domain relational calculus queries. We propose a new\ndefinition of support and confidence for entity-relationship rules and for the\nfrequency of entity-relationship queries. We prove that the definition of\nfrequency satisfies standard probability axioms and the Apriori property.\n",
          "  In this paper, we propose a technique to extract constrained formal concepts.\n",
          "  In this paper, the mining of hybrid association rules with rough set approach\nis investigated as the algorithm RSHAR.The RSHAR algorithm is constituted of\ntwo steps mainly. At first, to join the participant tables into a general table\nto generate the rules which is expressing the relationship between two or more\ndomains that belong to several different tables in a database. Then we apply\nthe mapping code on selected dimension, which can be added directly into the\ninformation system as one certain attribute. To find the association rules,\nfrequent itemsets are generated in second step where candidate itemsets are\ngenerated through equivalence classes and also transforming the mapping code in\nto real dimensions. The searching method for candidate itemset is similar to\napriori algorithm. The analysis of the performance of algorithm has been\ncarried out.\n",
          "  Engine assembly is a complex and heavily automated distributed-control\nprocess, with large amounts of faults data logged everyday. We describe an\napplication of temporal data mining for analyzing fault logs in an engine\nassembly plant. Frequent episode discovery framework is a model-free method\nthat can be used to deduce (temporal) correlations among events from the logs\nin an efficient manner. In addition to being theoretically elegant and\ncomputationally efficient, frequent episodes are also easy to interpret in the\nform actionable recommendations. Incorporation of domain-specific information\nis critical to successful application of the method for analyzing fault logs in\nthe manufacturing domain. We show how domain-specific knowledge can be\nincorporated using heuristic rules that act as pre-filters and post-filters to\nfrequent episode discovery. The system described here is currently being used\nin one of the engine assembly plants of General Motors and is planned for\nadaptation in other plants. To the best of our knowledge, this paper presents\nthe first real, large-scale application of temporal data mining in the\nmanufacturing domain. We believe that the ideas presented in this paper can\nhelp practitioners engineer tools for analysis in other similar or related\napplication domains as well.\n",
          "  Several technologies are emerging that provide new ways to capture, store,\npresent and use knowledge. This book is the first to provide a comprehensive\nintroduction to five of the most important of these technologies: Knowledge\nEngineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and\nSemantic Webs. For each of these, answers are given to a number of key\nquestions (What is it? How does it operate? How is a system developed? What can\nit be used for? What tools are available? What are the main issues?). The book\nis aimed at students, researchers and practitioners interested in Knowledge\nManagement, Artificial Intelligence, Design Engineering and Web Technologies.\n  During the 1990s, Nick worked at the University of Nottingham on the\napplication of AI techniques to knowledge management and on various knowledge\nacquisition projects to develop expert systems for military applications. In\n1999, he joined Epistemics where he worked on numerous knowledge projects and\nhelped establish knowledge management programmes at large organisations in the\nengineering, technology and legal sectors. He is author of the book \"Knowledge\nAcquisition in Practice\", which describes a step-by-step procedure for\nacquiring and implementing expertise. He maintains strong links with leading\nresearch organisations working on knowledge technologies, such as\nknowledge-based engineering, ontologies and semantic technologies.\n",
          "  This paper addresses the problem of finding the nearest neighbor (or one of\nthe R-nearest neighbors) of a query object q in a database of n objects. In\ncontrast with most existing approaches, we can only access the ``hidden'' space\nin which the objects live through a similarity oracle. The oracle, given two\nreference objects and a query object, returns the reference object closest to\nthe query object. The oracle attempts to model the behavior of human users,\ncapable of making statements about similarity, but not of assigning meaningful\nnumerical values to distances between objects.\n",
          "  Building rules on top of ontologies is the ultimate goal of the logical layer\nof the Semantic Web. To this aim an ad-hoc mark-up language for this layer is\ncurrently under discussion. It is intended to follow the tradition of hybrid\nknowledge representation and reasoning systems such as $\\mathcal{AL}$-log that\nintegrates the description logic $\\mathcal{ALC}$ and the function-free Horn\nclausal language \\textsc{Datalog}. In this paper we consider the problem of\nautomating the acquisition of these rules for the Semantic Web. We propose a\ngeneral framework for rule induction that adopts the methodological apparatus\nof Inductive Logic Programming and relies on the expressive and deductive power\nof $\\mathcal{AL}$-log. The framework is valid whatever the scope of induction\n(description vs. prediction) is. Yet, for illustrative purposes, we also\ndiscuss an instantiation of the framework which aims at description and turns\nout to be useful in Ontology Refinement.\n  Keywords: Inductive Logic Programming, Hybrid Knowledge Representation and\nReasoning Systems, Ontologies, Semantic Web.\n  Note: To appear in Theory and Practice of Logic Programming (TPLP)\n",
          "  In Data Mining, the usefulness of association rules is strongly limited by\nthe huge amount of delivered rules. In this paper we propose a new approach to\nprune and filter discovered rules. Using Domain Ontologies, we strengthen the\nintegration of user knowledge in the post-processing task. Furthermore, an\ninteractive and iterative framework is designed to assist the user along the\nanalyzing task. On the one hand, we represent user domain knowledge using a\nDomain Ontology over database. On the other hand, a novel technique is\nsuggested to prune and to filter discovered rules. The proposed framework was\napplied successfully over the client database provided by Nantes Habitat.\n",
          "  Association rule mining plays vital part in knowledge mining. The difficult\ntask is discovering knowledge or useful rules from the large number of rules\ngenerated for reduced support. For pruning or grouping rules, several\ntechniques are used such as rule structure cover methods, informative cover\nmethods, rule clustering, etc. Another way of selecting association rules is\nbased on interestingness measures such as support, confidence, correlation, and\nso on. In this paper, we study how rule clusters of the pattern Xi - Y are\ndistributed over different interestingness measures.\n",
          "  This version is ***superseded*** by a full version that can be found at\nhttp://www.itu.dk/people/pagh/papers/mining-jour.pdf, which contains stronger\ntheoretical results and fixes a mistake in the reporting of experiments.\n  Abstract: Sampling-based methods have previously been proposed for the\nproblem of finding interesting associations in data, even for low-support\nitems. While these methods do not guarantee precise results, they can be vastly\nmore efficient than approaches that rely on exact counting. However, for many\nsimilarity measures no such methods have been known. In this paper we show how\na wide variety of measures can be supported by a simple biased sampling method.\nThe method also extends to find high-confidence association rules. We\ndemonstrate theoretically that our method is superior to exact methods when the\nthreshold for \"interesting similarity/confidence\" is above the average pairwise\nsimilarity/confidence, and the average support is not too low. Our method is\nparticularly good when transactions contain many items. We confirm in\nexperiments on standard association mining benchmarks that this gives a\nsignificant speedup on real data sets (sometimes much larger than the\ntheoretical guarantees). Reductions in computation time of over an order of\nmagnitude, and significant savings in space, are observed.\n",
          "  This paper applies machine learning techniques to student modeling. It\npresents a method for discovering high-level student behaviors from a very\nlarge set of low-level traces corresponding to problem-solving actions in a\nlearning environment. Basic actions are encoded into sets of domain-dependent\nattribute-value patterns called cases. Then a domain-independent hierarchical\nclustering identifies what we call general attitudes, yielding automatic\ndiagnosis expressed in natural language, addressed in principle to teachers.\nThe method can be applied to individual students or to entire groups, like a\nclass. We exhibit examples of this system applied to thousands of students'\nactions in the domain of algebraic transformations.\n",
          "  Frequent episode discovery is a popular framework for pattern discovery in\nevent streams. An episode is a partially ordered set of nodes with each node\nassociated with an event type. Efficient (and separate) algorithms exist for\nepisode discovery when the associated partial order is total (serial episode)\nand trivial (parallel episode). In this paper, we propose efficient algorithms\nfor discovering frequent episodes with general partial orders. These algorithms\ncan be easily specialized to discover serial or parallel episodes. Also, the\nalgorithms are flexible enough to be specialized for mining in the space of\ncertain interesting subclasses of partial orders. We point out that there is an\ninherent combinatorial explosion in frequent partial order mining and most\nimportantly, frequency alone is not a sufficient measure of interestingness. We\npropose a new interestingness measure for general partial order episodes and a\ndiscovery method based on this measure, for filtering out uninteresting partial\norders. Simulations demonstrate the effectiveness of our algorithms.\n",
          "  In real life, media information has time attributes either implicitly or\nexplicitly known as temporal data. This paper investigates the usefulness of\napplying Bayesian classification to an interval encoded temporal database with\nprioritized items. The proposed method performs temporal mining by encoding the\ndatabase with weighted items which prioritizes the items according to their\nimportance from the user perspective. Naive Bayesian classification helps in\nmaking the resulting temporal rules more effective. The proposed priority based\ntemporal mining (PBTM) method added with classification aids in solving\nproblems in a well informed and systematic manner. The experimental results are\nobtained from the complaints database of the telecommunications system, which\nshows the feasibility of this method of classification based temporal mining.\n",
          "  Associative Classifier is a novel technique which is the integration of\nAssociation Rule Mining and Classification. The difficult task in building\nAssociative Classifier model is the selection of relevant rules from a large\nnumber of class association rules (CARs). A very popular method of ordering\nrules for selection is based on confidence, support and antecedent size (CSA).\nOther methods are based on hybrid orderings in which CSA method is combined\nwith other measures. In the present work, we study the effect of using\ndifferent interestingness measures of Association rules in CAR rule ordering\nand selection for associative classifier.\n",
          "  In this paper, we present the step by step knowledge acquisition process by\nchoosing a structured method through using a questionnaire as a knowledge\nacquisition tool. Here we want to depict the problem domain as, how to evaluate\nteachers performance in higher education through the use of expert system\ntechnology. The problem is how to acquire the specific knowledge for a selected\nproblem efficiently and effectively from human experts and encode it in the\nsuitable computer format. Acquiring knowledge from human experts in the process\nof expert systems development is one of the most common problems cited till\nyet. This questionnaire was sent to 87 domain experts within all public and\nprivate universities in Pakistani. Among them 25 domain experts sent their\nvaluable opinions. Most of the domain experts were highly qualified, well\nexperienced and highly responsible persons. The whole questionnaire was divided\ninto 15 main groups of factors, which were further divided into 99 individual\nquestions. These facts were analyzed further to give a final shape to the\nquestionnaire. This knowledge acquisition technique may be used as a learning\ntool for further research work.\n",
          "  The Web has enabled the availability of a huge amount of useful information,\nbut has also eased the ability to spread false information and rumors across\nmultiple sources, making it hard to distinguish between what is true and what\nis not. Recent examples include the premature Steve Jobs obituary, the second\nbankruptcy of United airlines, the creation of Black Holes by the operation of\nthe Large Hadron Collider, etc. Since it is important to permit the expression\nof dissenting and conflicting opinions, it would be a fallacy to try to ensure\nthat the Web provides only consistent information. However, to help in\nseparating the wheat from the chaff, it is essential to be able to determine\ndependence between sources. Given the huge number of data sources and the vast\nvolume of conflicting data available on the Web, doing so in a scalable manner\nis extremely challenging and has not been addressed by existing work yet.\n  In this paper, we present a set of research problems and propose some\npreliminary solutions on the issues involved in discovering dependence between\nsources. We also discuss how this knowledge can benefit a variety of\ntechnologies, such as data integration and Web 2.0, that help users manage and\naccess the totality of the available information from various sources.\n",
          "  Many databases store data in relational format, with different types of\nentities and information about links between the entities. The field of\nstatistical-relational learning (SRL) has developed a number of new statistical\nmodels for such data. In this paper we focus on learning class-level or\nfirst-order dependencies, which model the general database statistics over\nattributes of linked objects and links (e.g., the percentage of A grades given\nin computer science classes). Class-level statistical relationships are\nimportant in themselves, and they support applications like policy making,\nstrategic planning, and query optimization. Most current SRL methods find\nclass-level dependencies, but their main task is to support instance-level\npredictions about the attributes or links of specific entities. We focus only\non class-level prediction, and describe algorithms for learning class-level\nmodels that are orders of magnitude faster for this task. Our algorithms learn\nBayes nets with relational structure, leveraging the efficiency of single-table\nnonrelational Bayes net learners. An evaluation of our methods on three data\nsets shows that they are computationally feasible for realistic table sizes,\nand that the learned structures represent the statistical information in the\ndatabases well. After learning compiles the database statistics into a Bayes\nnet, querying these statistics via Bayes net inference is faster than with SQL\nqueries, and does not depend on the size of the database.\n",
          "  This paper formalises the concept of learning symbolic rules from multisource\ndata in a cardiac monitoring context. Our sources, electrocardiograms and\narterial blood pressure measures, describe cardiac behaviours from different\nviewpoints. To learn interpretable rules, we use an Inductive Logic Programming\n(ILP) method. We develop an original strategy to cope with the dimensionality\nissues caused by using this ILP technique on a rich multisource language. The\nresults show that our method greatly improves the feasibility and the\nefficiency of the process while staying accurate. They also confirm the\nbenefits of using multiple sources to improve the diagnosis of cardiac\narrhythmias.\n",
          "  Recently, different works proposed a new way to mine patterns in databases\nwith pathological size. For example, experiments in genome biology usually\nprovide databases with thousands of attributes (genes) but only tens of objects\n(experiments). In this case, mining the \"transposed\" database runs through a\nsmaller search space, and the Galois connection allows to infer the closed\npatterns of the original database. We focus here on constrained pattern mining\nfor those unusual databases and give a theoretical framework for database and\nconstraint transposition. We discuss the properties of constraint transposition\nand look into classical constraints. We then address the problem of generating\nthe closed patterns of the original database satisfying the constraint,\nstarting from those mined in the \"transposed\" database. Finally, we show how to\ngenerate all the patterns satisfying the constraint from the closed ones.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "4_discovering_association_patterns",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "4_discovering_association_patterns"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          3.424544334411621,
          3.1524651050567627,
          3.425745725631714,
          3.692591667175293,
          3.016852855682373,
          3.395338296890259,
          3.1976473331451416,
          3.354379892349243,
          3.453434705734253,
          3.5302364826202393,
          2.942145824432373,
          3.8109426498413086,
          3.849222183227539,
          3.458345890045166,
          2.9798619747161865,
          3.5123703479766846,
          2.7049400806427,
          3.1941258907318115,
          3.50335955619812,
          3.347292184829712
         ],
         "y": [
          3.766084671020508,
          4.001605033874512,
          3.725058078765869,
          4.074041843414307,
          4.162912368774414,
          3.9736692905426025,
          3.873765230178833,
          3.8077025413513184,
          3.7619807720184326,
          3.900495767593384,
          4.247844696044922,
          4.009164810180664,
          4.082517623901367,
          3.7368953227996826,
          4.233657360076904,
          4.136453151702881,
          4.2769999504089355,
          3.902130365371704,
          3.807709217071533,
          3.9726674556732178
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  Applications such as face recognition that deal with high-dimensional data\nneed a mapping technique that introduces representation of low-dimensional\nfeatures with enhanced discriminatory power and a proper classifier, able to\nclassify those complex features. Most of traditional Linear Discriminant\nAnalysis suffer from the disadvantage that their optimality criteria are not\ndirectly related to the classification ability of the obtained feature\nrepresentation. Moreover, their classification accuracy is affected by the\n\"small sample size\" problem which is often encountered in FR tasks. In this\nshort paper, we combine nonlinear kernel based mapping of data called KDDA with\nSupport Vector machine classifier to deal with both of the shortcomings in an\nefficient and cost effective manner. The proposed here method is compared, in\nterms of classification accuracy, to other commonly used FR methods on UMIST\nface database. Results indicate that the performance of the proposed method is\noverall superior to those of traditional FR approaches, such as the Eigenfaces,\nFisherfaces, and D-LDA methods and traditional linear classifiers.\n",
          "  Identity verification of authentic persons by their multiview faces is a real\nvalued problem in machine vision. Multiview faces are having difficulties due\nto non-linear representation in the feature space. This paper illustrates the\nusability of the generalization of LDA in the form of canonical covariate for\nface recognition to multiview faces. In the proposed work, the Gabor filter\nbank is used to extract facial features that characterized by spatial\nfrequency, spatial locality and orientation. Gabor face representation captures\nsubstantial amount of variations of the face instances that often occurs due to\nillumination, pose and facial expression changes. Convolution of Gabor filter\nbank to face images of rotated profile views produce Gabor faces with high\ndimensional features vectors. Canonical covariate is then used to Gabor faces\nto reduce the high dimensional feature spaces into low dimensional subspaces.\nFinally, support vector machines are trained with canonical sub-spaces that\ncontain reduced set of features and perform recognition task. The proposed\nsystem is evaluated with UMIST face database. The experiment results\ndemonstrate the efficiency and robustness of the proposed system with high\nrecognition rates.\n",
          "  A fundamental task in detecting foreground objects in both static and dynamic\nscenes is to take the best choice of color system representation and the\nefficient technique for background modeling. We propose in this paper a\nnon-parametric algorithm dedicated to segment and to detect objects in color\nimages issued from a football sports meeting. Indeed segmentation by pixel\nconcern many applications and revealed how the method is robust to detect\nobjects, even in presence of strong shadows and highlights. In the other hand\nto refine their playing strategy such as in football, handball, volley ball,\nRugby..., the coach need to have a maximum of technical-tactics information\nabout the on-going of the game and the players. We propose in this paper a\nrange of algorithms allowing the resolution of many problems appearing in the\nautomated process of team identification, where each player is affected to his\ncorresponding team relying on visual data. The developed system was tested on a\nmatch of the Tunisian national competition. This work is prominent for many\nnext computer vision studies as it's detailed in this study.\n",
          "  Recently, Adaboost has been widely used to improve the accuracy of any given\nlearning algorithm. In this paper we focus on designing an algorithm to employ\ncombination of Adaboost with Support Vector Machine as weak component\nclassifiers to be used in Face Detection Task. To obtain a set of effective\nSVM-weaklearner Classifier, this algorithm adaptively adjusts the kernel\nparameter in SVM instead of using a fixed one. Proposed combination outperforms\nin generalization in comparison with SVM on imbalanced classification problem.\nThe proposed here method is compared, in terms of classification accuracy, to\nother commonly used Adaboost methods, such as Decision Trees and Neural\nNetworks, on CMU+MIT face database. Results indicate that the performance of\nthe proposed method is overall superior to previous Adaboost approaches.\n",
          "  This paper shows how to improve the real-time object detection in complex\nrobotics applications, by exploring new visual features as AdaBoost weak\nclassifiers. These new features are symmetric Haar filters (enforcing global\nhorizontal and vertical symmetry) and N-connexity control points. Experimental\nevaluation on a car database show that the latter appear to provide the best\nresults for the vehicle-detection problem.\n",
          "  This paper proposes a method of gesture recognition with a focus on important\nactions for distinguishing similar gestures. The method generates a partial\naction sequence by using optical flow images, expresses the sequence in the\neigenspace, and checks the feature vector sequence by applying an optimum\npath-searching method of weighted graph to focus the important actions. Also\npresented are the results of an experiment on the recognition of similar sign\nlanguage words.\n",
          "  Nowadays government and private agencies use remote sensing imagery for a\nwide range of applications from military applications to farm development. The\nimages may be a panchromatic, multispectral, hyperspectral or even\nultraspectral of terra bytes. Remote sensing image classification is one\namongst the most significant application worlds for remote sensing. A few\nnumber of image classification algorithms have proved good precision in\nclassifying remote sensing data. But, of late, due to the increasing\nspatiotemporal dimensions of the remote sensing data, traditional\nclassification algorithms have exposed weaknesses necessitating further\nresearch in the field of remote sensing image classification. So an efficient\nclassifier is needed to classify the remote sensing images to extract\ninformation. We are experimenting with both supervised and unsupervised\nclassification. Here we compare the different classification methods and their\nperformances. It is found that Mahalanobis classifier performed the best in our\nclassification.\n",
          "  The repeatability and efficiency of a corner detector determines how likely\nit is to be useful in a real-world application. The repeatability is importand\nbecause the same scene viewed from different positions should yield features\nwhich correspond to the same real-world 3D locations [Schmid et al 2000]. The\nefficiency is important because this determines whether the detector combined\nwith further processing can operate at frame rate.\n  Three advances are described in this paper. First, we present a new heuristic\nfor feature detection, and using machine learning we derive a feature detector\nfrom this which can fully process live PAL video using less than 5% of the\navailable processing time. By comparison, most other detectors cannot even\noperate at frame rate (Harris detector 115%, SIFT 195%). Second, we generalize\nthe detector, allowing it to be optimized for repeatability, with little loss\nof efficiency. Third, we carry out a rigorous comparison of corner detectors\nbased on the above repeatability criterion applied to 3D scenes. We show that\ndespite being principally constructed for speed, on these stringent tests, our\nheuristic detector significantly outperforms existing feature detectors.\nFinally, the comparison demonstrates that using machine learning produces\nsignificant improvements in repeatability, yielding a detector that is both\nvery fast and very high quality.\n",
          "  We present promising results for real-time vehicle visual detection, obtained\nwith adaBoost using new original ?keypoints presence features?. These\nweak-classifiers produce a boolean response based on presence or absence in the\ntested image of a ?keypoint? (~ a SURF interest point) with a descriptor\nsufficiently similar (i.e. within a given distance) to a reference descriptor\ncharacterizing the feature. A first experiment was conducted on a public image\ndataset containing lateral-viewed cars, yielding 95% recall with 95% precision\non test set. Moreover, analysis of the positions of adaBoost-selected keypoints\nshow that they correspond to a specific part of the object category (such as\n?wheel? or ?side skirt?) and thus have a ?semantic? meaning.\n",
          "  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusion\ntherefore that ultimately the choice of technique adopted boils down to\npersonal preference and the uniqueness of the dataset at hand.\n",
          "  (ABRIDGED) In previous work, two platforms have been developed for testing\ncomputer-vision algorithms for robotic planetary exploration (McGuire et al.\n2004b,2005; Bartolo et al. 2007). The wearable-computer platform has been\ntested at geological and astrobiological field sites in Spain (Rivas\nVaciamadrid and Riba de Santiuste), and the phone-camera has been tested at a\ngeological field site in Malta. In this work, we (i) apply a Hopfield\nneural-network algorithm for novelty detection based upon color, (ii) integrate\na field-capable digital microscope on the wearable computer platform, (iii)\ntest this novelty detection with the digital microscope at Rivas Vaciamadrid,\n(iv) develop a Bluetooth communication mode for the phone-camera platform, in\norder to allow access to a mobile processing computer at the field sites, and\n(v) test the novelty detection on the Bluetooth-enabled phone-camera connected\nto a netbook computer at the Mars Desert Research Station in Utah. This systems\nengineering and field testing have together allowed us to develop a real-time\ncomputer-vision system that is capable, for example, of identifying lichens as\nnovel within a series of images acquired in semi-arid desert environments. We\nacquired sequences of images of geologic outcrops in Utah and Spain consisting\nof various rock types and colors to test this algorithm. The algorithm robustly\nrecognized previously-observed units by their color, while requiring only a\nsingle image or a few images to learn colors as familiar, demonstrating its\nfast learning capability.\n",
          "  Biogeography is the study of the geographical distribution of biological\norganisms. The mindset of the engineer is that we can learn from nature.\nBiogeography Based Optimization is a burgeoning nature inspired technique to\nfind the optimal solution of the problem. Satellite image classification is an\nimportant task because it is the only way we can know about the land cover map\nof inaccessible areas. Though satellite images have been classified in past by\nusing various techniques, the researchers are always finding alternative\nstrategies for satellite image classification so that they may be prepared to\nselect the most appropriate technique for the feature extraction task in hand.\nThis paper is focused on classification of the satellite image of a particular\nland cover using the theory of Biogeography based Optimization. The original\nBBO algorithm does not have the inbuilt property of clustering which is\nrequired during image classification. Hence modifications have been proposed to\nthe original algorithm and the modified algorithm is used to classify the\nsatellite image of a given region. The results indicate that highly accurate\nland cover features can be extracted effectively when the proposed algorithm is\nused.\n",
          "  In this project, we have developed a sign language tutor that lets users\nlearn isolated signs by watching recorded videos and by trying the same signs.\nThe system records the user's video and analyses it. If the sign is recognized,\nboth verbal and animated feedback is given to the user. The system is able to\nrecognize complex signs that involve both hand gestures and head movements and\nexpressions. Our performance tests yield a 99% recognition rate on signs\ninvolving only manual gestures and 85% recognition rate on signs that involve\nboth manual and non manual components, such as head movement and facial\nexpressions.\n",
          "  In this work, we first show that feature selection methods other than\nboosting can also be used for training an efficient object detector. In\nparticular, we introduce Greedy Sparse Linear Discriminant Analysis (GSLDA)\n\\cite{Moghaddam2007Fast} for its conceptual simplicity and computational\nefficiency; and slightly better detection performance is achieved compared with\n\\cite{Viola2004Robust}. Moreover, we propose a new technique, termed Boosted\nGreedy Sparse Linear Discriminant Analysis (BGSLDA), to efficiently train a\ndetection cascade. BGSLDA exploits the sample re-weighting property of boosting\nand the class-separability criterion of GSLDA.\n",
          "  Speaker identification is a powerful, non-invasive and in-expensive biometric\ntechnique. The recognition accuracy, however, deteriorates when noise levels\naffect a specific band of frequency. In this paper, we present a sub-band based\nspeaker identification that intends to improve the live testing performance.\nEach frequency sub-band is processed and classified independently. We also\ncompare the linear and non-linear merging techniques for the sub-bands\nrecognizer. Support vector machines and Gaussian Mixture models are the\nnon-linear merging techniques that are investigated. Results showed that the\nsub-band based method used with linear merging techniques enormously improved\nthe performance of the speaker identification over the performance of wide-band\nrecognizers when tested live. A live testing improvement of 9.78% was achieved\n",
          "  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusions that\nultimately the choice of technique adopted boils down to personal preference\nand the uniqueness of the dataset at hand.\n",
          "  This paper presents a tumor detection algorithm from mammogram. The proposed\nsystem focuses on the solution of two problems. One is how to detect tumors as\nsuspicious regions with a very weak contrast to their background and another is\nhow to extract features which categorize tumors. The tumor detection method\nfollows the scheme of (a) mammogram enhancement. (b) The segmentation of the\ntumor area. (c) The extraction of features from the segmented tumor area. (d)\nThe use of SVM classifier. The enhancement can be defined as conversion of the\nimage quality to a better and more understandable level. The mammogram\nenhancement procedure includes filtering, top hat operation, DWT. Then the\ncontrast stretching is used to increase the contrast of the image. The\nsegmentation of mammogram images has been playing an important role to improve\nthe detection and diagnosis of breast cancer. The most common segmentation\nmethod used is thresholding. The features are extracted from the segmented\nbreast area. Next stage include, which classifies the regions using the SVM\nclassifier. The method was tested on 75 mammographic images, from the mini-MIAS\ndatabase. The methodology achieved a sensitivity of 88.75%.\n",
          "  We present promising results for visual object categorization, obtained with\nadaBoost using new original ?keypoints-based features?. These weak-classifiers\nproduce a boolean response based on presence or absence in the tested image of\na ?keypoint? (a kind of SURF interest point) with a descriptor sufficiently\nsimilar (i.e. within a given distance) to a reference descriptor characterizing\nthe feature. A first experiment was conducted on a public image dataset\ncontaining lateral-viewed cars, yielding 95% recall with 95% precision on test\nset. Preliminary tests on a small subset of a pedestrians database also gives\npromising 97% recall with 92 % precision, which shows the generality of our new\nfamily of features. Moreover, analysis of the positions of adaBoost-selected\nkeypoints show that they correspond to a specific part of the object category\n(such as ?wheel? or ?side skirt? in the case of lateral-cars) and thus have a\n?semantic? meaning. We also made a first test on video for detecting vehicles\nfrom adaBoostselected keypoints filtered in real-time from all detected\nkeypoints.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "5_classifiers_classification_classifier",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "5_classifiers_classification_classifier"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          0.6547163128852844,
          0.5323556661605835,
          0.2906572222709656,
          0.7334747910499573,
          0.33914268016815186,
          0.5366765856742859,
          0.7559263706207275,
          0.3206445276737213,
          0.3099488317966461,
          0.8114856481552124,
          0.3373059034347534,
          0.7835290431976318,
          0.5285425782203674,
          0.4524402618408203,
          0.7619002461433411,
          0.8077278137207031,
          0.72822505235672,
          0.30862802267074585,
          0.5551848411560059
         ],
         "y": [
          6.100099086761475,
          6.105672836303711,
          5.859968662261963,
          5.984852313995361,
          5.949517250061035,
          6.231963634490967,
          5.468764305114746,
          5.961818218231201,
          5.9626665115356445,
          5.462774276733398,
          5.756622791290283,
          5.48195219039917,
          6.210962772369385,
          6.068490028381348,
          6.314826488494873,
          5.483861446380615,
          5.780846118927002,
          5.944452285766602,
          5.896117210388184
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  The ability of a classifier to take on new information and classes by\nevolving the classifier without it having to be fully retrained is known as\nincremental learning. Incremental learning has been successfully applied to\nmany classification problems, where the data is changing and is not all\navailable at once. In this paper there is a comparison between Learn++, which\nis one of the most recent incremental learning algorithms, and the new proposed\nmethod of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has\nshown good incremental learning capabilities on benchmark datasets on which the\nnew ILUGA method has been tested. ILUGA has also shown good incremental\nlearning ability using only a few classifiers and does not suffer from\ncatastrophic forgetting. The results obtained for ILUGA on the Optical\nCharacter Recognition (OCR) and Wine datasets are good, with an overall\naccuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT\nfor the difficult multi-class OCR dataset.\n",
          "  Ensemble classification is an emerging approach to land cover mapping whereby\nthe final classification output is a result of a consensus of classifiers.\nIntuitively, an ensemble system should consist of base classifiers which are\ndiverse i.e. classifiers whose decision boundaries err differently. In this\npaper ensemble feature selection is used to impose diversity in ensembles. The\nfeatures of the constituent base classifiers for each ensemble were created\nthrough an exhaustive search algorithm using different separability indices.\nFor each ensemble, the classification accuracy was derived as well as a\ndiversity measure purported to give a measure of the inensemble diversity. The\ncorrelation between ensemble classification accuracy and diversity measure was\ndetermined to establish the interplay between the two variables. From the\nfindings of this paper, diversity measures as currently formulated do not\nprovide an adequate means upon which to constitute ensembles for land cover\nmapping.\n",
          "  Feature selection is an indispensable preprocessing step when mining huge\ndatasets that can significantly improve the overall system performance.\nTherefore in this paper we focus on a hybrid approach of feature selection.\nThis method falls into two phases. The filter phase select the features with\nhighest information gain and guides the initialization of search process for\nwrapper phase whose output the final feature subset. The final feature subsets\nare passed through the Knearest neighbor classifier for classification of\nattacks. The effectiveness of this algorithm is demonstrated on DARPA KDDCUP99\ncyber attack dataset.\n",
          "  This article applies Machine Learning techniques to solve Intrusion Detection\nproblems within computer networks. Due to complex and dynamic nature of\ncomputer networks and hacking techniques, detecting malicious activities\nremains a challenging task for security experts, that is, currently available\ndefense systems suffer from low detection capability and high number of false\nalarms. To overcome such performance limitations, we propose a novel Machine\nLearning algorithm, namely Boosted Subspace Probabilistic Neural Network\n(BSPNN), which integrates an adaptive boosting technique and a semi parametric\nneural network to obtain good tradeoff between accuracy and generality. As the\nresult, learning bias and generalization variance can be significantly\nminimized. Substantial experiments on KDD 99 intrusion benchmark indicate that\nour model outperforms other state of the art learning algorithms, with\nsignificantly improved detection accuracy, minimal false alarms and relatively\nsmall computational complexity.\n",
          "  Boosting has attracted much research attention in the past decade. The\nsuccess of boosting algorithms may be interpreted in terms of the margin\ntheory. Recently it has been shown that generalization error of classifiers can\nbe obtained by explicitly taking the margin distribution of the training data\ninto account. Most of the current boosting algorithms in practice usually\noptimizes a convex loss function and do not make use of the margin\ndistribution. In this work we design a new boosting algorithm, termed\nmargin-distribution boosting (MDBoost), which directly maximizes the average\nmargin and minimizes the margin variance simultaneously. This way the margin\ndistribution is optimized. A totally-corrective optimization algorithm based on\ncolumn generation is proposed to implement MDBoost. Experiments on UCI datasets\nshow that MDBoost outperforms AdaBoost and LPBoost in most cases.\n",
          "  We develop the concept of ABC-Boost (Adaptive Base Class Boost) for\nmulti-class classification and present ABC-MART, a concrete implementation of\nABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has\nbeen very successful in large-scale applications. For binary classification,\nABC-MART recovers MART. For multi-class classification, ABC-MART considerably\nimproves MART, as evaluated on several public data sets.\n",
          "  We develop abc-logitboost, based on the prior work on abc-boost and robust\nlogitboost. Our extensive experiments on a variety of datasets demonstrate the\nconsiderable improvement of abc-logitboost over logitboost and abc-mart.\n",
          "  We present in this paper a study on the ability and the benefits of using a\nkeystroke dynamics authentication method for collaborative systems.\nAuthentication is a challenging issue in order to guarantee the security of use\nof collaborative systems during the access control step. Many solutions exist\nin the state of the art such as the use of one time passwords or smart-cards.\nWe focus in this paper on biometric based solutions that do not necessitate any\nadditional sensor. Keystroke dynamics is an interesting solution as it uses\nonly the keyboard and is invisible for users. Many methods have been published\nin this field. We make a comparative study of many of them considering the\noperational constraints of use for collaborative systems.\n",
          "  We study boosting algorithms from a new perspective. We show that the\nLagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with\ngeneralized hinge loss are all entropy maximization problems. By looking at the\ndual problems of these boosting algorithms, we show that the success of\nboosting algorithms can be understood in terms of maintaining a better margin\ndistribution by maximizing margins and at the same time controlling the margin\nvariance.We also theoretically prove that, approximately, AdaBoost maximizes\nthe average margin, instead of the minimum margin. The duality formulation also\nenables us to develop column generation based optimization algorithms, which\nare totally corrective. We show that they exhibit almost identical\nclassification results to that of standard stage-wise additive boosting\nalgorithms but with much faster convergence rates. Therefore fewer weak\nclassifiers are needed to build the ensemble using our proposed optimization\ntechnique.\n",
          "  In this paper we discuss the techniques involved in the design of the famous\nstatistical spam filters that include Naive Bayes, Term Frequency-Inverse\nDocument Frequency, K-Nearest Neighbor, Support Vector Machine, and Bayes\nAdditive Regression Tree. We compare these techniques with each other in terms\nof accuracy, recall, precision, etc. Further, we discuss the effectiveness and\nlimitations of statistical filters in filtering out various types of spam from\nlegitimate e-mails.\n",
          "  An approach to the acceleration of parametric weak classifier boosting is\nproposed. Weak classifier is called parametric if it has fixed number of\nparameters and, so, can be represented as a point into multidimensional space.\nGenetic algorithm is used instead of exhaustive search to learn parameters of\nsuch classifier. Proposed approach also takes cases when effective algorithm\nfor learning some of the classifier parameters exists into account. Experiments\nconfirm that such an approach can dramatically decrease classifier training\ntime while keeping both training and test errors small.\n",
          "  In this paper, we propose a special fusion method for combining ensembles of\nbase classifiers utilizing new neural networks in order to improve overall\nefficiency of classification. While ensembles are designed such that each\nclassifier is trained independently while the decision fusion is performed as a\nfinal procedure, in this method, we would be interested in making the fusion\nprocess more adaptive and efficient. This new combiner, called Neural Network\nKernel Least Mean Square1, attempts to fuse outputs of the ensembles of\nclassifiers. The proposed Neural Network has some special properties such as\nKernel abilities,Least Mean Square features, easy learning over variants of\npatterns and traditional neuron capabilities. Neural Network Kernel Least Mean\nSquare is a special neuron which is trained with Kernel Least Mean Square\nproperties. This new neuron is used as a classifiers combiner to fuse outputs\nof base neural network classifiers. Performance of this method is analyzed and\ncompared with other fusion methods. The analysis represents higher performance\nof our new method as opposed to others.\n",
          "  This paper aims to showcase the measure of structural diversity of an\nensemble of 9 classifiers and then map a relationship between this structural\ndiversity and accuracy. The structural diversity was induced by having\ndifferent architectures or structures of the classifiers The Genetical\nAlgorithms (GA) were used to derive the relationship between diversity and the\nclassification accuracy by evolving the classifiers and then picking 9\nclassifiers out on an ensemble of 60 classifiers. It was found that as the\nensemble became diverse the accuracy improved. However at a certain diversity\nmeasure the accuracy began to drop. The Kohavi-Wolpert variance method is used\nto measure the diversity of the ensemble. A method of voting is used to\naggregate the results from each classifier. The lowest error was observed at a\ndiversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024\nas maximum diversity measured. The parameters that were varied were: the number\nof hidden nodes, learning rate and the activation function.\n",
          "  In this paper entropy based methods are compared and used to measure\nstructural diversity of an ensemble of 21 classifiers. This measure is mostly\napplied in ecology, whereby species counts are used as a measure of diversity.\nThe measures used were Shannon entropy, Simpsons and the Berger Parker\ndiversity indexes. As the diversity indexes increased so did the accuracy of\nthe ensemble. An ensemble dominated by classifiers with the same structure\nproduced poor accuracy. Uncertainty rule from information theory was also used\nto further define diversity. Genetic algorithms were used to find the optimal\nensemble by using the diversity indices as the cost function. The method of\nvoting was used to aggregate the decisions.\n",
          "  User authentication and intrusion detection differ from standard\nclassification problems in that while we have data generated from legitimate\nusers, impostor or intrusion data is scarce or non-existent. We review existing\ntechniques for dealing with this problem and propose a novel alternative based\non a principled statistical decision-making view point. We examine the\ntechnique on a toy problem and validate it on complex real-world data from an\nRFID based access control system. The results indicate that it can\nsignificantly outperform the classical world model approach. The method could\nbe more generally useful in other decision-making scenarios where there is a\nlack of adversary data.\n",
          "  This empirical study is mainly devoted to comparing four tree-based boosting\nalgorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for\nmulti-class classification on a variety of publicly available datasets. Some of\nthose datasets have been thoroughly tested in prior studies using a broad range\nof classification algorithms including SVM, neural nets, and deep learning.\n  In terms of the empirical classification errors, our experiment results\ndemonstrate:\n  1. Abc-mart considerably improves mart. 2. Abc-logitboost considerably\nimproves (robust) logitboost. 3. Robust) logitboost} considerably improves mart\non most datasets. 4. Abc-logitboost considerably improves abc-mart on most\ndatasets. 5. These four boosting algorithms (especially abc-logitboost)\noutperform SVM on many datasets. 6. Compared to the best deep learning methods,\nthese four boosting algorithms (especially abc-logitboost) are competitive.\n",
          "  This paper proposes an efficient technique for partitioning large biometric\ndatabase during identification. In this technique feature vector which\ncomprises of global and local descriptors extracted from offline signature are\nused by fuzzy clustering technique to partition the database. As biometric\nfeatures posses no natural order of sorting, thus it is difficult to index them\nalphabetically or numerically. Hence, some supervised criteria is required to\npartition the search space. At the time of identification the fuzziness\ncriterion is introduced to find the nearest clusters for declaring the identity\nof query sample. The system is tested using bin-miss rate and performs better\nin comparison to traditional k-means approach.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "6_classifiers_classifier_classification",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "6_classifiers_classifier_classification"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          1.1160897016525269,
          1.2195779085159302,
          1.1939274072647095,
          1.2182358503341675,
          0.9619614481925964,
          0.9851876497268677,
          0.9378370642662048,
          1.034785509109497,
          0.9665422439575195,
          1.2391605377197266,
          0.9991132616996765,
          1.426193356513977,
          1.3347443342208862,
          1.269667387008667,
          1.1701319217681885,
          0.9540680646896362,
          0.8544086217880249,
          1.1106843948364258
         ],
         "y": [
          5.745619297027588,
          5.377476215362549,
          6.515815734863281,
          6.4645867347717285,
          5.950607776641846,
          6.03009033203125,
          5.998529434204102,
          6.40847110748291,
          5.9522833824157715,
          6.493264198303223,
          5.924606800079346,
          5.373619079589844,
          5.384960174560547,
          5.405280113220215,
          6.478903293609619,
          5.995344638824463,
          6.322323322296143,
          5.989516735076904
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  Minimizing the rank of a matrix subject to affine constraints is a\nfundamental problem with many important applications in machine learning and\nstatistics. In this paper we propose a simple and fast algorithm SVP (Singular\nValue Projection) for rank minimization with affine constraints (ARMP) and show\nthat SVP recovers the minimum rank solution for affine constraints that satisfy\nthe \"restricted isometry property\" and show robustness of our method to noise.\nOur results improve upon a recent breakthrough by Recht, Fazel and Parillo\n(RFP07) and Lee and Bresler (LB09) in three significant ways:\n  1) our method (SVP) is significantly simpler to analyze and easier to\nimplement,\n  2) we give recovery guarantees under strictly weaker isometry assumptions\n  3) we give geometric convergence guarantees for SVP even in presense of noise\nand, as demonstrated empirically, SVP is significantly faster on real-world and\nsynthetic problems.\n  In addition, we address the practically important problem of low-rank matrix\ncompletion (MCP), which can be seen as a special case of ARMP. We empirically\ndemonstrate that our algorithm recovers low-rank incoherent matrices from an\nalmost optimal number of uniformly sampled entries. We make partial progress\ntowards proving exact recovery and provide some intuition for the strong\nperformance of SVP applied to matrix completion by showing a more restricted\nisometry property. Our algorithm outperforms existing methods, such as those of\n\\cite{RFP07,CR08,CT09,CCS08,KOM09,LB09}, for ARMP and the matrix-completion\nproblem by an order of magnitude and is also significantly more robust to\nnoise.\n",
          "  The problem of completing a low-rank matrix from a subset of its entries is\noften encountered in the analysis of incomplete data sets exhibiting an\nunderlying factor model with applications in collaborative filtering, computer\nvision and control. Most recent work had been focused on constructing efficient\nalgorithms for exact or approximate recovery of the missing matrix entries and\nproving lower bounds for the number of known entries that guarantee a\nsuccessful recovery with high probability. A related problem from both the\nmathematical and algorithmic point of view is the distance geometry problem of\nrealizing points in a Euclidean space from a given subset of their pairwise\ndistances. Rigidity theory answers basic questions regarding the uniqueness of\nthe realization satisfying a given partial set of distances. We observe that\nbasic ideas and tools of rigidity theory can be adapted to determine uniqueness\nof low-rank matrix completion, where inner products play the role that\ndistances play in rigidity theory. This observation leads to an efficient\nrandomized algorithm for testing both local and global unique completion.\nCrucial to our analysis is a new matrix, which we call the completion matrix,\nthat serves as the analogue of the rigidity matrix.\n",
          "  This article treats the problem of learning a dictionary providing sparse\nrepresentations for a given signal class, via $\\ell_1$-minimisation. The\nproblem can also be seen as factorising a $\\ddim \\times \\nsig$ matrix $Y=(y_1\n>... y_\\nsig), y_n\\in \\R^\\ddim$ of training signals into a $\\ddim \\times\n\\natoms$ dictionary matrix $\\dico$ and a $\\natoms \\times \\nsig$ coefficient\nmatrix $\\X=(x_1... x_\\nsig), x_n \\in \\R^\\natoms$, which is sparse. The exact\nquestion studied here is when a dictionary coefficient pair $(\\dico,\\X)$ can be\nrecovered as local minimum of a (nonconvex) $\\ell_1$-criterion with input\n$Y=\\dico \\X$. First, for general dictionaries and coefficient matrices,\nalgebraic conditions ensuring local identifiability are derived, which are then\nspecialised to the case when the dictionary is a basis. Finally, assuming a\nrandom Bernoulli-Gaussian sparse model on the coefficient matrix, it is shown\nthat sufficiently incoherent bases are locally identifiable with high\nprobability. The perhaps surprising result is that the typically sufficient\nnumber of training samples $\\nsig$ grows up to a logarithmic factor only\nlinearly with the signal dimension, i.e. $\\nsig \\approx C \\natoms \\log\n\\natoms$, in contrast to previous approaches requiring combinatorially many\nsamples.\n",
          "  Sparse coding--that is, modelling data vectors as sparse linear combinations\nof basis elements--is widely used in machine learning, neuroscience, signal\nprocessing, and statistics. This paper focuses on the large-scale matrix\nfactorization problem that consists of learning the basis set, adapting it to\nspecific data. Variations of this problem include dictionary learning in signal\nprocessing, non-negative matrix factorization and sparse principal component\nanalysis. In this paper, we propose to address these tasks with a new online\noptimization algorithm, based on stochastic approximations, which scales up\ngracefully to large datasets with millions of training samples, and extends\nnaturally to various matrix factorization formulations, making it suitable for\na wide range of learning problems. A proof of convergence is presented, along\nwith experiments with natural images and genomic data demonstrating that it\nleads to state-of-the-art performance in terms of speed and optimization for\nboth small and large datasets.\n",
          "  In this paper we present a linear programming solution for sign pattern\nrecovery of a sparse signal from noisy random projections of the signal. We\nconsider two types of noise models, input noise, where noise enters before the\nrandom projection; and output noise, where noise enters after the random\nprojection. Sign pattern recovery involves the estimation of sign pattern of a\nsparse signal. Our idea is to pretend that no noise exists and solve the\nnoiseless $\\ell_1$ problem, namely, $\\min \\|\\beta\\|_1 ~ s.t. ~ y=G \\beta$ and\nquantizing the resulting solution. We show that the quantized solution\nperfectly reconstructs the sign pattern of a sufficiently sparse signal.\nSpecifically, we show that the sign pattern of an arbitrary k-sparse,\nn-dimensional signal $x$ can be recovered with $SNR=\\Omega(\\log n)$ and\nmeasurements scaling as $m= \\Omega(k \\log{n/k})$ for all sparsity levels $k$\nsatisfying $0< k \\leq \\alpha n$, where $\\alpha$ is a sufficiently small\npositive constant. Surprisingly, this bound matches the optimal\n\\emph{Max-Likelihood} performance bounds in terms of $SNR$, required number of\nmeasurements, and admissible sparsity level in an order-wise sense. In contrast\nto our results, previous results based on LASSO and Max-Correlation techniques\neither assume significantly larger $SNR$, sublinear sparsity levels or\nrestrictive assumptions on signal sets. Our proof technique is based on noisy\nperturbation of the noiseless $\\ell_1$ problem, in that, we estimate the\nmaximum admissible noise level before sign pattern recovery fails.\n",
          "  We consider the problem of reconstructing a low-rank matrix from a small\nsubset of its entries. In this paper, we describe the implementation of an\nefficient algorithm called OptSpace, based on singular value decomposition\nfollowed by local manifold optimization, for solving the low-rank matrix\ncompletion problem. It has been shown that if the number of revealed entries is\nlarge enough, the output of singular value decomposition gives a good estimate\nfor the original matrix, so that local optimization reconstructs the correct\nmatrix with high probability. We present numerical results which show that this\nalgorithm can reconstruct the low rank matrix exactly from a very small subset\nof its entries. We further study the robustness of the algorithm with respect\nto noise, and its performance on actual collaborative filtering datasets.\n",
          "  Motivated by the philosophy and phenomenal success of compressed sensing, the\nproblem of reconstructing a matrix from a sampling of its entries has attracted\nmuch attention recently. Such a problem can be viewed as an\ninformation-theoretic variant of the well-studied matrix completion problem,\nand the main objective is to design an efficient algorithm that can reconstruct\na matrix by inspecting only a small number of its entries. Although this is an\nimpossible task in general, Cand\\`es and co-authors have recently shown that\nunder a so-called incoherence assumption, a rank $r$ $n\\times n$ matrix can be\nreconstructed using semidefinite programming (SDP) after one inspects\n$O(nr\\log^6n)$ of its entries. In this paper we propose an alternative approach\nthat is much more efficient and can reconstruct a larger class of matrices by\ninspecting a significantly smaller number of the entries. Specifically, we\nfirst introduce a class of so-called stable matrices and show that it includes\nall those that satisfy the incoherence assumption. Then, we propose a\nrandomized basis pursuit (RBP) algorithm and show that it can reconstruct a\nstable rank $r$ $n\\times n$ matrix after inspecting $O(nr\\log n)$ of its\nentries. Our sampling bound is only a logarithmic factor away from the\ninformation-theoretic limit and is essentially optimal. Moreover, the runtime\nof the RBP algorithm is bounded by $O(nr^2\\log n+n^2r)$, which compares very\nfavorably with the $\\Omega(n^4r^2\\log^{12}n)$ runtime of the SDP-based\nalgorithm. Perhaps more importantly, our algorithm will provide an exact\nreconstruction of the input matrix in polynomial time. By contrast, the\nSDP-based algorithm can only provide an approximate one in polynomial time.\n",
          "  Suppose the signal x is realized by driving a k-sparse signal u through an\narbitrary unknown stable discrete-linear time invariant system H. These types\nof processes arise naturally in Reflection Seismology. In this paper we are\ninterested in several problems: (a) Blind-Deconvolution: Can we recover both\nthe filter $H$ and the sparse signal $u$ from noisy measurements? (b)\nCompressive Sensing: Is x compressible in the conventional sense of compressed\nsensing? Namely, can x, u and H be reconstructed from a sparse set of\nmeasurements. We develop novel L1 minimization methods to solve both cases and\nestablish sufficient conditions for exact recovery for the case when the\nunknown system H is auto-regressive (i.e. all pole) of a known order. In the\ncompressed sensing/sampling setting it turns out that both H and x can be\nreconstructed from O(k log(n)) measurements under certain technical conditions\non the support structure of u. Our main idea is to pass x through a linear time\ninvariant system G and collect O(k log(n)) sequential measurements. The filter\nG is chosen suitably, namely, its associated Toeplitz matrix satisfies the RIP\nproperty. We develop a novel LP optimization algorithm and show that both the\nunknown filter H and the sparse input u can be reliably estimated.\n",
          "  Let M be a random (alpha n) x n matrix of rank r<<n, and assume that a\nuniformly random subset E of its entries is observed. We describe an efficient\nalgorithm that reconstructs M from |E| = O(rn) observed entries with relative\nroot mean square error RMSE <= C(rn/|E|)^0.5 . Further, if r=O(1), M can be\nreconstructed exactly from |E| = O(n log(n)) entries. These results apply\nbeyond random matrices to general low-rank incoherent matrices.\n  This settles (in the case of bounded rank) a question left open by Candes and\nRecht and improves over the guarantees for their reconstruction algorithm. The\ncomplexity of our algorithm is O(|E|r log(n)), which opens the way to its use\nfor massive data sets. In the process of proving these statements, we obtain a\ngeneralization of a celebrated result by Friedman-Kahn-Szemeredi and Feige-Ofek\non the spectrum of sparse random matrices.\n",
          "  We present a convex formulation of dictionary learning for sparse signal\ndecomposition. Convexity is obtained by replacing the usual explicit upper\nbound on the dictionary size by a convex rank-reducing term similar to the\ntrace norm. In particular, our formulation introduces an explicit trade-off\nbetween size and sparsity of the decomposition of rectangular matrices. Using a\nlarge set of synthetic examples, we compare the estimation abilities of the\nconvex and non-convex approaches, showing that while the convex formulation has\na single local minimum, this may lead in some cases to performance which is\ninferior to the local minima of the non-convex formulation.\n",
          "  This article considers constrained $\\ell_1$ minimization methods for the\nrecovery of high dimensional sparse signals in three settings: noiseless,\nbounded error and Gaussian noise. A unified and elementary treatment is given\nin these noise settings for two $\\ell_1$ minimization methods: the Dantzig\nselector and $\\ell_1$ minimization with an $\\ell_2$ constraint. The results of\nthis paper improve the existing results in the literature by weakening the\nconditions and tightening the error bounds. The improvement on the conditions\nshows that signals with larger support can be recovered accurately. This paper\nalso establishes connections between restricted isometry property and the\nmutual incoherence property. Some results of Candes, Romberg and Tao (2006) and\nDonoho, Elad, and Temlyakov (2006) are extended.\n",
          "  Higher-order tensor decompositions are analogous to the familiar Singular\nValue Decomposition (SVD), but they transcend the limitations of matrices\n(second-order tensors). SVD is a powerful tool that has achieved impressive\nresults in information retrieval, collaborative filtering, computational\nlinguistics, computational vision, and other fields. However, SVD is limited to\ntwo-dimensional arrays of data (two modes), and many potential applications\nhave three or more modes, which require higher-order tensor decompositions.\nThis paper evaluates four algorithms for higher-order tensor decomposition:\nHigher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal\nIteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We\nmeasure the time (elapsed run time), space (RAM and disk space requirements),\nand fit (tensor reconstruction accuracy) of the four algorithms, under a\nvariety of conditions. We find that standard implementations of HO-SVD and HOOI\ndo not scale up to larger tensors, due to increasing RAM requirements. We\nrecommend HOOI for tensors that are small enough for the available RAM and MP\nfor larger tensors.\n",
          "  Given a matrix M of low-rank, we consider the problem of reconstructing it\nfrom noisy observations of a small, random subset of its entries. The problem\narises in a variety of applications, from collaborative filtering (the `Netflix\nproblem') to structure-from-motion and positioning. We study a low complexity\nalgorithm introduced by Keshavan et al.(2009), based on a combination of\nspectral techniques and manifold optimization, that we call here OptSpace. We\nprove performance guarantees that are order-optimal in a number of\ncircumstances.\n",
          "  We show that matrix completion with trace-norm regularization can be\nsignificantly hurt when entries of the matrix are sampled non-uniformly. We\nintroduce a weighted version of the trace-norm regularizer that works well also\nwith non-uniform sampling. Our experimental results demonstrate that the\nweighted trace-norm regularization indeed yields significant gains on the\n(highly non-uniformly sampled) Netflix dataset.\n",
          "  We consider a problem of significant practical importance, namely, the\nreconstruction of a low-rank data matrix from a small subset of its entries.\nThis problem appears in many areas such as collaborative filtering, computer\nvision and wireless sensor networks. In this paper, we focus on the matrix\ncompletion problem in the case when the observed samples are corrupted by\nnoise. We compare the performance of three state-of-the-art matrix completion\nalgorithms (OptSpace, ADMiRA and FPCA) on a single simulation platform and\npresent numerical results. We show that in practice these efficient algorithms\ncan be used to reconstruct real data matrices, as well as randomly generated\nmatrices, accurately.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "7_matrix_matrices_sparse",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "7_matrix_matrices_sparse"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          2.3355143070220947,
          2.3245010375976562,
          1.937542200088501,
          1.7525510787963867,
          2.0692362785339355,
          2.3204920291900635,
          2.4036898612976074,
          2.1008658409118652,
          2.4736568927764893,
          1.9254629611968994,
          2.0058696269989014,
          2.294110059738159,
          2.3649415969848633,
          2.272400140762329,
          2.3193411827087402,
          2.193344831466675
         ],
         "y": [
          9.16951847076416,
          9.13049030303955,
          9.553240776062012,
          9.423088073730469,
          9.597064018249512,
          9.11854076385498,
          9.295475959777832,
          9.56756591796875,
          9.27608585357666,
          9.508423805236816,
          9.593122482299805,
          9.175094604492188,
          9.174967765808105,
          9.131217956542969,
          9.161638259887695,
          9.32503604888916
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  We consider the task of learning a classifier from the feature space\n$\\mathcal{X}$ to the set of classes $\\mathcal{Y} = \\{0, 1\\}$, when the features\ncan be partitioned into class-conditionally independent feature sets\n$\\mathcal{X}_1$ and $\\mathcal{X}_2$. We show the surprising fact that the\nclass-conditional independence can be used to represent the original learning\ntask in terms of 1) learning a classifier from $\\mathcal{X}_2$ to\n$\\mathcal{X}_1$ and 2) learning the class-conditional distribution of the\nfeature set $\\mathcal{X}_1$. This fact can be exploited for semi-supervised\nlearning because the former task can be accomplished purely from unlabeled\nsamples. We present experimental evaluation of the idea in two real world\napplications.\n",
          "  Ensemble learning aims to improve generalization ability by using multiple\nbase learners. It is well-known that to construct a good ensemble, the base\nlearners should be accurate as well as diverse. In this paper, unlabeled data\nis exploited to facilitate ensemble learning by helping augment the diversity\namong the base learners. Specifically, a semi-supervised ensemble method named\nUDEED is proposed. Unlike existing semi-supervised ensemble methods where\nerror-prone pseudo-labels are estimated for unlabeled data to enlarge the\nlabeled data to improve accuracy, UDEED works by maximizing accuracies of base\nlearners on labeled data while maximizing diversity among them on unlabeled\ndata. Experiments show that UDEED can effectively utilize unlabeled data for\nensemble learning and is highly competitive to well-established semi-supervised\nensemble methods.\n",
          "  Machine Learning is usually defined as a subfield of AI, which is busy with\ninformation extraction from raw data sets. Despite of its common acceptance and\nwidespread recognition, this definition is wrong and groundless. Meaningful\ninformation does not belong to the data that bear it. It belongs to the\nobservers of the data and it is a shared agreement and a convention among them.\nTherefore, this private information cannot be extracted from the data by any\nmeans. Therefore, all further attempts of Machine Learning apologists to\njustify their funny business are inappropriate.\n",
          "  We present Searn, an algorithm for integrating search and learning to solve\ncomplex structured prediction problems such as those that occur in natural\nlanguage, speech, computational biology, and vision. Searn is a meta-algorithm\nthat transforms these complex problems into simple classification problems to\nwhich any binary classifier may be applied. Unlike current algorithms for\nstructured learning that require decomposition of both the loss function and\nthe feature functions over the predicted structure, Searn is able to learn\nprediction functions for any loss function and any class of features. Moreover,\nSearn comes with a strong, natural theoretical guarantee: good performance on\nthe derived classification problems implies good performance on the structured\nprediction problem.\n",
          "  We describe an adaptation and application of a search-based structured\nprediction algorithm \"Searn\" to unsupervised learning problems. We show that it\nis possible to reduce unsupervised learning to supervised learning and\ndemonstrate a high-quality unsupervised shift-reduce parsing model. We\nadditionally show a close connection between unsupervised Searn and expectation\nmaximization. Finally, we demonstrate the efficacy of a semi-supervised\nextension. The key idea that enables this is an application of the predict-self\nidea for unsupervised learning.\n",
          "  In conventional supervised pattern recognition tasks, model selection is\ntypically accomplished by minimizing the classification error rate on a set of\nso-called development data, subject to ground-truth labeling by human experts\nor some other means. In the context of speech processing systems and other\nlarge-scale practical applications, however, such labeled development data are\ntypically costly and difficult to obtain. This article proposes an alternative\nsemi-supervised framework for likelihood-based model selection that leverages\nunlabeled data by using trained classifiers representing each model to\nautomatically generate putative labels. The errors that result from this\nautomatic labeling are shown to be amenable to results from robust statistics,\nwhich in turn provide for minimax-optimal censored likelihood ratio tests that\nrecover the nonparametric sign test as a limiting case. This approach is then\nvalidated experimentally using a state-of-the-art automatic speech recognition\nsystem to select between candidate word pronunciations using unlabeled speech\ndata that only potentially contain instances of the words under test. Results\nprovide supporting evidence for the utility of this approach, and suggest that\nit may also find use in other applications of machine learning.\n",
          "  We present an approach to semi-supervised learning based on an exponential\nfamily characterization. Our approach generalizes previous work on coupled\npriors for hybrid generative/discriminative models. Our model is more flexible\nand natural than previous approaches. Experimental results on several data sets\nshow that our approach also performs better in practice.\n",
          "  The problem of classifying sonar signals from rocks and mines first studied\nby Gorman and Sejnowski has become a benchmark against which many learning\nalgorithms have been tested. We show that both the training set and the test\nset of this benchmark are linearly separable, although with different\nhyperplanes. Moreover, the complete set of learning and test patterns together,\nis also linearly separable. We give the weights that separate these sets, which\nmay be used to compare results found by other algorithms.\n",
          "  A bag-of-words based probabilistic classifier is trained using regularized\nlogistic regression to detect vandalism in the English Wikipedia. Isotonic\nregression is used to calibrate the class membership probabilities. Learning\ncurve, reliability, ROC, and cost analysis are performed.\n",
          "  Many popular linear classifiers, such as logistic regression, boosting, or\nSVM, are trained by optimizing a margin-based risk function. Traditionally,\nthese risk functions are computed based on a labeled dataset. We develop a\nnovel technique for estimating such risks using only unlabeled data and the\nmarginal label distribution. We prove that the proposed risk estimator is\nconsistent on high-dimensional datasets and demonstrate it on synthetic and\nreal-world data. In particular, we show how the estimate is used for evaluating\nclassifiers in transfer learning, and for training classifiers with no labeled\ndata whatsoever.\n",
          "  Semisupervised learning has emerged as a popular framework for improving\nmodeling accuracy while controlling labeling cost. Based on an extension of\nstochastic composite likelihood we quantify the asymptotic accuracy of\ngenerative semi-supervised learning. In doing so, we complement\ndistribution-free analysis by providing an alternative framework to measure the\nvalue associated with different labeling policies and resolve the fundamental\nquestion of how much data to label and in what manner. We demonstrate our\napproach with both simulation studies and real world experiments using naive\nBayes for text classification and MRFs and CRFs for structured prediction in\nNLP.\n",
          "  Multi-instance learning attempts to learn from a training set consisting of\nlabeled bags each containing many unlabeled instances. Previous studies\ntypically treat the instances in the bags as independently and identically\ndistributed. However, the instances in a bag are rarely independent, and\ntherefore a better performance can be expected if the instances are treated in\nan non-i.i.d. way that exploits the relations among instances. In this paper,\nwe propose a simple yet effective multi-instance learning method, which regards\neach bag as a graph and uses a specific kernel to distinguish the graphs by\nconsidering the features of the nodes as well as the features of the edges that\nconvey some relations among instances. The effectiveness of the proposed method\nis validated by experiments.\n",
          "  Conditional Random Fields (CRFs) constitute a popular and efficient approach\nfor supervised sequence labelling. CRFs can cope with large description spaces\nand can integrate some form of structural dependency between labels. In this\ncontribution, we address the issue of efficient feature selection for CRFs\nbased on imposing sparsity through an L1 penalty. We first show how sparsity of\nthe parameter set can be exploited to significantly speed up training and\nlabelling. We then introduce coordinate descent parameter update schemes for\nCRFs with L1 regularization. We finally provide some empirical comparisons of\nthe proposed approach with state-of-the-art CRF training strategies. In\nparticular, it is shown that the proposed approach is able to take profit of\nthe sparsity to speed up processing and hence potentially handle larger\ndimensional models.\n",
          "  In this paper, we propose the MIML (Multi-Instance Multi-Label learning)\nframework where an example is described by multiple instances and associated\nwith multiple class labels. Compared to traditional learning frameworks, the\nMIML framework is more convenient and natural for representing complicated\nobjects which have multiple semantic meanings. To learn from MIML examples, we\npropose the MimlBoost and MimlSvm algorithms based on a simple degeneration\nstrategy, and experiments show that solving problems involving complicated\nobjects with multiple semantic meanings in the MIML framework can lead to good\nperformance. Considering that the degeneration process may lose information, we\npropose the D-MimlSvm algorithm which tackles MIML problems directly in a\nregularization framework. Moreover, we show that even when we do not have\naccess to the real objects and thus cannot capture more information from real\nobjects by using the MIML representation, MIML is still useful. We propose the\nInsDif and SubCod algorithms. InsDif works by transforming single-instances\ninto the MIML representation for learning, while SubCod works by transforming\nsingle-label examples into the MIML representation for learning. Experiments\nshow that in some tasks they are able to achieve better performance than\nlearning the single-instances or single-label examples directly.\n",
          "  Classification of some objects in classes of concepts is an essential and\neven breathtaking task in many applications. A solution is discussed here based\non Multi-Agent systems. A kernel of some expert agents in several classes is to\nconsult a central agent decide among the classification problem of a certain\nobject. This kernel is moderated with the center agent, trying to manage the\nquerying agents for any decision problem by means of a data-header like feature\nset. Agents have cooperation among concepts related to the classes of this\nclassification decision-making; and may affect on each others' results on a\ncertain query object in a multi-agent learning approach. This leads to an\nonline feature learning via the consulting trend. The performance is discussed\nto be much better in comparison to some other prior trends while system's\nmessage passing overload is decreased to less agents and the expertism helps\nthe performance and operability of system win the comparison.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "8_supervised_classifiers_classifier",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "8_supervised_classifiers_classifier"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          2.353687047958374,
          2.0781819820404053,
          1.9041129350662231,
          2.3463330268859863,
          2.3613526821136475,
          2.104536771774292,
          2.2576956748962402,
          2.305478096008301,
          1.8186544179916382,
          1.9606785774230957,
          2.289602518081665,
          2.4611754417419434,
          2.365471601486206,
          2.487358808517456,
          2.2660531997680664,
          2.224024772644043
         ],
         "y": [
          6.203406810760498,
          6.02625036239624,
          6.472897052764893,
          6.253025531768799,
          6.0581135749816895,
          6.07964563369751,
          6.091323375701904,
          6.2685227394104,
          6.239115238189697,
          6.18444299697876,
          5.975403308868408,
          5.774672985076904,
          5.938002586364746,
          5.656215190887451,
          5.745507717132568,
          6.064436435699463
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  In a variety of disciplines such as social sciences, psychology, medicine and\neconomics, the recorded data are considered to be noisy measurements of latent\nvariables connected by some causal structure. This corresponds to a family of\ngraphical models known as the structural equation model with latent variables.\nWhile linear non-Gaussian variants have been well-studied, inference in\nnonparametric structural equation models is still underdeveloped. We introduce\na sparse Gaussian process parameterization that defines a non-linear structure\nconnecting latent variables, unlike common formulations of Gaussian process\nlatent variable models. The sparse parameterization is given a full Bayesian\ntreatment without compromising Markov chain Monte Carlo efficiency. We compare\nthe stability of the sampling procedure and the predictive ability of the model\nagainst the current practice.\n",
          "  In this paper we adapt online estimation strategies to perform model-based\nclustering on large networks. Our work focuses on two algorithms, the first\nbased on the SAEM algorithm, and the second on variational methods. These two\nstrategies are compared with existing approaches on simulated and real data. We\nuse the method to decipher the connexion structure of the political websphere\nduring the US political campaign in 2008. We show that our online EM-based\nalgorithms offer a good trade-off between precision and speed, when estimating\nparameters for mixture distributions in the context of random graphs.\n",
          "  In this paper, we present two classes of Bayesian approaches to the\ntwo-sample problem. Our first class of methods extends the Bayesian t-test to\ninclude all parametric models in the exponential family and their conjugate\npriors. Our second class of methods uses Dirichlet process mixtures (DPM) of\nsuch conjugate-exponential distributions as flexible nonparametric priors over\nthe unknown distributions.\n",
          "  We propose a nonparametric Bayesian factor regression model that accounts for\nuncertainty in the number of factors, and the relationship between factors. To\naccomplish this, we propose a sparse variant of the Indian Buffet Process and\ncouple this with a hierarchical model over factors, based on Kingman's\ncoalescent. We apply this model to two problems (factor analysis and factor\nregression) in gene-expression data analysis.\n",
          "  Discovering latent representations of the observed world has become\nincreasingly more relevant in data analysis. Much of the effort concentrates on\nbuilding latent variables which can be used in prediction problems, such as\nclassification and regression. A related goal of learning latent structure from\ndata is that of identifying which hidden common causes generate the\nobservations, such as in applications that require predicting the effect of\npolicies. This will be the main problem tackled in our contribution: given a\ndataset of indicators assumed to be generated by unknown and unmeasured common\ncauses, we wish to discover which hidden common causes are those, and how they\ngenerate our data. This is possible under the assumption that observed\nvariables are linear functions of the latent causes with additive noise.\nPrevious results in the literature present solutions for the case where each\nobserved variable is a noisy function of a single latent variable. We show how\nto extend the existing results for some cases where observed variables measure\nmore than one latent variable.\n",
          "  Given R groups of numerical variables X1, ... XR, we assume that each group\nis the result of one underlying latent variable, and that all latent variables\nare bound together through a linear equation system. Moreover, we assume that\nsome explanatory latent variables may interact pairwise in one or more\nequations. We basically consider PLS Path Modelling's algorithm to estimate\nboth latent variables and the model's coefficients. New \"external\" estimation\nschemes are proposed that draw latent variables towards strong group structures\nin a more flexible way. New \"internal\" estimation schemes are proposed to\nenable PLSPM to make good use of variable group complementarity and to deal\nwith interactions. Application examples are given.\n",
          "  Detection of rare variants by resequencing is important for the\nidentification of individuals carrying disease variants. Rapid sequencing by\nnew technologies enables low-cost resequencing of target regions, although it\nis still prohibitive to test more than a few individuals. In order to improve\ncost trade-offs, it has recently been suggested to apply pooling designs which\nenable the detection of carriers of rare alleles in groups of individuals.\nHowever, this was shown to hold only for a relatively low number of individuals\nin a pool, and requires the design of pooling schemes for particular cases.\n  We propose a novel pooling design, based on a compressed sensing approach,\nwhich is both general, simple and efficient. We model the experimental\nprocedure and show via computer simulations that it enables the recovery of\nrare allele carriers out of larger groups than were possible before, especially\nin situations where high coverage is obtained for each individual.\n  Our approach can also be combined with barcoding techniques to enhance\nperformance and provide a feasible solution based on current resequencing\ncosts. For example, when targeting a small enough genomic region (~100\nbase-pairs) and using only ~10 sequencing lanes and ~10 distinct barcodes, one\ncan recover the identity of 4 rare allele carriers out of a population of over\n4000 individuals.\n",
          "  We consider the problem of analyzing the heterogeneity of clustering\ndistributions for multiple groups of observed data, each of which is indexed by\na covariate value, and inferring global clusters arising from observations\naggregated over the covariate domain. We propose a novel Bayesian nonparametric\nmethod reposing on the formalism of spatial modeling and a nested hierarchy of\nDirichlet processes. We provide an analysis of the model properties, relating\nand contrasting the notions of local and global clusters. We also provide an\nefficient inference algorithm, and demonstrate the utility of our method in\nseveral data examples, including the problem of object tracking and a global\nclustering analysis of functional data where the functional identity\ninformation is not available.\n",
          "  Clusters of genes that have evolved by repeated segmental duplication present\ndifficult challenges throughout genomic analysis, from sequence assembly to\nfunctional analysis. Improved understanding of these clusters is of utmost\nimportance, since they have been shown to be the source of evolutionary\ninnovation, and have been linked to multiple diseases, including HIV and a\nvariety of cancers. Previously, Zhang et al. (2008) developed an algorithm for\nreconstructing parsimonious evolutionary histories of such gene clusters, using\nonly human genomic sequence data. In this paper, we propose a probabilistic\nmodel for the evolution of gene clusters on a phylogeny, and an MCMC algorithm\nfor reconstruction of duplication histories from genomic sequences in multiple\nspecies. Several projects are underway to obtain high quality BAC-based\nassemblies of duplicated clusters in multiple species, and we anticipate that\nour method will be useful in analyzing these valuable new data sets.\n",
          "  Observations consisting of measurements on relationships for pairs of objects\narise in many settings, such as protein interaction and gene regulatory\nnetworks, collections of author-recipient email, and social networks. Analyzing\nsuch data with probabilisic models can be delicate because the simple\nexchangeability assumptions underlying many boilerplate models no longer hold.\nIn this paper, we describe a latent variable model of such data called the\nmixed membership stochastic blockmodel. This model extends blockmodels for\nrelational data to ones which capture mixed membership latent relational\nstructure, thus providing an object-specific low-dimensional representation. We\ndevelop a general variational inference algorithm for fast approximate\nposterior inference. We explore applications to social and protein interaction\nnetworks.\n",
          "  Statistically resolving the underlying haplotype pair for a genotype\nmeasurement is an important intermediate step in gene mapping studies, and has\nreceived much attention recently. Consequently, a variety of methods for this\nproblem have been developed. Different methods employ different statistical\nmodels, and thus implicitly encode different assumptions about the nature of\nthe underlying haplotype structure. Depending on the population sample in\nquestion, their relative performance can vary greatly, and it is unclear which\nmethod to choose for a particular sample. Instead of choosing a single method,\nwe explore combining predictions returned by different methods in a principled\nway, and thereby circumvent the problem of method selection.\n  We propose several techniques for combining haplotype reconstructions and\nanalyze their computational properties. In an experimental study on real-world\nhaplotype data we show that such techniques can provide more accurate and\nrobust reconstructions, and are useful for outlier detection. Typically, the\ncombined prediction is at least as accurate as or even more accurate than the\nbest individual method, effectively circumventing the method selection problem.\n",
          "  We investigate the problem of learning a topic model - the well-known Latent\nDirichlet Allocation - in a distributed manner, using a cluster of C processors\nand dividing the corpus to be learned equally among them. We propose a simple\napproximated method that can be tuned, trading speed for accuracy according to\nthe task at hand. Our approach is asynchronous, and therefore suitable for\nclusters of heterogenous machines.\n",
          "  We develop a Bayesian framework for tackling the supervised clustering\nproblem, the generic problem encountered in tasks such as reference matching,\ncoreference resolution, identity uncertainty and record linkage. Our clustering\nmodel is based on the Dirichlet process prior, which enables us to define\ndistributions over the countably infinite sets that naturally arise in this\nproblem. We add supervision to our model by positing the existence of a set of\nunobserved random variables (we call these \"reference types\") that are generic\nacross all clusters. Inference in our framework, which requires integrating\nover infinitely many parameters, is solved using Markov chain Monte Carlo\ntechniques. We present algorithms for both conjugate and non-conjugate priors.\nWe present a simple--but general--parameterization of our model based on a\nGaussian assumption. We evaluate this model on one artificial task and three\nreal-world tasks, comparing it against both unsupervised and state-of-the-art\nsupervised algorithms. Our results show that our model is able to outperform\nother models across a variety of tasks and performance metrics.\n",
          "  Dirichlet process (DP) mixture models provide a flexible Bayesian framework\nfor density estimation. Unfortunately, their flexibility comes at a cost:\ninference in DP mixture models is computationally expensive, even when\nconjugate distributions are used. In the common case when one seeks only a\nmaximum a posteriori assignment of data points to clusters, we show that search\nalgorithms provide a practical alternative to expensive MCMC and variational\ntechniques. When a true posterior sample is desired, the solution found by\nsearch can serve as a good initializer for MCMC. Experimental results show that\nusing these techniques is it possible to apply DP mixture models to very large\ndata sets.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "9_models_sparse_latent",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "9_models_sparse_latent"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          4.661062717437744,
          4.126901149749756,
          4.5373945236206055,
          4.5525383949279785,
          4.729292392730713,
          4.731328964233398,
          4.620098114013672,
          4.475303649902344,
          4.615683555603027,
          4.206617832183838,
          4.590991020202637,
          4.382652282714844,
          4.509832382202148,
          4.531247138977051,
          4.51935338973999
         ],
         "y": [
          5.580745697021484,
          4.909178733825684,
          5.473282814025879,
          5.45182466506958,
          5.698439598083496,
          5.682394981384277,
          4.955145835876465,
          5.409614086151123,
          4.959344863891602,
          4.785326957702637,
          4.93168306350708,
          5.2351813316345215,
          5.45086669921875,
          5.434956073760986,
          5.282713413238525
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  The key approaches for machine learning, especially learning in unknown\nprobabilistic environments are new representations and computation mechanisms.\nIn this paper, a novel quantum reinforcement learning (QRL) method is proposed\nby combining quantum theory and reinforcement learning (RL). Inspired by the\nstate superposition principle and quantum parallelism, a framework of value\nupdating algorithm is introduced. The state (action) in traditional RL is\nidentified as the eigen state (eigen action) in QRL. The state (action) set can\nbe represented with a quantum superposition state and the eigen state (eigen\naction) can be obtained by randomly observing the simulated quantum state\naccording to the collapse postulate of quantum measurement. The probability of\nthe eigen action is determined by the probability amplitude, which is\nparallelly updated according to rewards. Some related characteristics of QRL\nsuch as convergence, optimality and balancing between exploration and\nexploitation are also analyzed, which shows that this approach makes a good\ntradeoff between exploration and exploitation using the probability amplitude\nand can speed up learning through the quantum parallelism. To evaluate the\nperformance and practicability of QRL, several simulated experiments are given\nand the results demonstrate the effectiveness and superiority of QRL algorithm\nfor some complex problems. The present work is also an effective exploration on\nthe application of quantum computation to artificial intelligence.\n",
          "  This paper presents studies on a deterministic annealing algorithm based on\nquantum annealing for variational Bayes (QAVB) inference, which can be seen as\nan extension of the simulated annealing for variational Bayes (SAVB) inference.\nQAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a better\nlocal optimum than SAVB in terms of the variational free energy in latent\nDirichlet allocation (LDA).\n",
          "  Last year, in 2008, I gave a talk titled {\\it Quantum Calisthenics}. This\nyear I am going to tell you about how the work I described then has spun off\ninto a most unlikely direction. What I am going to talk about is how one maps\nthe problem of finding clusters in a given data set into a problem in quantum\nmechanics. I will then use the tricks I described to let quantum evolution lets\nthe clusters come together on their own.\n",
          "  Statistical modeling of nuclear data provides a novel approach to nuclear\nsystematics complementary to established theoretical and phenomenological\napproaches based on quantum theory. Continuing previous studies in which global\nstatistical modeling is pursued within the general framework of machine\nlearning theory, we implement advances in training algorithms designed to\nimproved generalization, in application to the problem of reproducing and\npredicting the halflives of nuclear ground states that decay 100% by the beta^-\nmode. More specifically, fully-connected, multilayer feedforward artificial\nneural network models are developed using the Levenberg-Marquardt optimization\nalgorithm together with Bayesian regularization and cross-validation. The\npredictive performance of models emerging from extensive computer experiments\nis compared with that of traditional microscopic and phenomenological models as\nwell as with the performance of other learning systems, including earlier\nneural network models as well as the support vector machines recently applied\nto the same problem. In discussing the results, emphasis is placed on\npredictions for nuclei that are far from the stability line, and especially\nthose involved in the r-process nucleosynthesis. It is found that the new\nstatistical models can match or even surpass the predictive performance of\nconventional models for beta-decay systematics and accordingly should provide a\nvaluable additional tool for exploring the expanding nuclear landscape.\n",
          "  In this article we develop quantum algorithms for learning and testing\njuntas, i.e. Boolean functions which depend only on an unknown set of k out of\nn input variables. Our aim is to develop efficient algorithms:\n  - whose sample complexity has no dependence on n, the dimension of the domain\nthe Boolean functions are defined over;\n  - with no access to any classical or quantum membership (\"black-box\")\nqueries. Instead, our algorithms use only classical examples generated\nuniformly at random and fixed quantum superpositions of such classical\nexamples;\n  - which require only a few quantum examples but possibly many classical\nrandom examples (which are considered quite \"cheap\" relative to quantum\nexamples).\n  Our quantum algorithms are based on a subroutine FS which enables sampling\naccording to the Fourier spectrum of f; the FS subroutine was used in earlier\nwork of Bshouty and Jackson on quantum learning. Our results are as follows:\n  - We give an algorithm for testing k-juntas to accuracy $\\epsilon$ that uses\n$O(k/\\epsilon)$ quantum examples. This improves on the number of examples used\nby the best known classical algorithm.\n  - We establish the following lower bound: any FS-based k-junta testing\nalgorithm requires $\\Omega(\\sqrt{k})$ queries.\n  - We give an algorithm for learning $k$-juntas to accuracy $\\epsilon$ that\nuses $O(\\epsilon^{-1} k\\log k)$ quantum examples and $O(2^k \\log(1/\\epsilon))$\nrandom examples. We show that this learning algorithms is close to optimal by\ngiving a related lower bound.\n",
          "  Quantum classification is defined as the task of predicting the associated\nclass of an unknown quantum state drawn from an ensemble of pure states given a\nfinite number of copies of this state. By recasting the state discrimination\nproblem within the framework of Machine Learning (ML), we can use the notion of\nlearning reduction coming from classical ML to solve different variants of the\nclassification task, such as the weighted binary and the multiclass versions.\n",
          "  The enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum random walk (QRW) with the\nproblem of data clustering, and develop two clustering algorithms based on the\none dimensional QRW. Then, the probability distributions on the positions\ninduced by QRW in these algorithms are investigated, which also indicates the\npossibility of obtaining better results. Consequently, the experimental results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms are of fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach.\n",
          "  The paper describes a neural approach for modelling and control of a\nturbocharged Diesel engine. A neural model, whose structure is mainly based on\nsome physical equations describing the engine behaviour, is built for the\nrotation speed and the exhaust gas opacity. The model is composed of three\ninterconnected neural submodels, each of them constituting a nonlinear\nmulti-input single-output error model. The structural identification and the\nparameter estimation from data gathered on a real engine are described. The\nneural direct model is then used to determine a neural controller of the\nengine, in a specialized training scheme minimising a multivariable criterion.\nSimulations show the effect of the pollution constraint weighting on a\ntrajectory tracking of the engine speed. Neural networks, which are flexible\nand parsimonious nonlinear black-box models, with universal approximation\ncapabilities, can accurately describe or control complex nonlinear systems,\nwith little a priori theoretical knowledge. The presented work extends optimal\nneuro-control to the multivariable case and shows the flexibility of neural\noptimisers. Considering the preliminary results, it appears that neural\nnetworks can be used as embedded models for engine control, to satisfy the more\nand more restricting pollutant emission legislation. Particularly, they are\nable to model nonlinear dynamics and outperform during transients the control\nschemes based on static mappings.\n",
          "  In this paper, we introduce elements of probabilistic model that is suitable\nfor modeling of learning algorithms in biologically plausible artificial neural\nnetworks framework. Model is based on two of the main concepts in quantum\nphysics - a density matrix and the Born rule. As an example, we will show that\nproposed probabilistic interpretation is suitable for modeling of on-line\nlearning algorithms for PSA, which are preferably realized by a parallel\nhardware based on very simple computational units. Proposed concept (model) can\nbe used in the context of improving algorithm convergence speed, learning\nfactor choice, or input signal scale robustness. We are going to see how the\nBorn rule and the Hebbian learning rule are connected\n",
          "  We define a new model of quantum learning that we call Predictive Quantum\n(PQ). This is a quantum analogue of PAC, where during the testing phase the\nstudent is only required to answer a polynomial number of testing queries.\n  We demonstrate a relational concept class that is efficiently learnable in\nPQ, while in any \"reasonable\" classical model exponential amount of training\ndata would be required. This is the first unconditional separation between\nquantum and classical learning.\n  We show that our separation is the best possible in several ways; in\nparticular, there is no analogous result for a functional class, as well as for\nseveral weaker versions of quantum learning. In order to demonstrate tightness\nof our separation we consider a special case of one-way communication that we\ncall single-input mode, where Bob receives no input. Somewhat surprisingly,\nthis setting becomes nontrivial when relational communication tasks are\nconsidered. In particular, any problem with two-sided input can be transformed\ninto a single-input relational problem of equal classical one-way cost. We show\nthat the situation is different in the quantum case, where the same\ntransformation can make the communication complexity exponentially larger. This\nhappens if and only if the original problem has exponential gap between quantum\nand classical one-way communication costs. We believe that these auxiliary\nresults might be of independent interest.\n",
          "  Enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum game with the problem of data\nclustering, and then develop a quantum-game-based clustering algorithm, in\nwhich data points in a dataset are considered as players who can make decisions\nand implement quantum strategies in quantum games. After each round of a\nquantum game, each player's expected payoff is calculated. Later, he uses a\nlink-removing-and-rewiring (LRR) function to change his neighbors and adjust\nthe strength of links connecting to them in order to maximize his payoff.\nFurther, algorithms are discussed and analyzed in two cases of strategies, two\npayoff matrixes and two LRR functions. Consequently, the simulation results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms have fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach.\n",
          "  We examine the complexity of learning the distributions produced by\nfinite-state quantum sources. We show how prior techniques for learning hidden\nMarkov models can be adapted to the quantum generator model to find that the\nanalogous state of affairs holds: information-theoretically, a polynomial\nnumber of samples suffice to approximately identify the distribution, but\ncomputationally, the problem is as hard as learning parities with noise, a\nnotorious open question in computational learning theory.\n",
          "  This paper studies quantum annealing (QA) for clustering, which can be seen\nas an extension of simulated annealing (SA). We derive a QA algorithm for\nclustering and propose an annealing schedule, which is crucial in practice.\nExperiments show the proposed QA algorithm finds better clustering assignments\nthan SA. Furthermore, QA is as easy as SA to implement.\n",
          "  In a previous publication we proposed discrete global optimization as a\nmethod to train a strong binary classifier constructed as a thresholded sum\nover weak classifiers. Our motivation was to cast the training of a classifier\ninto a format amenable to solution by the quantum adiabatic algorithm. Applying\nadiabatic quantum computing (AQC) promises to yield solutions that are superior\nto those which can be achieved with classical heuristic solvers. Interestingly\nwe found that by using heuristic solvers to obtain approximate solutions we\ncould already gain an advantage over the standard method AdaBoost. In this\ncommunication we generalize the baseline method to large scale classifier\ntraining. By large scale we mean that either the cardinality of the dictionary\nof candidate weak classifiers or the number of weak learners used in the strong\nclassifier exceed the number of variables that can be handled effectively in a\nsingle global optimization. For such situations we propose an iterative and\npiecewise approach in which a subset of weak classifiers is selected in each\niteration via global optimization. The strong classifier is then constructed by\nconcatenating the subsets of weak classifiers. We show in numerical studies\nthat the generalized method again successfully competes with AdaBoost. We also\nprovide theoretical arguments as to why the proposed optimization method, which\ndoes not only minimize the empirical loss but also adds L0-norm regularization,\nis superior to versions of boosting that only minimize the empirical loss. By\nconducting a Quantum Monte Carlo simulation we gather evidence that the quantum\nadiabatic algorithm is able to handle a generic training problem efficiently.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "10_quantum_qavb_qrw",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "10_quantum_qavb_qrw"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.639626979827881,
          6.469695568084717,
          6.59773588180542,
          6.1056976318359375,
          6.5210089683532715,
          6.567094802856445,
          6.576768398284912,
          6.264934539794922,
          6.364805698394775,
          6.492249488830566,
          6.592778205871582,
          6.491143703460693,
          6.614126205444336,
          6.563221454620361,
          6.490063667297363
         ],
         "y": [
          8.367668151855469,
          8.518453598022461,
          8.596734046936035,
          8.181623458862305,
          8.47926139831543,
          8.429920196533203,
          8.563286781311035,
          8.147708892822266,
          8.271675109863281,
          8.441301345825195,
          8.57755184173584,
          8.451875686645508,
          8.621485710144043,
          8.503795623779297,
          8.439453125
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  We consider the design of cognitive Medium Access Control (MAC) protocols\nenabling an unlicensed (secondary) transmitter-receiver pair to communicate\nover the idle periods of a set of licensed channels, i.e., the primary network.\nThe objective is to maximize data throughput while maintaining the\nsynchronization between secondary users and avoiding interference with licensed\n(primary) users. No statistical information about the primary traffic is\nassumed to be available a-priori to the secondary user. We investigate two\ndistinct sensing scenarios. In the first, the secondary transmitter is capable\nof sensing all the primary channels, whereas it senses one channel only in the\nsecond scenario. In both cases, we propose MAC protocols that efficiently learn\nthe statistics of the primary traffic online. Our simulation results\ndemonstrate that the proposed blind protocols asymptotically achieve the\nthroughput obtained when prior knowledge of primary traffic statistics is\navailable.\n",
          "  In a sensor network, in practice, the communication among sensors is subject\nto:(1) errors or failures at random times; (3) costs; and(2) constraints since\nsensors and networks operate under scarce resources, such as power, data rate,\nor communication. The signal-to-noise ratio (SNR) is usually a main factor in\ndetermining the probability of error (or of communication failure) in a link.\nThese probabilities are then a proxy for the SNR under which the links operate.\nThe paper studies the problem of designing the topology, i.e., assigning the\nprobabilities of reliable communication among sensors (or of link failures) to\nmaximize the rate of convergence of average consensus, when the link\ncommunication costs are taken into account, and there is an overall\ncommunication budget constraint. To consider this problem, we address a number\nof preliminary issues: (1) model the network as a random topology; (2)\nestablish necessary and sufficient conditions for mean square sense (mss) and\nalmost sure (a.s.) convergence of average consensus when network links fail;\nand, in particular, (3) show that a necessary and sufficient condition for both\nmss and a.s. convergence is for the algebraic connectivity of the mean graph\ndescribing the network topology to be strictly positive. With these results, we\nformulate topology design, subject to random link failures and to a\ncommunication cost constraint, as a constrained convex optimization problem to\nwhich we apply semidefinite programming techniques. We show by an extensive\nnumerical study that the optimal design improves significantly the convergence\nspeed of the consensus algorithm and can achieve the asymptotic performance of\na non-random network at a fraction of the communication cost.\n",
          "  In this paper, we consider uplink transmissions involving multiple users\ncommunicating with a base station over a fading channel. We assume that the\nbase station does not coordinate the transmissions of the users and hence the\nusers employ random access communication. The situation is modeled as a\nnon-cooperative repeated game with incomplete information. Each user attempts\nto minimize its long term power consumption subject to a minimum rate\nrequirement. We propose a two timescale stochastic gradient algorithm (TTSGA)\nfor tuning the users' transmission probabilities. The algorithm includes a\n'waterfilling threshold update mechanism' that ensures that the rate\nconstraints are satisfied. We prove that under the algorithm, the users'\ntransmission probabilities converge to a Nash equilibrium. Moreover, we also\nprove that the rate constraints are satisfied; this is also demonstrated using\nsimulation studies.\n",
          "  We consider the task of opportunistic channel access in a primary system\ncomposed of independent Gilbert-Elliot channels where the secondary (or\nopportunistic) user does not dispose of a priori information regarding the\nstatistical characteristics of the system. It is shown that this problem may be\ncast into the framework of model-based learning in a specific class of\nPartially Observed Markov Decision Processes (POMDPs) for which we introduce an\nalgorithm aimed at striking an optimal tradeoff between the exploration (or\nestimation) and exploitation requirements. We provide finite horizon regret\nbounds for this algorithm as well as a numerical evaluation of its performance\nin the single channel model as well as in the case of stochastically identical\nchannels.\n",
          "  In this paper, we consider delay-optimal power and subcarrier allocation\ndesign for OFDMA systems with $N_F$ subcarriers, $K$ mobiles and one base\nstation. There are $K$ queues at the base station for the downlink traffic to\nthe $K$ mobiles with heterogeneous packet arrivals and delay requirements. We\nshall model the problem as a $K$-dimensional infinite horizon average reward\nMarkov Decision Problem (MDP) where the control actions are assumed to be a\nfunction of the instantaneous Channel State Information (CSI) as well as the\njoint Queue State Information (QSI). This problem is challenging because it\ncorresponds to a stochastic Network Utility Maximization (NUM) problem where\ngeneral solution is still unknown. We propose an {\\em online stochastic value\niteration} solution using {\\em stochastic approximation}. The proposed power\ncontrol algorithm, which is a function of both the CSI and the QSI, takes the\nform of multi-level water-filling. We prove that under two mild conditions in\nTheorem 1 (One is the stepsize condition. The other is the condition on\naccessibility of the Markov Chain, which can be easily satisfied in most of the\ncases we are interested.), the proposed solution converges to the optimal\nsolution almost surely (with probability 1) and the proposed framework offers a\npossible solution to the general stochastic NUM problem. By exploiting the\nbirth-death structure of the queue dynamics, we obtain a reduced complexity\ndecomposed solution with linear $\\mathcal{O}(KN_F)$ complexity and\n$\\mathcal{O}(K)$ memory requirement.\n",
          "  In this paper, we propose a general cross-layer optimization framework in\nwhich we explicitly consider both the heterogeneous and dynamically changing\ncharacteristics of delay-sensitive applications and the underlying time-varying\nnetwork conditions. We consider both the independently decodable data units\n(DUs, e.g. packets) and the interdependent DUs whose dependencies are captured\nby a directed acyclic graph (DAG). We first formulate the cross-layer design as\na non-linear constrained optimization problem by assuming complete knowledge of\nthe application characteristics and the underlying network conditions. The\nconstrained cross-layer optimization is decomposed into several cross-layer\noptimization subproblems for each DU and two master problems. The proposed\ndecomposition method determines the necessary message exchanges between layers\nfor achieving the optimal cross-layer solution. However, the attributes (e.g.\ndistortion impact, delay deadline etc) of future DUs as well as the network\nconditions are often unknown in the considered real-time applications. The\nimpact of current cross-layer actions on the future DUs can be characterized by\na state-value function in the Markov decision process (MDP) framework. Based on\nthe dynamic programming solution to the MDP, we develop a low-complexity\ncross-layer optimization algorithm using online learning for each DU\ntransmission. This online algorithm can be implemented in real-time in order to\ncope with unknown source characteristics, network dynamics and resource\nconstraints. Our numerical results demonstrate the efficiency of the proposed\nonline algorithm.\n",
          "  Knowing the largest rate at which data can be sent on an end-to-end path such\nthat the egress rate is equal to the ingress rate with high probability can be\nvery practical when choosing transmission rates in video streaming or selecting\npeers in peer-to-peer applications. We introduce probabilistic available\nbandwidth, which is defined in terms of ingress rates and egress rates of\ntraffic on a path, rather than in terms of capacity and utilization of the\nconstituent links of the path like the standard available bandwidth metric. In\nthis paper, we describe a distributed algorithm, based on a probabilistic\ngraphical model and Bayesian active learning, for simultaneously estimating the\nprobabilistic available bandwidth of multiple paths through a network. Our\nprocedure exploits the fact that each packet train provides information not\nonly about the path it traverses, but also about any path that shares a link\nwith the monitored path. Simulations and PlanetLab experiments indicate that\nthis process can dramatically reduce the number of probes required to generate\naccurate estimates.\n",
          "  Cross-layer optimization solutions have been proposed in recent years to\nimprove the performance of network users operating in a time-varying,\nerror-prone wireless environment. However, these solutions often rely on ad-hoc\noptimization approaches, which ignore the different environmental dynamics\nexperienced at various layers by a user and violate the layered network\narchitecture of the protocol stack by requiring layers to provide access to\ntheir internal protocol parameters to other layers. This paper presents a new\ntheoretic foundation for cross-layer optimization, which allows each layer to\nmake autonomous decisions individually, while maximizing the utility of the\nwireless user by optimally determining what information needs to be exchanged\namong layers. Hence, this cross-layer framework does not change the current\nlayered architecture. Specifically, because the wireless user interacts with\nthe environment at various layers of the protocol stack, the cross-layer\noptimization problem is formulated as a layered Markov decision process (MDP)\nin which each layer adapts its own protocol parameters and exchanges\ninformation (messages) with other layers in order to cooperatively maximize the\nperformance of the wireless user. The message exchange mechanism for\ndetermining the optimal cross-layer transmission strategies has been designed\nfor both off-line optimization and on-line dynamic adaptation. We also show\nthat many existing cross-layer optimization algorithms can be formulated as\nsimplified, sub-optimal, versions of our layered MDP framework.\n",
          "  In our previous work, we proposed a systematic cross-layer framework for\ndynamic multimedia systems, which allows each layer to make autonomous and\nforesighted decisions that maximize the system's long-term performance, while\nmeeting the application's real-time delay constraints. The proposed solution\nsolved the cross-layer optimization offline, under the assumption that the\nmultimedia system's probabilistic dynamics were known a priori. In practice,\nhowever, these dynamics are unknown a priori and therefore must be learned\nonline. In this paper, we address this problem by allowing the multimedia\nsystem layers to learn, through repeated interactions with each other, to\nautonomously optimize the system's long-term performance at run-time. We\npropose two reinforcement learning algorithms for optimizing the system under\ndifferent design constraints: the first algorithm solves the cross-layer\noptimization in a centralized manner, and the second solves it in a\ndecentralized manner. We analyze both algorithms in terms of their required\ncomputation, memory, and inter-layer communication overheads. After noting that\nthe proposed reinforcement learning algorithms learn too slowly, we introduce a\ncomplementary accelerated learning algorithm that exploits partial knowledge\nabout the system's dynamics in order to dramatically improve the system's\nperformance. In our experiments, we demonstrate that decentralized learning can\nperform as well as centralized learning, while enabling the layers to act\nautonomously. Additionally, we show that existing application-independent\nreinforcement learning algorithms, and existing myopic learning algorithms\ndeployed in multimedia systems, perform significantly worse than our proposed\napplication-aware and foresighted learning methods.\n",
          "  We study the problem of dynamic spectrum sensing and access in cognitive\nradio systems as a partially observed Markov decision process (POMDP). A group\nof cognitive users cooperatively tries to exploit vacancies in primary\n(licensed) channels whose occupancies follow a Markovian evolution. We first\nconsider the scenario where the cognitive users have perfect knowledge of the\ndistribution of the signals they receive from the primary users. For this\nproblem, we obtain a greedy channel selection and access policy that maximizes\nthe instantaneous reward, while satisfying a constraint on the probability of\ninterfering with licensed transmissions. We also derive an analytical universal\nupper bound on the performance of the optimal policy. Through simulation, we\nshow that our scheme achieves good performance relative to the upper bound and\nimproved performance relative to an existing scheme.\n  We then consider the more practical scenario where the exact distribution of\nthe signal from the primary is unknown. We assume a parametric model for the\ndistribution and develop an algorithm that can learn the true distribution,\nstill guaranteeing the constraint on the interference probability. We show that\nthis algorithm outperforms the naive design that assumes a worst case value for\nthe parameter. We also provide a proof for the convergence of the learning\nalgorithm.\n",
          "  This paper suggests the use of intelligent network-aware processing agents in\nwireless local area network drivers to generate metrics for bandwidth\nestimation based on real-time channel statistics to enable wireless multimedia\napplication adaptation. Various configurations in the wireless digital home are\nstudied and the experimental results with performance variations are presented.\n",
          "  In this paper, we model the various wireless users in a cognitive radio\nnetwork as a collection of selfish, autonomous agents that strategically\ninteract in order to acquire the dynamically available spectrum opportunities.\nOur main focus is on developing solutions for wireless users to successfully\ncompete with each other for the limited and time-varying spectrum\nopportunities, given the experienced dynamics in the wireless network. We\ncategorize these dynamics into two types: one is the disturbance due to the\nenvironment (e.g. wireless channel conditions, source traffic characteristics,\netc.) and the other is the impact caused by competing users. To analyze the\ninteractions among users given the environment disturbance, we propose a\ngeneral stochastic framework for modeling how the competition among users for\nspectrum opportunities evolves over time. At each stage of the dynamic resource\nallocation, a central spectrum moderator auctions the available resources and\nthe users strategically bid for the required resources. The joint bid actions\naffect the resource allocation and hence, the rewards and future strategies of\nall users. Based on the observed resource allocation and corresponding rewards\nfrom previous allocations, we propose a best response learning algorithm that\ncan be deployed by wireless users to improve their bidding policy at each\nstage. The simulation results show that by deploying the proposed best response\nlearning algorithm, the wireless users can significantly improve their own\nperformance in terms of both the packet loss rate and the incurred cost for the\nused resources.\n",
          "  In this paper, spectrum access in cognitive radio networks is modeled as a\nrepeated auction game subject to monitoring and entry costs. For secondary\nusers, sensing costs are incurred as the result of primary users' activity.\nFurthermore, each secondary user pays the cost of transmissions upon successful\nbidding for a channel. Knowledge regarding other secondary users' activity is\nlimited due to the distributed nature of the network. The resulting formulation\nis thus a dynamic game with incomplete information. In this paper, an efficient\nbidding learning algorithm is proposed based on the outcome of past\ntransactions. As demonstrated through extensive simulations, the proposed\ndistributed scheme outperforms a myopic one-stage algorithm, and can achieve a\ngood balance between efficiency and fairness.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "11_optimal_optimization_bandwidth",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "11_optimal_optimization_bandwidth"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          8.161649703979492,
          7.877881050109863,
          8.200775146484375,
          8.177486419677734,
          8.232193946838379,
          8.081280708312988,
          7.758116722106934,
          8.109036445617676,
          7.940190315246582,
          8.228384971618652,
          7.978098392486572,
          8.20592975616455,
          8.197113037109375,
          8.088318824768066
         ],
         "y": [
          5.956258296966553,
          6.045050621032715,
          6.027012825012207,
          6.092215061187744,
          6.054906845092773,
          5.922441005706787,
          5.73297643661499,
          5.935784816741943,
          6.068526744842529,
          6.030251502990723,
          5.755385398864746,
          5.982417583465576,
          5.971985816955566,
          5.9673237800598145
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  A dictionary defines words in terms of other words. Definitions can tell you\nthe meanings of words you don't know, but only if you know the meanings of the\ndefining words. How many words do you need to know (and which ones) in order to\nbe able to learn all the rest from definitions? We reduced dictionaries to\ntheir \"grounding kernels\" (GKs), about 10% of the dictionary, from which all\nthe other words could be defined. The GK words turned out to have\npsycholinguistic correlates: they were learned at an earlier age and more\nconcrete than the rest of the dictionary. But one can compress still more: the\nGK turns out to have internal structure, with a strongly connected \"kernel\ncore\" (KC) and a surrounding layer, from which a hierarchy of definitional\ndistances can be derived, all the way out to the periphery of the full\ndictionary. These definitional distances, too, are correlated with\npsycholinguistic variables (age of acquisition, concreteness, imageability,\noral and written frequency) and hence perhaps with the \"mental lexicon\" in each\nof our heads.\n",
          "  The generation of meaningless \"words\" matching certain statistical and/or\nlinguistic criteria is frequently needed for experimental purposes in\nPsycholinguistics. Such stimuli receive the name of pseudowords or nonwords in\nthe Cognitive Neuroscience literatue. The process for building nonwords\nsometimes has to be based on linguistic units such as syllables or morphemes,\nresulting in a numerical explosion of combinations when the size of the\nnonwords is increased. In this paper, a reactive tabu search scheme is proposed\nto generate nonwords of variables size. The approach builds pseudowords by\nusing a modified Metaheuristic algorithm based on a local search procedure\nenhanced by a feedback-based scheme. Experimental results show that the new\nalgorithm is a practical and effective tool for nonword generation.\n",
          "  We present an algorithmic framework for learning multiple related tasks. Our\nframework exploits a form of prior knowledge that relates the output spaces of\nthese tasks. We present PAC learning results that analyze the conditions under\nwhich such learning is possible. We present results on learning a shallow\nparser and named-entity recognition system that exploits our framework, showing\nconsistent improvements over baseline methods.\n",
          "  Many AI researchers and cognitive scientists have argued that analogy is the\ncore of cognition. The most influential work on computational modeling of\nanalogy-making is Structure Mapping Theory (SMT) and its implementation in the\nStructure Mapping Engine (SME). A limitation of SME is the requirement for\ncomplex hand-coded representations. We introduce the Latent Relation Mapping\nEngine (LRME), which combines ideas from SME and Latent Relational Analysis\n(LRA) in order to remove the requirement for hand-coded representations. LRME\nbuilds analogical mappings between lists of words, using a large corpus of raw\ntext to automatically discover the semantic relations among the words. We\nevaluate LRME on a set of twenty analogical mapping problems, ten based on\nscientific analogies and ten based on common metaphors. LRME achieves\nhuman-level performance on the twenty problems. We compare LRME with a variety\nof alternative approaches and find that they are not able to reach the same\nlevel of performance.\n",
          "  Nous pr\\'esentons dans cette contribution une approche \\`a la fois symbolique\net probabiliste permettant d'extraire l'information sur la segmentation du\nsignal de parole \\`a partir d'information prosodique. Nous utilisons pour ce\nfaire des grammaires probabilistes poss\\'edant une structure hi\\'erarchique\nminimale. La phase de construction des grammaires ainsi que leur pouvoir de\npr\\'ediction sont \\'evalu\\'es qualitativement ainsi que quantitativement.\n  -----\n  Methodologically oriented, the present work sketches an approach for prosodic\ninformation retrieval and speech segmentation, based on both symbolic and\nprobabilistic information. We have recourse to probabilistic grammars, within\nwhich we implement a minimal hierarchical structure. Both the stages of\nprobabilistic grammar building and its testing in prediction are explored and\nquantitatively and qualitatively evaluated.\n",
          "  This paper presents the current state of a work in progress, whose objective\nis to better understand the effects of factors that significantly influence the\nperformance of Latent Semantic Analysis (LSA). A difficult task, which consists\nin answering (French) biology Multiple Choice Questions, is used to test the\nsemantic properties of the truncated singular space and to study the relative\ninfluence of main parameters. A dedicated software has been designed to fine\ntune the LSA semantic space for the Multiple Choice Questions task. With\noptimal parameters, the performances of our simple model are quite surprisingly\nequal or superior to those of 7th and 8th grades students. This indicates that\nsemantic spaces were quite good despite their low dimensions and the small\nsizes of training data sets. Besides, we present an original entropy global\nweighting of answers' terms of each question of the Multiple Choice Questions\nwhich was necessary to achieve the model's success.\n",
          "  Data mining allows the exploration of sequences of phenomena, whereas one\nusually tends to focus on isolated phenomena or on the relation between two\nphenomena. It offers invaluable tools for theoretical analyses and exploration\nof the structure of sentences, texts, dialogues, and speech. We report here the\nresults of an attempt at using it for inspecting sequences of verbs from French\naccounts of road accidents. This analysis comes from an original approach of\nunsupervised training allowing the discovery of the structure of sequential\ndata. The entries of the analyzer were only made of the verbs appearing in the\nsentences. It provided a classification of the links between two successive\nverbs into four distinct clusters, allowing thus text segmentation. We give\nhere an interpretation of these clusters by applying a statistical analysis to\nindependent semantic annotations.\n",
          "  We present BayeSum (for ``Bayesian summarization''), a model for sentence\nextraction in query-focused summarization. BayeSum leverages the common case in\nwhich multiple documents are relevant to a single query. Using these documents\nas reinforcement for query terms, BayeSum is not afflicted by the paucity of\ninformation in short queries. We show that approximate inference in BayeSum is\npossible on large data sets and results in a state-of-the-art summarization\nsystem. Furthermore, we show how BayeSum can be understood as a justified query\nexpansion technique in the language modeling for IR framework.\n",
          "  Scenarios for the emergence or bootstrap of a lexicon involve the repeated\ninteraction between at least two agents who must reach a consensus on how to\nname N objects using H words. Here we consider minimal models of two types of\nlearning algorithms: cross-situational learning, in which the individuals\ndetermine the meaning of a word by looking for something in common across all\nobserved uses of that word, and supervised operant conditioning learning, in\nwhich there is strong feedback between individuals about the intended meaning\nof the words. Despite the stark differences between these learning schemes, we\nshow that they yield the same communication accuracy in the realistic limits of\nlarge N and H, which coincides with the result of the classical occupancy\nproblem of randomly assigning N objects to H words.\n",
          "  Computers understand very little of the meaning of human language. This\nprofoundly limits our ability to give instructions to computers, the ability of\ncomputers to explain their actions to us, and the ability of computers to\nanalyse and process text. Vector space models (VSMs) of semantics are beginning\nto address these limits. This paper surveys the use of VSMs for semantic\nprocessing of text. We organize the literature on VSMs according to the\nstructure of the matrix in a VSM. There are currently three broad classes of\nVSMs, based on term-document, word-context, and pair-pattern matrices, yielding\nthree classes of applications. We survey a broad range of applications in these\nthree categories and we take a detailed look at a specific open source project\nin each category. Our goal in this survey is to show the breadth of\napplications of VSMs for semantics, to provide a new perspective on VSMs for\nthose who are already familiar with the area, and to provide pointers into the\nliterature for those who are less familiar with the field.\n",
          "  The recognition, involvement, and description of main actors influences the\nstory line of the whole text. This is of higher importance as the text per se\nrepresents a flow of words and expressions that once it is read it is lost. In\nthis respect, the understanding of a text and moreover on how the actor exactly\nbehaves is not only a major concern: as human beings try to store a given input\non short-term memory while associating diverse aspects and actors with\nincidents, the following approach represents a virtual architecture, where\ncollocations are concerned and taken as the associative completion of the\nactors' acting. Once that collocations are discovered, they become managed in\nseparated memory blocks broken down by the actors. As for human beings, the\nmemory blocks refer to associative mind-maps. We then present several priority\nfunctions to represent the actual temporal situation inside a mind-map to\nenable the user to reconstruct the recent events from the discovered temporal\nresults.\n",
          "  Recognizing analogies, synonyms, antonyms, and associations appear to be four\ndistinct tasks, requiring distinct NLP algorithms. In the past, the four tasks\nhave been treated independently, using a wide variety of algorithms. These four\nsemantic classes, however, are a tiny sample of the full range of semantic\nphenomena, and we cannot afford to create ad hoc algorithms for each semantic\nphenomenon; we need to seek a unified approach. We propose to subsume a broad\nrange of phenomena under analogies. To limit the scope of this paper, we\nrestrict our attention to the subsumption of synonyms, antonyms, and\nassociations. We introduce a supervised corpus-based machine learning algorithm\nfor classifying analogous word pairs, and we show that it can solve\nmultiple-choice SAT analogy questions, TOEFL synonym questions, ESL\nsynonym-antonym questions, and similar-associated-both questions from cognitive\npsychology.\n",
          "  Analogical reasoning depends fundamentally on the ability to learn and\ngeneralize about relations between objects. We develop an approach to\nrelational learning which, given a set of pairs of objects\n$\\mathbf{S}=\\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\\ldots,A^{(N)}:B ^{(N)}\\}$,\nmeasures how well other pairs A:B fit in with the set $\\mathbf{S}$. Our work\naddresses the following question: is the relation between objects A and B\nanalogous to those relations found in $\\mathbf{S}$? Such questions are\nparticularly relevant in information retrieval, where an investigator might\nwant to search for analogous pairs of objects that match the query set of\ninterest. There are many ways in which objects can be related, making the task\nof measuring analogies very challenging. Our approach combines a similarity\nmeasure on function spaces with Bayesian analysis to produce a ranking. It\nrequires data containing features of the objects of interest and a link matrix\nspecifying which relationships exist; no further attributes of such\nrelationships are necessary. We illustrate the potential of our method on text\nanalysis and information networks. An application on discovering functional\ninteractions between pairs of proteins is discussed in detail, where we show\nthat our approach can work in practice even if a small set of protein pairs is\nprovided.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "12_semantic_semantics_analogies",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "12_semantic_semantics_analogies"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          2.3878190517425537,
          2.3905704021453857,
          2.3653037548065186,
          2.4440219402313232,
          2.6232712268829346,
          2.4437828063964844,
          2.491691827774048,
          2.441112756729126,
          2.417137622833252,
          2.4197957515716553,
          2.2636542320251465,
          2.446916341781616,
          2.6019046306610107,
          2.4413063526153564
         ],
         "y": [
          4.304185390472412,
          4.3474507331848145,
          4.7518391609191895,
          4.270463943481445,
          4.487249374389648,
          4.259161949157715,
          4.394702911376953,
          4.5941162109375,
          4.3641815185546875,
          4.3300628662109375,
          4.428476333618164,
          4.272791385650635,
          4.190835475921631,
          4.384270668029785
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  Mappings to structured output spaces (strings, trees, partitions, etc.) are\ntypically learned using extensions of classification algorithms to simple\ngraphical structures (eg., linear chains) in which search and parameter\nestimation can be performed exactly. Unfortunately, in many complex problems,\nit is rare that exact search or parameter estimation is tractable. Instead of\nlearning exact models and searching via heuristic means, we embrace this\ndifficulty and treat the structured output problem in terms of approximate\nsearch. We present a framework for learning as search optimization, and two\nparameter updates with convergence theorems and bounds. Empirical evidence\nshows that our integrated approach to learning and decoding can outperform\nexact models at smaller computational cost.\n",
          "  We introduce a natural generalization of submodular set cover and exact\nactive learning with a finite hypothesis class (query learning). We call this\nnew problem interactive submodular set cover. Applications include advertising\nin social networks with hidden information. We give an approximation guarantee\nfor a novel greedy algorithm and give a hardness of approximation result which\nmatches up to constant factors. We also discuss negative results for simpler\napproaches and present encouraging early experimental results.\n",
          "  We consider the problem of estimating the conditional probability of a label\nin time $O(\\log n)$, where $n$ is the number of possible labels. We analyze a\nnatural reduction of this problem to a set of binary regression problems\norganized in a tree structure, proving a regret bound that scales with the\ndepth of the tree. Motivated by this analysis, we propose the first online\nalgorithm which provably constructs a logarithmic depth tree on the set of\nlabels to solve this problem. We test the algorithm empirically, showing that\nit works succesfully on a dataset with roughly $10^6$ labels.\n",
          "  For a classification problem described by the joint density $P(\\omega,x)$,\nmodels of $P(\\omega\\eq\\omega'|x,x')$ (the ``Bayesian similarity measure'') have\nbeen shown to be an optimal similarity measure for nearest neighbor\nclassification. This paper analyzes demonstrates several additional properties\nof that conditional distribution. The paper first shows that we can\nreconstruct, up to class labels, the class posterior distribution $P(\\omega|x)$\ngiven $P(\\omega\\eq\\omega'|x,x')$, gives a procedure for recovering the class\nlabels, and gives an asymptotically Bayes-optimal classification procedure. It\nalso shows, given such an optimal similarity measure, how to construct a\nclassifier that outperforms the nearest neighbor classifier and achieves\nBayes-optimal classification rates. The paper then analyzes Bayesian similarity\nin a framework where a classifier faces a number of related classification\ntasks (multitask learning) and illustrates that reconstruction of the class\nposterior distribution is not possible in general. Finally, the paper\nidentifies a distinct class of classification problems using\n$P(\\omega\\eq\\omega'|x,x')$ and shows that using $P(\\omega\\eq\\omega'|x,x')$ to\nsolve those problems is the Bayes optimal solution.\n",
          "  We consider multi-label prediction problems with large output spaces under\nthe assumption of output sparsity -- that the target (label) vectors have small\nsupport. We develop a general theory for a variant of the popular error\ncorrecting output code scheme, using ideas from compressed sensing for\nexploiting this sparsity. The method can be regarded as a simple reduction from\nmulti-label regression problems to binary regression problems. We show that the\nnumber of subproblems need only be logarithmic in the total number of possible\nlabels, making this approach radically more efficient than others. We also\nstate and prove robustness guarantees for this method in the form of regret\ntransform bounds (in general), and also provide a more detailed analysis for\nthe linear prediction setting.\n",
          "  We present an algorithm, called the Offset Tree, for learning to make\ndecisions in situations where the payoff of only one choice is observed, rather\nthan all choices. The algorithm reduces this setting to binary classification,\nallowing one to reuse of any existing, fully supervised binary classification\nalgorithm in this partial information setting. We show that the Offset Tree is\nan optimal reduction to binary classification. In particular, it has regret at\nmost $(k-1)$ times the regret of the binary classifier it uses (where $k$ is\nthe number of choices), and no reduction to binary classification can do\nbetter. This reduction is also computationally optimal, both at training and\ntest time, requiring just $O(\\log_2 k)$ work to train on an example or make a\nprediction.\n  Experiments with the Offset Tree show that it generally performs better than\nseveral alternative approaches.\n",
          "  We analyze the expected cost of a greedy active learning algorithm. Our\nanalysis extends previous work to a more general setting in which different\nqueries have different costs. Moreover, queries may have more than two possible\nresponses and the distribution over hypotheses may be non uniform. Specific\napplications include active learning with label costs, active learning for\nmulticlass and partial label queries, and batch mode active learning. We also\ndiscuss an approximate version of interest when there are very many queries.\n",
          "  Collecting large labeled data sets is a laborious and expensive task, whose\nscaling up requires division of the labeling workload between many teachers.\nWhen the number of classes is large, miscorrespondences between the labels\ngiven by the different teachers are likely to occur, which, in the extreme\ncase, may reach total inconsistency. In this paper we describe how globally\nconsistent labels can be obtained, despite the absence of teacher coordination,\nand discuss the possible efficiency of this process in terms of human labor. We\ndefine a notion of label efficiency, measuring the ratio between the number of\nglobally consistent labels obtained and the number of labels provided by\ndistributed teachers. We show that the efficiency depends critically on the\nratio alpha between the number of data instances seen by a single teacher, and\nthe number of classes. We suggest several algorithms for the distributed\nlabeling problem, and analyze their efficiency as a function of alpha. In\naddition, we provide an upper bound on label efficiency for the case of\ncompletely uncoordinated teachers, and show that efficiency approaches 0 as the\nratio between the number of labels each teacher provides and the number of\nclasses drops (i.e. alpha goes to 0).\n",
          "  We present a practical and statistically consistent scheme for actively\nlearning binary classifiers under general loss functions. Our algorithm uses\nimportance weighting to correct sampling bias, and by controlling the variance,\nwe are able to give rigorous label complexity bounds for the learning process.\nExperiments on passively labeled data show that this approach reduces the label\ncomplexity required to achieve good predictive performance on many learning\nproblems.\n",
          "  Supervised learning deals with the inference of a distribution over an output\nor label space $\\CY$ conditioned on points in an observation space $\\CX$, given\na training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of\napplications of interest, acquisition of large amounts of observations is easy,\nwhile the process of generating labels is time-consuming or costly. One way to\ndeal with this problem is {\\em active} learning, where points to be labelled\nare selected with the aim of creating a model with better performance than that\nof an model trained on an equal number of randomly sampled points. In this\npaper, we instead propose to deal with the labelling cost directly: The\nlearning goal is defined as the minimisation of a cost which is a function of\nthe expected model performance and the total cost of the labels used. This\nallows the development of general strategies and specific algorithms for (a)\noptimal stopping, where the expected cost dictates whether label acquisition\nshould continue (b) empirical evaluation, where the cost is used as a\nperformance metric for a given combination of inference, stopping and sampling\nmethods. Though the main focus of the paper is optimal stopping, we also aim to\nprovide the background for further developments and discussion in the related\nfield of active learning.\n",
          "  We define a novel, basic, unsupervised learning problem - learning the lowest\ndensity homogeneous hyperplane separator of an unknown probability\ndistribution. This task is relevant to several problems in machine learning,\nsuch as semi-supervised learning and clustering stability. We investigate the\nquestion of existence of a universally consistent algorithm for this problem.\nWe propose two natural learning paradigms and prove that, on input unlabeled\nrandom samples generated by any member of a rich family of distributions, they\nare guaranteed to converge to the optimal separator for that distribution. We\ncomplement this result by showing that no learning algorithm for our task can\nachieve uniform learning rates (that are independent of the data generating\ndistribution).\n",
          "  The major challenge in designing a discriminative learning algorithm for\npredicting structured data is to address the computational issues arising from\nthe exponential size of the output space. Existing algorithms make different\nassumptions to ensure efficient, polynomial time estimation of model\nparameters. For several combinatorial structures, including cycles, partially\nordered sets, permutations and other graph classes, these assumptions do not\nhold. In this thesis, we address the problem of designing learning algorithms\nfor predicting combinatorial structures by introducing two new assumptions: (i)\nThe first assumption is that a particular counting problem can be solved\nefficiently. The consequence is a generalisation of the classical ridge\nregression for structured prediction. (ii) The second assumption is that a\nparticular sampling problem can be solved efficiently. The consequence is a new\ntechnique for designing and analysing probabilistic structured prediction\nmodels. These results can be applied to solve several complex learning problems\nincluding but not limited to multi-label classification, multi-category\nhierarchical classification, and label ranking.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "13_labeling_supervised_labels",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "13_labeling_supervised_labels"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          2.547207832336426,
          2.8759427070617676,
          3.0705339908599854,
          2.577885627746582,
          3.333235025405884,
          2.5854127407073975,
          2.7865583896636963,
          2.8558690547943115,
          2.5205841064453125,
          2.894839286804199,
          2.5754244327545166,
          2.915879726409912,
          2.794947624206543
         ],
         "y": [
          6.223556995391846,
          6.253769874572754,
          6.578838348388672,
          6.195352554321289,
          6.638874053955078,
          6.460397720336914,
          6.223204612731934,
          6.207482814788818,
          6.294609546661377,
          6.2591938972473145,
          6.438361644744873,
          6.444196701049805,
          6.351486682891846
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  We show how text from news articles can be used to predict intraday price\nmovements of financial assets using support vector machines. Multiple kernel\nlearning is used to combine equity returns with text as predictive features to\nincrease classification performance and we develop an analytic center cutting\nplane method to solve the kernel learning problem efficiently. We observe that\nwhile the direction of returns is not predictable using either text or returns,\ntheir size is, with text features producing significantly better performance\nthan historical returns alone.\n",
          "  We propose a randomized algorithm for training Support vector machines(SVMs)\non large datasets. By using ideas from Random projections we show that the\ncombinatorial dimension of SVMs is $O({log} n)$ with high probability. This\nestimate of combinatorial dimension is used to derive an iterative algorithm,\ncalled RandSVM, which at each step calls an existing solver to train SVMs on a\nrandomly chosen subset of size $O({log} n)$. The algorithm has probabilistic\nguarantees and is capable of training SVMs with Kernels for both classification\nand regression problems. Experiments done on synthetic and real life data sets\ndemonstrate that the algorithm scales up existing SVM learners, without loss of\naccuracy.\n",
          "  Using a support vector machine requires to set two types of hyperparameters:\nthe soft margin parameter C and the parameters of the kernel. To perform this\nmodel selection task, the method of choice is cross-validation. Its\nleave-one-out variant is known to produce an estimator of the generalization\nerror which is almost unbiased. Its major drawback rests in its time\nrequirement. To overcome this difficulty, several upper bounds on the\nleave-one-out error of the pattern recognition SVM have been derived. Among\nthose bounds, the most popular one is probably the radius-margin bound. It\napplies to the hard margin pattern recognition SVM, and by extension to the\n2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\nas a direct extension of the 2-norm SVM to the multi-class case. For this\nmachine, a generalized radius-margin bound is then established.\n",
          "  We consider regularized support vector machines (SVMs) and show that they are\nprecisely equivalent to a new robust optimization formulation. We show that\nthis equivalence of robust optimization and regularization has implications for\nboth algorithms, and analysis. In terms of algorithms, the equivalence suggests\nmore general SVM-like algorithms for classification that explicitly build in\nprotection to noise, and at the same time control overfitting. On the analysis\nfront, the equivalence of robustness and regularization, provides a robust\noptimization interpretation for the success of regularized SVMs. We use the\nthis new robustness interpretation of SVMs to give a new proof of consistency\nof (kernelized) SVMs, thus establishing robustness as the reason regularized\nSVMs generalize well.\n",
          "  In the process of training Support Vector Machines (SVMs) by decomposition\nmethods, working set selection is an important technique, and some exciting\nschemes were employed into this field. To improve working set selection, we\npropose a new model for working set selection in sequential minimal\noptimization (SMO) decomposition methods. In this model, it selects B as\nworking set without reselection. Some properties are given by simple proof, and\nexperiments demonstrate that the proposed method is in general faster than\nexisting methods.\n",
          "  Support vector machines (SVMs) are an extremely successful type of\nclassification and regression algorithms. Building an SVM entails solving a\nconstrained convex quadratic programming problem, which is quadratic in the\nnumber of training samples. We introduce an efficient parallel implementation\nof an support vector regression solver, based on the Gaussian Belief\nPropagation algorithm (GaBP).\n  In this paper, we demonstrate that methods from the complex system domain\ncould be utilized for performing efficient distributed computation. We compare\nthe proposed algorithm to previously proposed distributed and single-node SVM\nsolvers. Our comparison shows that the proposed algorithm is just as accurate\nas these solvers, while being significantly faster, especially for large\ndatasets. We demonstrate scalability of the proposed algorithm to up to 1,024\ncomputing nodes and hundreds of thousands of data points using an IBM Blue Gene\nsupercomputer. As far as we know, our work is the largest parallel\nimplementation of belief propagation ever done, demonstrating the applicability\nof this algorithm for large scale distributed computing systems.\n",
          "  We present multiplicative updates for solving hard and soft margin support\nvector machines (SVM) with non-negative kernels. They follow as a natural\nextension of the updates for non-negative matrix factorization. No additional\nparam- eter setting, such as choosing learning, rate is required. Ex- periments\ndemonstrate rapid convergence to good classifiers. We analyze the rates of\nasymptotic convergence of the up- dates and establish tight bounds. We test the\nperformance on several datasets using various non-negative kernels and report\nequivalent generalization errors to that of a standard SVM.\n",
          "  The learning of appropriate distance metrics is a critical problem in image\nclassification and retrieval. In this work, we propose a boosting-based\ntechnique, termed \\BoostMetric, for learning a Mahalanobis distance metric. One\nof the primary difficulties in learning such a metric is to ensure that the\nMahalanobis matrix remains positive semidefinite. Semidefinite programming is\nsometimes used to enforce this constraint, but does not scale well.\n\\BoostMetric is instead based on a key observation that any positive\nsemidefinite matrix can be decomposed into a linear positive combination of\ntrace-one rank-one matrices. \\BoostMetric thus uses rank-one positive\nsemidefinite matrices as weak learners within an efficient and scalable\nboosting-based learning process. The resulting method is easy to implement,\ndoes not require tuning, and can accommodate various types of constraints.\nExperiments on various datasets show that the proposed algorithm compares\nfavorably to those state-of-the-art methods in terms of classification accuracy\nand running time.\n",
          "  We propose a method for support vector machine classification using\nindefinite kernels. Instead of directly minimizing or stabilizing a nonconvex\nloss function, our algorithm simultaneously computes support vectors and a\nproxy kernel matrix used in forming the loss. This can be interpreted as a\npenalized kernel learning problem where indefinite kernel matrices are treated\nas a noisy observations of a true Mercer kernel. Our formulation keeps the\nproblem convex and relatively large problems can be solved efficiently using\nthe projected gradient or analytic center cutting plane methods. We compare the\nperformance of our technique with other methods on several classic data sets.\n",
          "  We present a streaming model for large-scale classification (in the context\nof $\\ell_2$-SVM) by leveraging connections between learning and computational\ngeometry. The streaming model imposes the constraint that only a single pass\nover the data is allowed. The $\\ell_2$-SVM is known to have an equivalent\nformulation in terms of the minimum enclosing ball (MEB) problem, and an\nefficient algorithm based on the idea of \\emph{core sets} exists (Core Vector\nMachine, CVM). CVM learns a $(1+\\varepsilon)$-approximate MEB for a set of\npoints and yields an approximate solution to corresponding SVM instance.\nHowever CVM works in batch mode requiring multiple passes over the data. This\npaper presents a single-pass SVM which is based on the minimum enclosing ball\nof streaming data. We show that the MEB updates for the streaming case can be\neasily adapted to learn the SVM weight vector in a way similar to using online\nstochastic gradient updates. Our algorithm performs polylogarithmic computation\nat each example, and requires very small and constant storage. Experimental\nresults show that, even in such restrictive settings, we can learn efficiently\nin just one pass and get accuracies comparable to other state-of-the-art SVM\nsolvers (batch and online). We also give an analysis of the algorithm, and\ndiscuss some open issues and possible extensions.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "14_svms_svm_classification",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "14_svms_svm_classification"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          1.5913822650909424,
          1.5882078409194946,
          1.428457498550415,
          1.4574623107910156,
          1.4403181076049805,
          2.4522926807403564,
          1.4528945684432983,
          1.9914864301681519,
          1.414731740951538,
          1.7673765420913696,
          1.6584609746932983
         ],
         "y": [
          7.282434940338135,
          7.301316261291504,
          7.174675941467285,
          7.373119354248047,
          7.155207633972168,
          8.191788673400879,
          7.239480018615723,
          7.653759479522705,
          7.140311241149902,
          7.460227012634277,
          7.3972320556640625
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  This paper uses the notion of algorithmic stability to derive novel\ngeneralization bounds for several families of transductive regression\nalgorithms, both by using convexity and closed-form solutions. Our analysis\nhelps compare the stability of these algorithms. It also shows that a number of\nwidely used transductive regression algorithms are in fact unstable. Finally,\nit reports the results of experiments with local transductive regression\ndemonstrating the benefit of our stability bounds for model selection, for one\nof the algorithms, in particular for determining the radius of the local\nneighborhood used by the algorithm.\n",
          "  Lasso, or $\\ell^1$ regularized least squares, has been explored extensively\nfor its remarkable sparsity properties. It is shown in this paper that the\nsolution to Lasso, in addition to its sparsity, has robustness properties: it\nis the solution to a robust optimization problem. This has two important\nconsequences. First, robustness provides a connection of the regularizer to a\nphysical property, namely, protection from noise. This allows a principled\nselection of the regularizer, and in particular, generalizations of Lasso that\nalso yield convex optimization problems are obtained by considering different\nuncertainty sets.\n  Secondly, robustness can itself be used as an avenue to exploring different\nproperties of the solution. In particular, it is shown that robustness of the\nsolution explains why the solution is sparse. The analysis as well as the\nspecific results obtained differ from standard sparsity results, providing\ndifferent geometric intuition. Furthermore, it is shown that the robust\noptimization formulation is related to kernel density estimation, and based on\nthis approach, a proof that Lasso is consistent is given using robustness\ndirectly. Finally, a theorem saying that sparsity and algorithmic stability\ncontradict each other, and hence Lasso is not stable, is presented.\n",
          "  We consider the least-square linear regression problem with regularization by\nthe l1-norm, a problem usually referred to as the Lasso. In this paper, we\npresent a detailed asymptotic analysis of model consistency of the Lasso. For\nvarious decays of the regularization parameter, we compute asymptotic\nequivalents of the probability of correct model selection (i.e., variable\nselection). For a specific rate decay, we show that the Lasso selects all the\nvariables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection algorithm, referred to as the Bolasso, is\ncompared favorably to other linear regression methods on synthetic data and\ndatasets from the UCI machine learning repository.\n",
          "  We consider the problem of high-dimensional non-linear variable selection for\nsupervised learning. Our approach is based on performing linear selection among\nexponentially many appropriately defined positive definite kernels that\ncharacterize non-linear interactions between the original variables. To select\nefficiently from these many kernels, we use the natural hierarchical structure\nof the problem to extend the multiple kernel learning framework to kernels that\ncan be embedded in a directed acyclic graph; we show that it is then possible\nto perform kernel selection through a graph-adapted sparsity-inducing norm, in\npolynomial time in the number of selected kernels. Moreover, we study the\nconsistency of variable selection in high-dimensional settings, showing that\nunder certain assumptions, our regularization framework allows a number of\nirrelevant variables which is exponential in the number of observations. Our\nsimulations on synthetic datasets and datasets from the UCI repository show\nstate-of-the-art predictive performance for non-linear regression problems.\n",
          "  We analyze the convergence behaviour of a recently proposed algorithm for\nregularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is\nbased on a new interpretation of DAL as a proximal minimization algorithm. We\ntheoretically show under some conditions that DAL converges super-linearly in a\nnon-asymptotic and global sense. Due to a special modelling of sparse\nestimation problems in the context of machine learning, the assumptions we make\nare milder and more natural than those made in conventional analysis of\naugmented Lagrangian algorithms. In addition, the new interpretation enables us\nto generalize DAL to wide varieties of sparse estimation problems. We\nexperimentally confirm our analysis in a large scale $\\ell_1$-regularized\nlogistic regression problem and extensively compare the efficiency of DAL\nalgorithm to previously proposed algorithms on both synthetic and benchmark\ndatasets.\n",
          "  A key issue in statistics and machine learning is to automatically select the\n\"right\" model complexity, e.g., the number of neighbors to be averaged over in\nk nearest neighbor (kNN) regression or the polynomial degree in regression with\npolynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) -\nfor model selection in regression and classification. It is based on the loss\nrank, which counts how many other (fictitious) data would be fitted better.\nLoRP selects the model that has minimal loss rank. Unlike most penalized\nmaximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the\nregression functions and the loss function. It works without a stochastic noise\nmodel, and is directly applicable to any non-parametric regressor, like kNN.\n",
          "  Most of the non-asymptotic theoretical work in regression is carried out for\nthe square loss, where estimators can be obtained through closed-form\nexpressions. In this paper, we use and extend tools from the convex\noptimization literature, namely self-concordant functions, to provide simple\nextensions of theoretical results for the square loss to the logistic loss. We\napply the extension techniques to logistic regression with regularization by\nthe $\\ell_2$-norm and regularization by the $\\ell_1$-norm, showing that new\nresults for binary classification through logistic regression can be easily\nderived from corresponding results for least-squares regression.\n",
          "  We consider the least-square linear regression problem with regularization by\nthe $\\ell^1$-norm, a problem usually referred to as the Lasso. In this paper,\nwe first present a detailed asymptotic analysis of model consistency of the\nLasso in low-dimensional settings. For various decays of the regularization\nparameter, we compute asymptotic equivalents of the probability of correct\nmodel selection. For a specific rate decay, we show that the Lasso selects all\nthe variables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection procedure, referred to as the Bolasso, is\nextended to high-dimensional settings by a provably consistent two-step\nprocedure.\n",
          "  We consider the least-square regression problem with regularization by a\nblock 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\nthan one. This problem, referred to as the group Lasso, extends the usual\nregularization by the 1-norm where all spaces have dimension one, where it is\ncommonly referred to as the Lasso. In this paper, we study the asymptotic model\nconsistency of the group Lasso. We derive necessary and sufficient conditions\nfor the consistency of group Lasso under practical assumptions, such as model\nmisspecification. When the linear predictors and Euclidean norms are replaced\nby functions and reproducing kernel Hilbert norms, the problem is usually\nreferred to as multiple kernel learning and is commonly used for learning from\nheterogeneous data sources and for non linear variable selection. Using tools\nfrom functional analysis, and in particular covariance operators, we extend the\nconsistency results to this infinite dimensional case and also propose an\nadaptive scheme to obtain a consistent model estimate, even when the necessary\ncondition required for the non adaptive scheme is not satisfied.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "15_lasso_regularization_regularized",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "15_lasso_regularization_regularized"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          1.2296472787857056,
          1.1759048700332642,
          1.1339126825332642,
          1.3775194883346558,
          1.0496686697006226,
          1.1307944059371948,
          1.1373525857925415,
          1.1171472072601318,
          1.2093589305877686,
          1.173478603363037
         ],
         "y": [
          8.126556396484375,
          8.158034324645996,
          8.165006637573242,
          7.971681594848633,
          8.084746360778809,
          8.067551612854004,
          8.135333061218262,
          8.164583206176758,
          8.112957000732422,
          8.10960578918457
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  Recent spectral clustering methods are a propular and powerful technique for\ndata clustering. These methods need to solve the eigenproblem whose\ncomputational complexity is $O(n^3)$, where $n$ is the number of data samples.\nIn this paper, a non-eigenproblem based clustering method is proposed to deal\nwith the clustering problem. Its performance is comparable to the spectral\nclustering algorithms but it is more efficient with computational complexity\n$O(n^2)$. We show that with a transitive distance and an observed property,\ncalled K-means duality, our algorithm can be used to handle data sets with\ncomplex cluster shapes, multi-scale clusters, and noise. Moreover, no\nparameters except the number of clusters need to be set in our algorithm.\n",
          "  We describe the Median K-Flats (MKF) algorithm, a simple online method for\nhybrid linear modeling, i.e., for approximating data by a mixture of flats.\nThis algorithm simultaneously partitions the data into clusters while finding\ntheir corresponding best approximating l1 d-flats, so that the cumulative l1\nerror is minimized. The current implementation restricts d-flats to be\nd-dimensional linear subspaces. It requires a negligible amount of storage, and\nits complexity, when modeling data consisting of N points in D-dimensional\nEuclidean space with K d-dimensional linear subspaces, is of order O(n K d D+n\nd^2 D), where n is the number of iterations required for convergence\n(empirically on the order of 10^4). Since it is an online algorithm, data can\nbe supplied to it incrementally and it can incrementally produce the\ncorresponding output. The performance of the algorithm is carefully evaluated\nusing synthetic and real data.\n",
          "  We present a new algorithm for clustering points in R^n. The key property of\nthe algorithm is that it is affine-invariant, i.e., it produces the same\npartition for any affine transformation of the input. It has strong guarantees\nwhen the input is drawn from a mixture model. For a mixture of two arbitrary\nGaussians, the algorithm correctly classifies the sample assuming only that the\ntwo components are separable by a hyperplane, i.e., there exists a halfspace\nthat contains most of one Gaussian and almost none of the other in probability\nmass. This is nearly the best possible, improving known results substantially.\nFor k > 2 components, the algorithm requires only that there be some\n(k-1)-dimensional subspace in which the emoverlap in every direction is small.\nHere we define overlap to be the ratio of the following two quantities: 1) the\naverage squared distance between a point and the mean of its component, and 2)\nthe average squared distance between a point and the mean of the mixture. The\nmain result may also be stated in the language of linear discriminant analysis:\nif the standard Fisher discriminant is small enough, labels are not needed to\nestimate the optimal subspace for projection. Our main tools are isotropic\ntransformation, spectral projection and a simple reweighting technique. We call\nthis combination isotropic PCA.\n",
          "  Recently a new clustering algorithm called 'affinity propagation' (AP) has\nbeen proposed, which efficiently clustered sparsely related data by passing\nmessages between data points. However, we want to cluster large scale data\nwhere the similarities are not sparse in many cases. This paper presents two\nvariants of AP for grouping large scale data with a dense similarity matrix.\nThe local approach is partition affinity propagation (PAP) and the global\nmethod is landmark affinity propagation (LAP). PAP passes messages in the\nsubsets of data first and then merges them as the number of initial step of\niterations; it can effectively reduce the number of iterations of clustering.\nLAP passes messages between the landmark data points first and then clusters\nnon-landmark data points; it is a large global approximation method to speed up\nclustering. Experiments are conducted on many datasets, such as random data\npoints, manifold subspaces, images of faces and Chinese calligraphy, and the\nresults demonstrate that the two approaches are feasible and practicable.\n",
          "  The k-means algorithm is a well-known method for partitioning n points that\nlie in the d-dimensional space into k clusters. Its main features are\nsimplicity and speed in practice. Theoretically, however, the best known upper\nbound on its running time (i.e. O(n^{kd})) can be exponential in the number of\npoints. Recently, Arthur and Vassilvitskii [3] showed a super-polynomial\nworst-case analysis, improving the best known lower bound from \\Omega(n) to\n2^{\\Omega(\\sqrt{n})} with a construction in d=\\Omega(\\sqrt{n}) dimensions. In\n[3] they also conjectured the existence of superpolynomial lower bounds for any\nd >= 2.\n  Our contribution is twofold: we prove this conjecture and we improve the\nlower bound, by presenting a simple construction in the plane that leads to the\nexponential lower bound 2^{\\Omega(n)}.\n",
          "  In this paper we present a method for learning the parameters of a mixture of\n$k$ identical spherical Gaussians in $n$-dimensional space with an arbitrarily\nsmall separation between the components. Our algorithm is polynomial in all\nparameters other than $k$. The algorithm is based on an appropriate grid search\nover the space of parameters. The theoretical analysis of the algorithm hinges\non a reduction of the problem to 1 dimension and showing that two 1-dimensional\nmixtures whose densities are close in the $L^2$ norm must have similar means\nand mixing coefficients. To produce such a lower bound for the $L^2$ norm in\nterms of the distances between the corresponding means, we analyze the behavior\nof the Fourier transform of a mixture of Gaussians in 1 dimension around the\norigin, which turns out to be closely related to the properties of the\nVandermonde matrix obtained from the component means. Analysis of this matrix\ntogether with basic function approximation results allows us to provide a lower\nbound for the norm of the mixture in the Fourier domain.\n  In recent years much research has been aimed at understanding the\ncomputational aspects of learning parameters of Gaussians mixture distributions\nin high dimension. To the best of our knowledge all existing work on learning\nparameters of Gaussian mixtures assumes minimum separation between components\nof the mixture which is an increasing function of either the dimension of the\nspace $n$ or the number of components $k$. In our paper we prove the first\nresult showing that parameters of a $n$-dimensional Gaussian mixture model with\narbitrarily small component separation can be learned in time polynomial in\n$n$.\n",
          "  In recent years, spectral clustering has become one of the most popular\nmodern clustering algorithms. It is simple to implement, can be solved\nefficiently by standard linear algebra software, and very often outperforms\ntraditional clustering algorithms such as the k-means algorithm. On the first\nglance spectral clustering appears slightly mysterious, and it is not obvious\nto see why it works at all and what it really does. The goal of this tutorial\nis to give some intuition on those questions. We describe different graph\nLaplacians and their basic properties, present the most common spectral\nclustering algorithms, and derive those algorithms from scratch by several\ndifferent approaches. Advantages and disadvantages of the different spectral\nclustering algorithms are discussed.\n",
          "  In the past few years powerful generalizations to the Euclidean k-means\nproblem have been made, such as Bregman clustering [7], co-clustering (i.e.,\nsimultaneous clustering of rows and columns of an input matrix) [9,18], and\ntensor clustering [8,34]. Like k-means, these more general problems also suffer\nfrom the NP-hardness of the associated optimization. Researchers have developed\napproximation algorithms of varying degrees of sophistication for k-means,\nk-medians, and more recently also for Bregman clustering [2]. However, there\nseem to be no approximation algorithms for Bregman co- and tensor clustering.\nIn this paper we derive the first (to our knowledge) guaranteed methods for\nthese increasingly important clustering settings. Going beyond Bregman\ndivergences, we also prove an approximation factor for tensor clustering with\narbitrary separable metrics. Through extensive experiments we evaluate the\ncharacteristics of our method, and show that it also has practical impact.\n",
          "  One of the most popular algorithms for clustering in Euclidean space is the\n$k$-means algorithm; $k$-means is difficult to analyze mathematically, and few\ntheoretical guarantees are known about it, particularly when the data is {\\em\nwell-clustered}. In this paper, we attempt to fill this gap in the literature\nby analyzing the behavior of $k$-means on well-clustered data. In particular,\nwe study the case when each cluster is distributed as a different Gaussian --\nor, in other words, when the input comes from a mixture of Gaussians.\n  We analyze three aspects of the $k$-means algorithm under this assumption.\nFirst, we show that when the input comes from a mixture of two spherical\nGaussians, a variant of the 2-means algorithm successfully isolates the\nsubspace containing the means of the mixture components. Second, we show an\nexact expression for the convergence of our variant of the 2-means algorithm,\nwhen the input is a very large number of samples from a mixture of spherical\nGaussians. Our analysis does not require any lower bound on the separation\nbetween the mixture components.\n  Finally, we study the sample requirement of $k$-means; for a mixture of 2\nspherical Gaussians, we show an upper bound on the number of samples required\nby a variant of 2-means to get close to the true solution. The sample\nrequirement grows with increasing dimensionality of the data, and decreasing\nseparation between the means of the Gaussians. To match our upper bound, we\nshow an information-theoretic lower bound on any algorithm that learns mixtures\nof two spherical Gaussians; our lower bound indicates that in the case when the\noverlap between the probability masses of the two distributions is small, the\nsample requirement of $k$-means is {\\em near-optimal}.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "16_clustering_cluster_clusters",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "16_clustering_cluster_clusters"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          3.3601999282836914,
          3.342703342437744,
          3.413761615753174,
          3.4788997173309326,
          3.3762028217315674,
          3.4013068675994873,
          3.4362668991088867,
          3.38775372505188,
          3.3594202995300293,
          3.3951683044433594
         ],
         "y": [
          5.16050910949707,
          5.337586879730225,
          5.407861709594727,
          5.243906497955322,
          5.4354143142700195,
          5.506719589233398,
          5.315130710601807,
          5.3775224685668945,
          5.358977794647217,
          5.349292278289795
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  Among all the partition based clustering algorithms K-means is the most\npopular and well known method. It generally shows impressive results even in\nconsiderably large data sets. The computational complexity of K-means does not\nsuffer from the size of the data set. The main disadvantage faced in performing\nthis clustering is that the selection of initial means. If the user does not\nhave adequate knowledge about the data set, it may lead to erroneous results.\nThe algorithm Automatic Initialization of Means (AIM), which is an extension to\nK-means, has been proposed to overcome the problem of initial mean generation.\nIn this paper an attempt has been made to compare the performance of the\nalgorithms through implementation\n",
          "  In data analysis new forms of complex data have to be considered like for\nexample (symbolic data, functional data, web data, trees, SQL query and\nmultimedia data, ...). In this context classical data analysis for knowledge\ndiscovery based on calculating the center of gravity can not be used because\ninput are not $\\mathbb{R}^p$ vectors. In this paper, we present an application\non real world symbolic data using the self-organizing map. To this end, we\npropose an extension of the self-organizing map that can handle symbolic data.\n",
          "  In many real world applications, data cannot be accurately represented by\nvectors. In those situations, one possible solution is to rely on dissimilarity\nmeasures that enable sensible comparison between observations. Kohonen's\nSelf-Organizing Map (SOM) has been adapted to data described only through their\ndissimilarity matrix. This algorithm provides both non linear projection and\nclustering of non vector data. Unfortunately, the algorithm suffers from a high\ncost that makes it quite difficult to use with voluminous data sets. In this\npaper, we propose a new algorithm that provides an important reduction of the\ntheoretical cost of the dissimilarity SOM without changing its outcome (the\nresults are exactly the same as the ones obtained with the original algorithm).\nMoreover, we introduce implementation methods that result in very short running\ntimes. Improvements deduced from the theoretical cost model are validated on\nsimulated and real world data (a word list clustering problem). We also\ndemonstrate that the proposed implementation methods reduce by a factor up to 3\nthe running time of the fast algorithm over a standard implementation.\n",
          "  The ability to monitor the progress of students academic performance is a\ncritical issue to the academic community of higher learning. A system for\nanalyzing students results based on cluster analysis and uses standard\nstatistical algorithms to arrange their scores data according to the level of\ntheir performance is described. In this paper, we also implemented k mean\nclustering algorithm for analyzing students result data. The model was combined\nwith the deterministic model to analyze the students results of a private\nInstitution in Nigeria which is a good benchmark to monitor the progression of\nacademic performance of students in higher Institution for the purpose of\nmaking an effective decision by the academic planners.\n",
          "  In this paper we have investigated the performance of PSO Particle Swarm\nOptimization based clustering on few real world data sets and one artificial\ndata set. The performances are measured by two metric namely quantization error\nand inter-cluster distance. The K means clustering algorithm is first\nimplemented for all data sets, the results of which form the basis of\ncomparison of PSO based approaches. We have explored different variants of PSO\nsuch as gbest, lbest ring, lbest vonneumann and Hybrid PSO for comparison\npurposes. The results reveal that PSO based clustering algorithms perform\nbetter compared to K means in all data sets.\n",
          "  Many data analysis methods cannot be applied to data that are not represented\nby a fixed number of real values, whereas most of real world observations are\nnot readily available in such a format. Vector based data analysis methods have\ntherefore to be adapted in order to be used with non standard complex data. A\nflexible and general solution for this adaptation is to use a (dis)similarity\nmeasure. Indeed, thanks to expert knowledge on the studied data, it is\ngenerally possible to define a measure that can be used to make pairwise\ncomparison between observations. General data analysis methods are then\nobtained by adapting existing methods to (dis)similarity matrices. In this\narticle, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to\n(dis)similarity data. The proposed algorithm is an adapted version of the\nvector based batch SOM. The method is validated on real world data: we provide\nan analysis of the usage patterns of the web site of the Institut National de\nRecherche en Informatique et Automatique, constructed thanks to web log mining\nmethod.\n",
          "  This paper introduces an evaluation methodologies for the e-learners'\nbehaviour that will be a feedback to the decision makers in e-learning system.\nLearner's profile plays a crucial role in the evaluation process to improve the\ne-learning process performance. The work focuses on the clustering of the\ne-learners based on their behaviour into specific categories that represent the\nlearner's profiles. The learners' classes named as regular, workers, casual,\nbad, and absent. The work may answer the question of how to return bad students\nto be regular ones. The work presented the use of different fuzzy clustering\ntechniques as fuzzy c-means and kernelized fuzzy c-means to find the learners'\ncategories and predict their profiles. The paper presents the main phases as\ndata description, preparation, features selection, and the experiments design\nusing different fuzzy clustering models. Analysis of the obtained results and\ncomparison with the real world behavior of those learners proved that there is\na match with percentage of 78%. Fuzzy clustering reflects the learners'\nbehavior more than crisp clustering. Comparison between FCM and KFCM proved\nthat the KFCM is much better than FCM in predicting the learners' behaviour.\n",
          "  Median clustering extends popular neural data analysis methods such as the\nself-organizing map or neural gas to general data structures given by a\ndissimilarity matrix only. This offers flexible and robust global data\ninspection methods which are particularly suited for a variety of data as\noccurs in biomedical domains. In this chapter, we give an overview about median\nclustering and its properties and extensions, with a particular focus on\nefficient implementations adapted to large scale data analysis.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "17_clustering_cluster_algorithms",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "17_clustering_cluster_algorithms"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          3.208972215652466,
          3.345174789428711,
          3.3541386127471924,
          2.9966623783111572,
          3.2490718364715576,
          3.3913848400115967,
          2.7937328815460205,
          3.3355445861816406,
          3.2093350887298584
         ],
         "y": [
          5.053959369659424,
          4.812752723693848,
          4.9049577713012695,
          4.628118515014648,
          5.041277885437012,
          4.763835430145264,
          4.514432907104492,
          4.877965450286865,
          4.824662208557129
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  The method of defensive forecasting is applied to the problem of prediction\nwith expert advice for binary outcomes. It turns out that defensive forecasting\nis not only competitive with the Aggregating Algorithm but also handles the\ncase of \"second-guessing\" experts, whose advice depends on the learner's\nprediction; this paper assumes that the dependence on the learner's prediction\nis continuous.\n",
          "  We show how models for prediction with expert advice can be defined concisely\nand clearly using hidden Markov models (HMMs); standard HMM algorithms can then\nbe used to efficiently calculate, among other things, how the expert\npredictions should be weighted according to the model. We cast many existing\nmodels as HMMs and recover the best known running times in each case. We also\ndescribe two new models: the switch distribution, which was recently developed\nto improve Bayesian/Minimum Description Length model selection, and a new\ngeneralisation of the fixed share algorithm based on run-length coding. We give\nloss bounds for all models and shed new light on their relationships.\n",
          "  Using the game-theoretic framework for probability, Vovk and Shafer. have\nshown that it is always possible, using randomization, to make sequential\nprobability forecasts that pass any countable set of well-behaved statistical\ntests. This result generalizes work by other authors, who consider only tests\nof calbration.\n  We complement this result with a lower bound. We show that Vovk and Shafer's\nresult is valid only when the forecasts are computed with unrestrictedly\nincreasing degree of accuracy.\n  When some level of discreteness is fixed, we present a game-theoretic\ngeneralization of Oakes' example for randomized forecasting that is a test\nfailing any given method of deferministic forecasting; originally, this example\nwas presented for deterministic calibration.\n",
          "  The games of prediction with expert advice are considered in this paper. We\npresent some modification of Kalai and Vempala algorithm of following the\nperturbed leader for the case of unrestrictedly large one-step gains. We show\nthat in general case the cumulative gain of any probabilistic prediction\nalgorithm can be much worse than the gain of some expert of the pool.\nNevertheless, we give the lower bound for this cumulative gain in general case\nand construct a universal algorithm which has the optimal performance; we also\nprove that in case when one-step gains of experts of the pool have ``limited\ndeviations'' the performance of our algorithm is close to the performance of\nthe best expert.\n",
          "  We show that the Brier game of prediction is mixable and find the optimal\nlearning rate and substitution function for it. The resulting prediction\nalgorithm is applied to predict results of football and tennis matches. The\ntheoretical performance guarantee turns out to be rather tight on these data\nsets, especially in the case of the more extensive tennis data.\n",
          "  We introduce a new protocol for prediction with expert advice in which each\nexpert evaluates the learner's and his own performance using a loss function\nthat may change over time and may be different from the loss functions used by\nthe other experts. The learner's goal is to perform better or not much worse\nthan each expert, as evaluated by that expert, for all experts simultaneously.\nIf the loss functions used by the experts are all proper scoring rules and all\nmixable, we show that the defensive forecasting algorithm enjoys the same\nperformance guarantee as that attainable by the Aggregating Algorithm in the\nstandard setting and known to be optimal. This result is also applied to the\ncase of \"specialist\" (or \"sleeping\") experts. In this case, the defensive\nforecasting algorithm reduces to a simple modification of the Aggregating\nAlgorithm.\n",
          "  Defensive forecasting is a method of transforming laws of probability (stated\nin game-theoretic terms as strategies for Sceptic) into forecasting algorithms.\nThere are two known varieties of defensive forecasting: \"continuous\", in which\nSceptic's moves are assumed to depend on the forecasts in a (semi)continuous\nmanner and which produces deterministic forecasts, and \"randomized\", in which\nthe dependence of Sceptic's moves on the forecasts is arbitrary and\nForecaster's moves are allowed to be randomized. This note shows that the\nrandomized variety can be obtained from the continuous variety by smearing\nSceptic's moves to make them continuous.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "18_forecasting_prediction_forecaster",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "18_forecasting_prediction_forecaster"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          6.606009006500244,
          6.6344895362854,
          6.535918712615967,
          6.690244674682617,
          6.6807169914245605,
          6.62807559967041,
          6.585484504699707,
          6.622990608215332
         ],
         "y": [
          6.239995002746582,
          6.3069071769714355,
          6.218476295471191,
          6.30869197845459,
          6.294018268585205,
          6.273230075836182,
          6.220784664154053,
          6.266014575958252
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  In recent years, the spectral analysis of appropriately defined kernel\nmatrices has emerged as a principled way to extract the low-dimensional\nstructure often prevalent in high-dimensional data. Here we provide an\nintroduction to spectral methods for linear and nonlinear dimension reduction,\nemphasizing ways to overcome the computational limitations currently faced by\npractitioners with massive datasets. In particular, a data subsampling or\nlandmark selection process is often employed to construct a kernel based on\npartial information, followed by an approximate spectral analysis termed the\nNystrom extension. We provide a quantitative framework to analyse this\nprocedure, and use it to demonstrate algorithmic performance bounds on a range\nof practical approaches designed to optimize the landmark selection process. We\ncompare the practical implications of these bounds by way of real-world\nexamples drawn from the field of computer vision, whereby low-dimensional\nmanifold structure is shown to emerge from high-dimensional video data streams.\n",
          "  We present a general framework of semi-supervised dimensionality reduction\nfor manifold learning which naturally generalizes existing supervised and\nunsupervised learning frameworks which apply the spectral decomposition.\nAlgorithms derived under our framework are able to employ both labeled and\nunlabeled examples and are able to handle complex problems where data form\nseparate clusters of manifolds. Our framework offers simple views, explains\nrelationships among existing frameworks and provides further extensions which\ncan improve existing algorithms. Furthermore, a new semi-supervised\nkernelization framework called ``KPCA trick'' is proposed to handle non-linear\nproblems.\n",
          "  Manifold learning is a hot research topic in the field of computer science\nand has many applications in the real world. A main drawback of manifold\nlearning methods is, however, that there is no explicit mappings from the input\ndata manifold to the output embedding. This prohibits the application of\nmanifold learning methods in many practical problems such as classification and\ntarget detection. Previously, in order to provide explicit mappings for\nmanifold learning methods, many methods have been proposed to get an\napproximate explicit representation mapping with the assumption that there\nexists a linear projection between the high-dimensional data samples and their\nlow-dimensional embedding. However, this linearity assumption may be too\nrestrictive. In this paper, an explicit nonlinear mapping is proposed for\nmanifold learning, based on the assumption that there exists a polynomial\nmapping between the high-dimensional data samples and their low-dimensional\nrepresentations. As far as we know, this is the first time that an explicit\nnonlinear mapping for manifold learning is given. In particular, we apply this\nto the method of Locally Linear Embedding (LLE) and derive an explicit\nnonlinear manifold learning algorithm, named Neighborhood Preserving Polynomial\nEmbedding (NPPE). Experimental results on both synthetic and real-world data\nshow that the proposed mapping is much more effective in preserving the local\nneighborhood information and the nonlinear geometry of the high-dimensional\ndata samples than previous work.\n",
          "  Isometric feature mapping (Isomap) is a promising manifold learning method.\nHowever, Isomap fails to work on data which distribute on clusters in a single\nmanifold or manifolds. Many works have been done on extending Isomap to\nmulti-manifolds learning. In this paper, we first proposed a new\nmulti-manifolds learning algorithm (M-Isomap) with help of a general procedure.\nThe new algorithm preserves intra-manifold geodesics and multiple\ninter-manifolds edges precisely. Compared with previous methods, this algorithm\ncan isometrically learn data distributed on several manifolds. Secondly, the\noriginal multi-cluster manifold learning algorithm first proposed in\n\\cite{DCIsomap} and called D-C Isomap has been revised so that the revised D-C\nIsomap can learn multi-manifolds data. Finally, the features and effectiveness\nof the proposed multi-manifolds learning algorithms are demonstrated and\ncompared through experiments.\n",
          "  The emergence of low-cost sensor architectures for diverse modalities has\nmade it possible to deploy sensor arrays that capture a single event from a\nlarge number of vantage points and using multiple modalities. In many\nscenarios, these sensors acquire very high-dimensional data such as audio\nsignals, images, and video. To cope with such high-dimensional data, we\ntypically rely on low-dimensional models. Manifold models provide a\nparticularly powerful model that captures the structure of high-dimensional\ndata when it is governed by a low-dimensional set of parameters. However, these\nmodels do not typically take into account dependencies among multiple sensors.\nWe thus propose a new joint manifold framework for data ensembles that exploits\nsuch dependencies. We show that simple algorithms can exploit the joint\nmanifold structure to improve their performance on standard signal processing\napplications. Additionally, recent results concerning dimensionality reduction\nfor manifolds enable us to formulate a network-scalable data compression scheme\nthat uses random projections of the sensed data. This scheme efficiently fuses\nthe data from all sensors through the addition of such projections, regardless\nof the data modalities and dimensions.\n",
          "  The large number of spectral variables in most data sets encountered in\nspectral chemometrics often renders the prediction of a dependent variable\nuneasy. The number of variables hopefully can be reduced, by using either\nprojection techniques or selection methods; the latter allow for the\ninterpretation of the selected variables. Since the optimal approach of testing\nall possible subsets of variables with the prediction model is intractable, an\nincremental selection approach using a nonparametric statistics is a good\noption, as it avoids the computationally intensive use of the model itself. It\nhas two drawbacks however: the number of groups of variables to test is still\nhuge, and colinearities can make the results unstable. To overcome these\nlimitations, this paper presents a method to select groups of spectral\nvariables. It consists in a forward-backward procedure applied to the\ncoefficients of a B-Spline representation of the spectra. The criterion used in\nthe forward-backward procedure is the mutual information, allowing to find\nnonlinear dependencies between variables, on the contrary of the generally used\ncorrelation. The spline representation is used to get interpretability of the\nresults, as groups of consecutive spectral variables will be selected. The\nexperiments conducted on NIR spectra from fescue grass and diesel fuels show\nthat the method provides clearly identified groups of selected variables,\nmaking interpretation easy, while keeping a low computational load. The\nprediction performances obtained using the selected coefficients are higher\nthan those obtained by the same method applied directly to the original\nvariables and similar to those obtained using traditional models, although\nusing significantly less spectral variables.\n",
          "  The recent increase in dimensionality of data has thrown a great challenge to\nthe existing dimensionality reduction methods in terms of their effectiveness.\nDimensionality reduction has emerged as one of the significant preprocessing\nsteps in machine learning applications and has been effective in removing\ninappropriate data, increasing learning accuracy, and improving\ncomprehensibility. Feature redundancy exercises great influence on the\nperformance of classification process. Towards the better classification\nperformance, this paper addresses the usefulness of truncating the highly\ncorrelated and redundant attributes. Here, an effort has been made to verify\nthe utility of dimensionality reduction by applying LVQ (Learning Vector\nQuantization) method on two Benchmark datasets of 'Pima Indian Diabetic\npatients' and 'Lung cancer patients'.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "19_manifolds_manifold_supervised",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "19_manifolds_manifold_supervised"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          1.8311309814453125,
          1.8050726652145386,
          1.9119001626968384,
          1.8744755983352661,
          1.9165890216827393,
          0.7607212662696838,
          1.5017955303192139,
          1.657383680343628
         ],
         "y": [
          8.486879348754883,
          8.473880767822266,
          8.425105094909668,
          8.382349014282227,
          8.53226375579834,
          7.835449695587158,
          8.609705924987793,
          8.392232894897461
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "  Covariances from categorical variables are defined using a regular simplex\nexpression for categories. The method follows the variance definition by Gini,\nand it gives the covariance as a solution of simultaneous equations. The\ncalculated results give reasonable values for test data. A method of principal\ncomponent analysis (RS-PCA) is also proposed using regular simplex expressions,\nwhich allows easy interpretation of the principal components. The proposed\nmethods apply to variable selection problem of categorical data USCensus1990\ndata. The proposed methods give appropriate criterion for the variable\nselection problem of categorical\n",
          "  In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology.\n",
          "  We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero.\n",
          "  Estimating intrinsic dimensionality of data is a classic problem in pattern\nrecognition and statistics. Principal Component Analysis (PCA) is a powerful\ntool in discovering dimensionality of data sets with a linear structure; it,\nhowever, becomes ineffective when data have a nonlinear structure. In this\npaper, we propose a new PCA-based method to estimate intrinsic dimension of\ndata with nonlinear structures. Our method works by first finding a minimal\ncover of the data set, then performing PCA locally on each subset in the cover\nand finally giving the estimation result by checking up the data variance on\nall small neighborhood regions. The proposed method utilizes the whole data set\nto estimate its intrinsic dimension and is convenient for incremental learning.\nIn addition, our new PCA procedure can filter out noise in data and converge to\na stable estimation with the neighborhood region size increasing. Experiments\non synthetic and real world data sets show effectiveness of the proposed\nmethod.\n",
          "  Principal component analysis (PCA) is a widely used technique for data\nanalysis and dimension reduction with numerous applications in science and\nengineering. However, the standard PCA suffers from the fact that the principal\ncomponents (PCs) are usually linear combinations of all the original variables,\nand it is thus often difficult to interpret the PCs. To alleviate this\ndrawback, various sparse PCA approaches were proposed in literature [15, 6, 17,\n28, 8, 25, 18, 7, 16]. Despite success in achieving sparsity, some important\nproperties enjoyed by the standard PCA are lost in these methods such as\nuncorrelation of PCs and orthogonality of loading vectors. Also, the total\nexplained variance that they attempt to maximize can be too optimistic. In this\npaper we propose a new formulation for sparse PCA, aiming at finding sparse and\nnearly uncorrelated PCs with orthogonal loading vectors while explaining as\nmuch of the total variance as possible. We also develop a novel augmented\nLagrangian method for solving a class of nonsmooth constrained optimization\nproblems, which is well suited for our formulation of sparse PCA. We show that\nit converges to a feasible point, and moreover under some regularity\nassumptions, it converges to a stationary point. Additionally, we propose two\nnonmonotone gradient methods for solving the augmented Lagrangian subproblems,\nand establish their global and local convergence. Finally, we compare our\nsparse PCA approach with several existing methods on synthetic, random, and\nreal data, respectively. The computational results demonstrate that the sparse\nPCs produced by our approach substantially outperform those by other methods in\nterms of total explained variance, correlation of PCs, and orthogonality of\nloading vectors.\n",
          null
         ],
         "marker": {
          "opacity": 0.5,
          "size": 5
         },
         "mode": "markers+text",
         "name": "20_pca_dimensionality_sparse",
         "text": [
          "",
          "",
          "",
          "",
          "",
          "20_pca_dimensionality_sparse"
         ],
         "textfont": {
          "size": 12
         },
         "type": "scattergl",
         "x": [
          1.613470196723938,
          1.678890347480774,
          1.6448094844818115,
          1.6604959964752197,
          1.686293601989746,
          1.6567919254302979
         ],
         "y": [
          9.28344440460205,
          9.398080825805664,
          9.192479133605957,
          8.986173629760742,
          9.404617309570312,
          9.252958297729492
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": 0.24705863893032073,
          "y": 7.1281031489372255,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": 4.857040838897229,
          "xshift": 10,
          "y": 11.089906930923462
         }
        ],
        "height": 750,
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": 4.857040838897229,
          "x1": 4.857040838897229,
          "y0": 3.1662993669509887,
          "y1": 11.089906930923462
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": 0.24705863893032073,
          "x1": 9.467023038864136,
          "y0": 7.1281031489372255,
          "y1": 7.1281031489372255
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Documents and Topics</b>",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1200,
        "xaxis": {
         "visible": false
        },
        "yaxis": {
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>Similarity Score: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "0_generalization_models_mar...",
          "1_adaptive_reinforcement_ma...",
          "2_bandits_bandit_optimal",
          "3_kernelized_kernels_superv...",
          "4_discovering_association_p...",
          "5_classifiers_classificatio...",
          "6_classifiers_classifier_cl...",
          "7_matrix_matrices_sparse",
          "8_supervised_classifiers_cl...",
          "9_models_sparse_latent",
          "10_quantum_qavb_qrw",
          "11_optimal_optimization_ban...",
          "12_semantic_semantics_analo...",
          "13_labeling_supervised_labels",
          "14_svms_svm_classification",
          "15_lasso_regularization_reg...",
          "16_clustering_cluster_clust...",
          "17_clustering_cluster_algor...",
          "18_forecasting_prediction_f...",
          "19_manifolds_manifold_super...",
          "20_pca_dimensionality_sparse"
         ],
         "xaxis": "x",
         "y": [
          "0_generalization_models_mar...",
          "1_adaptive_reinforcement_ma...",
          "2_bandits_bandit_optimal",
          "3_kernelized_kernels_superv...",
          "4_discovering_association_p...",
          "5_classifiers_classificatio...",
          "6_classifiers_classifier_cl...",
          "7_matrix_matrices_sparse",
          "8_supervised_classifiers_cl...",
          "9_models_sparse_latent",
          "10_quantum_qavb_qrw",
          "11_optimal_optimization_ban...",
          "12_semantic_semantics_analo...",
          "13_labeling_supervised_labels",
          "14_svms_svm_classification",
          "15_lasso_regularization_reg...",
          "16_clustering_cluster_clust...",
          "17_clustering_cluster_algor...",
          "18_forecasting_prediction_f...",
          "19_manifolds_manifold_super...",
          "20_pca_dimensionality_sparse"
         ],
         "yaxis": "y",
         "z": [
          [
           0.9999999403953552,
           0.5682041645050049,
           0.6264923810958862,
           0.6511068344116211,
           0.4275442063808441,
           0.3822113871574402,
           0.49155837297439575,
           0.637553870677948,
           0.6230520606040955,
           0.6201398372650146,
           0.5626220703125,
           0.513198971748352,
           0.41103440523147583,
           0.7552335262298584,
           0.5792532563209534,
           0.6649317145347595,
           0.5904239416122437,
           0.339876264333725,
           0.5890902876853943,
           0.545674741268158,
           0.4142591953277588
          ],
          [
           0.5682041645050049,
           1.0000001192092896,
           0.7030984163284302,
           0.3661853075027466,
           0.3288348913192749,
           0.21607723832130432,
           0.36055031418800354,
           0.2773948013782501,
           0.42780134081840515,
           0.352292001247406,
           0.5315848588943481,
           0.5916774868965149,
           0.3519243001937866,
           0.5088522434234619,
           0.3254240155220032,
           0.3070141077041626,
           0.1873881220817566,
           0.16204918920993805,
           0.6618030071258545,
           0.19262206554412842,
           0.17087024450302124
          ],
          [
           0.6264923810958862,
           0.7030984163284302,
           1.0000001192092896,
           0.4499889612197876,
           0.28872424364089966,
           0.19091719388961792,
           0.331989049911499,
           0.4323802590370178,
           0.4240530729293823,
           0.35028886795043945,
           0.4029541313648224,
           0.6162375211715698,
           0.3104991018772125,
           0.6247668266296387,
           0.4105534553527832,
           0.45975273847579956,
           0.2824931740760803,
           0.13105912506580353,
           0.6236844062805176,
           0.25807875394821167,
           0.15085385739803314
          ],
          [
           0.6511068344116211,
           0.3661853075027466,
           0.4499889612197876,
           1.000000238418579,
           0.42497169971466064,
           0.5843939781188965,
           0.5969208478927612,
           0.6174278259277344,
           0.7830809354782104,
           0.4988900423049927,
           0.41945409774780273,
           0.31488460302352905,
           0.45682621002197266,
           0.7093057036399841,
           0.8240988850593567,
           0.7426628470420837,
           0.5519998669624329,
           0.4048348367214203,
           0.4012303352355957,
           0.7655905485153198,
           0.5386964678764343
          ],
          [
           0.4275442063808441,
           0.3288348913192749,
           0.28872424364089966,
           0.42497169971466064,
           0.9999997615814209,
           0.3193994164466858,
           0.47792530059814453,
           0.2682507634162903,
           0.5592714548110962,
           0.4139416515827179,
           0.28020811080932617,
           0.19481563568115234,
           0.5899150371551514,
           0.5117784738540649,
           0.4095727503299713,
           0.2775574326515198,
           0.3204798102378845,
           0.49038103222846985,
           0.3853747844696045,
           0.33994296193122864,
           0.3237638771533966
          ],
          [
           0.3822113871574402,
           0.21607723832130432,
           0.19091719388961792,
           0.5843939781188965,
           0.3193994164466858,
           1,
           0.6709632873535156,
           0.34615465998649597,
           0.60820072889328,
           0.22775977849960327,
           0.23972970247268677,
           0.12736554443836212,
           0.29718929529190063,
           0.44788581132888794,
           0.6514078378677368,
           0.3703739047050476,
           0.4216349720954895,
           0.3917998671531677,
           0.22659654915332794,
           0.5151522755622864,
           0.41298243403434753
          ],
          [
           0.49155837297439575,
           0.36055031418800354,
           0.331989049911499,
           0.5969208478927612,
           0.47792530059814453,
           0.6709632873535156,
           1.0000001192092896,
           0.32411274313926697,
           0.6728129386901855,
           0.32645919919013977,
           0.36197102069854736,
           0.26933154463768005,
           0.3143095374107361,
           0.5690180063247681,
           0.6622748374938965,
           0.44152548909187317,
           0.4058971107006073,
           0.5132003426551819,
           0.48637837171554565,
           0.4773790240287781,
           0.40170007944107056
          ],
          [
           0.637553870677948,
           0.2773948013782501,
           0.4323802590370178,
           0.6174278259277344,
           0.2682507634162903,
           0.34615465998649597,
           0.32411274313926697,
           0.9999998807907104,
           0.4208698868751526,
           0.4619029462337494,
           0.33006805181503296,
           0.37051889300346375,
           0.2750535309314728,
           0.5134506225585938,
           0.5384035110473633,
           0.626799464225769,
           0.5817536115646362,
           0.2675992250442505,
           0.27376818656921387,
           0.6097479462623596,
           0.5741106271743774
          ],
          [
           0.6230520606040955,
           0.42780134081840515,
           0.4240530729293823,
           0.7830809354782104,
           0.5592714548110962,
           0.60820072889328,
           0.6728129386901855,
           0.4208698868751526,
           1.0000001192092896,
           0.4687656760215759,
           0.413640558719635,
           0.25270208716392517,
           0.5818251371383667,
           0.8075910806655884,
           0.731718897819519,
           0.5714850425720215,
           0.41172465682029724,
           0.4253552556037903,
           0.4856450855731964,
           0.6022053956985474,
           0.39462703466415405
          ],
          [
           0.6201398372650146,
           0.352292001247406,
           0.35028886795043945,
           0.4988900423049927,
           0.4139416515827179,
           0.22775977849960327,
           0.32645919919013977,
           0.4619029462337494,
           0.4687656760215759,
           0.9999999403953552,
           0.40403658151626587,
           0.25780144333839417,
           0.42647072672843933,
           0.5116480588912964,
           0.3721489906311035,
           0.4831593334674835,
           0.5398964881896973,
           0.38468098640441895,
           0.3180747926235199,
           0.4563055634498596,
           0.5021280646324158
          ],
          [
           0.5626220703125,
           0.5315848588943481,
           0.4029541313648224,
           0.41945409774780273,
           0.28020811080932617,
           0.23972970247268677,
           0.36197102069854736,
           0.33006805181503296,
           0.413640558719635,
           0.40403658151626587,
           1,
           0.3607461452484131,
           0.3027333617210388,
           0.46586889028549194,
           0.3818947672843933,
           0.3178389072418213,
           0.400694340467453,
           0.3014305830001831,
           0.4407068192958832,
           0.3413763642311096,
           0.2476203292608261
          ],
          [
           0.513198971748352,
           0.5916774868965149,
           0.6162375211715698,
           0.31488460302352905,
           0.19481563568115234,
           0.12736554443836212,
           0.26933154463768005,
           0.37051889300346375,
           0.25270208716392517,
           0.25780144333839417,
           0.3607461452484131,
           0.9999998807907104,
           0.14033463597297668,
           0.41413265466690063,
           0.23598620295524597,
           0.278086394071579,
           0.2556513845920563,
           0.11007280647754669,
           0.4519372582435608,
           0.18716388940811157,
           0.13518674671649933
          ],
          [
           0.41103440523147583,
           0.3519243001937866,
           0.3104991018772125,
           0.45682621002197266,
           0.5899150371551514,
           0.29718929529190063,
           0.3143095374107361,
           0.2750535309314728,
           0.5818251371383667,
           0.42647072672843933,
           0.3027333617210388,
           0.14033463597297668,
           0.9999997615814209,
           0.5002223253250122,
           0.386873722076416,
           0.23307359218597412,
           0.26610293984413147,
           0.31195199489593506,
           0.2845473885536194,
           0.30324071645736694,
           0.23643732070922852
          ],
          [
           0.7552335262298584,
           0.5088522434234619,
           0.6247668266296387,
           0.7093057036399841,
           0.5117784738540649,
           0.44788581132888794,
           0.5690180063247681,
           0.5134506225585938,
           0.8075910806655884,
           0.5116480588912964,
           0.46586889028549194,
           0.41413265466690063,
           0.5002223253250122,
           1.000000238418579,
           0.6612408757209778,
           0.6068412065505981,
           0.4234326481819153,
           0.3248395323753357,
           0.5172827243804932,
           0.5353680849075317,
           0.3362174332141876
          ],
          [
           0.5792532563209534,
           0.3254240155220032,
           0.4105534553527832,
           0.8240988850593567,
           0.4095727503299713,
           0.6514078378677368,
           0.6622748374938965,
           0.5384035110473633,
           0.731718897819519,
           0.3721489906311035,
           0.3818947672843933,
           0.23598620295524597,
           0.386873722076416,
           0.6612408757209778,
           1.0000001192092896,
           0.6450276374816895,
           0.48887938261032104,
           0.43921080231666565,
           0.43144047260284424,
           0.5853279829025269,
           0.47271522879600525
          ],
          [
           0.6649317145347595,
           0.3070141077041626,
           0.45975273847579956,
           0.7426628470420837,
           0.2775574326515198,
           0.3703739047050476,
           0.44152548909187317,
           0.626799464225769,
           0.5714850425720215,
           0.4831593334674835,
           0.3178389072418213,
           0.278086394071579,
           0.23307359218597412,
           0.6068412065505981,
           0.6450276374816895,
           1.0000001192092896,
           0.4560922384262085,
           0.24988976120948792,
           0.4027039408683777,
           0.5718699097633362,
           0.49769288301467896
          ],
          [
           0.5904239416122437,
           0.1873881220817566,
           0.2824931740760803,
           0.5519998669624329,
           0.3204798102378845,
           0.4216349720954895,
           0.4058971107006073,
           0.5817536115646362,
           0.41172465682029724,
           0.5398964881896973,
           0.400694340467453,
           0.2556513845920563,
           0.26610293984413147,
           0.4234326481819153,
           0.48887938261032104,
           0.4560922384262085,
           1.000000238418579,
           0.6784454584121704,
           0.21265393495559692,
           0.6131532192230225,
           0.5632271766662598
          ],
          [
           0.339876264333725,
           0.16204918920993805,
           0.13105912506580353,
           0.4048348367214203,
           0.49038103222846985,
           0.3917998671531677,
           0.5132003426551819,
           0.2675992250442505,
           0.4253552556037903,
           0.38468098640441895,
           0.3014305830001831,
           0.11007280647754669,
           0.31195199489593506,
           0.3248395323753357,
           0.43921080231666565,
           0.24988976120948792,
           0.6784454584121704,
           0.9999995231628418,
           0.2654353678226471,
           0.4407283663749695,
           0.42923688888549805
          ],
          [
           0.5890902876853943,
           0.6618030071258545,
           0.6236844062805176,
           0.4012303352355957,
           0.3853747844696045,
           0.22659654915332794,
           0.48637837171554565,
           0.27376818656921387,
           0.4856450855731964,
           0.3180747926235199,
           0.4407068192958832,
           0.4519372582435608,
           0.2845473885536194,
           0.5172827243804932,
           0.43144047260284424,
           0.4027039408683777,
           0.21265393495559692,
           0.2654353678226471,
           1.000000238418579,
           0.2503375709056854,
           0.15008917450904846
          ],
          [
           0.545674741268158,
           0.19262206554412842,
           0.25807875394821167,
           0.7655905485153198,
           0.33994296193122864,
           0.5151522755622864,
           0.4773790240287781,
           0.6097479462623596,
           0.6022053956985474,
           0.4563055634498596,
           0.3413763642311096,
           0.18716388940811157,
           0.30324071645736694,
           0.5353680849075317,
           0.5853279829025269,
           0.5718699097633362,
           0.6131532192230225,
           0.4407283663749695,
           0.2503375709056854,
           0.9999998807907104,
           0.6304731369018555
          ],
          [
           0.4142591953277588,
           0.17087024450302124,
           0.15085385739803314,
           0.5386964678764343,
           0.3237638771533966,
           0.41298243403434753,
           0.40170007944107056,
           0.5741106271743774,
           0.39462703466415405,
           0.5021280646324158,
           0.2476203292608261,
           0.13518674671649933,
           0.23643732070922852,
           0.3362174332141876,
           0.47271522879600525,
           0.49769288301467896,
           0.5632271766662598,
           0.42923688888549805,
           0.15008917450904846,
           0.6304731369018555,
           0.9999998807907104
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Similarity Score"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(247,252,240)"
          ],
          [
           0.125,
           "rgb(224,243,219)"
          ],
          [
           0.25,
           "rgb(204,235,197)"
          ],
          [
           0.375,
           "rgb(168,221,181)"
          ],
          [
           0.5,
           "rgb(123,204,196)"
          ],
          [
           0.625,
           "rgb(78,179,211)"
          ],
          [
           0.75,
           "rgb(43,140,190)"
          ],
          [
           0.875,
           "rgb(8,104,172)"
          ],
          [
           1,
           "rgb(8,64,129)"
          ]
         ]
        },
        "height": 800,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "legend": {
         "title": {
          "text": "Trend"
         }
        },
        "margin": {
         "t": 60
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Similarity Matrix</b>",
         "x": 0.55,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on the most similar topic given new_topic_text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar topic: 2, Similarity Score: 0.6836685538291931\n"
     ]
    }
   ],
   "source": [
    "topic_embeddings = topic_model.topic_embeddings_\n",
    "\n",
    "# TODO adapt the text to your own and compare multipble cosine similarities...\n",
    "new_topic_text = \"I love to play soccer\"\n",
    "new_topic_text = \"AI is good for clustering and predictions\"\n",
    "new_topic_text = \"Reinforcement Learning, optimization, bandit\"\n",
    "\n",
    "new_topic_embedding = topic_model._extract_embeddings([new_topic_text], method=\"document\")\n",
    "similarities = cosine_similarity(new_topic_embedding, topic_embeddings)\n",
    "most_similar_topic = np.argmax(similarities)\n",
    "similarity_score = similarities[0][most_similar_topic]\n",
    "\n",
    "print(f\"Most similar topic: {most_similar_topic}, Similarity Score: {similarity_score}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all the similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 22 artists>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar([i for i in range(len(similarities[0]))], similarities[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINT: know you can go back to the figure created with topic_model.visualize_topics() and topic_model.visualize_documents() and find the most similar topic and also find some similar topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
